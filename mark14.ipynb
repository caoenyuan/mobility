{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1069f22-de81-4734-974c-96a9d4b6db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8807a08b-7e57-402d-9c70-63a064494ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c818de870092468bbf872bcb191e047a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "model = MllamaForConditionalGeneration.from_pretrained(model_name,\n",
    "                                             #load_in_8bit=True,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )\n",
    "#processor = AutoProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdd76904-6506-49a3-b27a-9ca29e392030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2098\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 指定user文件夹路径\n",
    "user_folder = \"users_cleaned\"  \n",
    "\n",
    "# 创建一个字典存储所有df\n",
    "dfs = {}\n",
    "\n",
    "# 遍历user文件夹下的文件\n",
    "for i, filename in enumerate(os.listdir(user_folder)):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # 读取每个csv文件\n",
    "        filepath = os.path.join(user_folder, filename)\n",
    "        dfs[i+1] = pd.read_csv(filepath)\n",
    "        #print(i+1)\n",
    "        #print(filename)\n",
    "        #print(dfs[i+1].head())  # 打印前5行以确认读取成功\n",
    "else:\n",
    "    if not dfs:  # 判断dfs字典是否为空\n",
    "        print(\"未在user文件夹中找到csv文件。\")\n",
    "\n",
    "print(len(dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d8a9969-41ea-4285-92fb-3d206fe8dcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      " You are a helpful AI assistant, answer the question as short as possible.\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      " Hello \n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('prompt_poi.txt', 'r') as file:\n",
    "    prompt = file.read()\n",
    "\n",
    "def generate_prompt(content):\n",
    "    begin = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\"\n",
    "    #syst = \"<<SYS>> You are a helpful assistant, always answer as helpfully as possible.\\n If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.<</SYS>>\\n\"\n",
    "    #inst = \"Read the following text. Does it mention the Gilbert damping constant of a certain material? If so, list the corresponding material and its Gilbert damping canstant.\\n\" + content\n",
    "    syst = \"You are a helpful AI assistant, answer the question as short as possible.\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\"\n",
    "    #inst = \"What is the attitude of the following text towards electric vehicles or autonomous driving? Answer with positive, negative or neutral. Also classify it as one of the following categories:\\n1: Safety\\n2: Price and Cost\\n3: Technology maturity and infrastructure\\n4: User Experience\\n5: Policy and market\\n6: Ethical and social impact\\n7: Other or cannot be classified\\nOnly answer the attitude and the category number, and do not answer anything else.\\nText: \"+content\n",
    "    end = \"\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    prompt = (\" \").join([begin, syst, content, end])\n",
    "    return prompt\n",
    "\n",
    "print(generate_prompt('Hello'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "665cdeca-3cac-491c-9d4d-4def3cad7908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df):\n",
    "    \"\"\"\n",
    "    筛选 DataFrame 中坐标在深圳范围内的行，并返回格式化后的字符串。\n",
    "\n",
    "    Args:\n",
    "        df: 一个 pandas DataFrame，包含 'coordinate', 'publish_time', 'location', \n",
    "           'content', 'img_urls' 等列。\n",
    "\n",
    "    Returns:\n",
    "        一个格式化后的字符串，包含符合条件的行的信息。\n",
    "    \"\"\"\n",
    "    # 深圳坐标范围\n",
    "    lon_min, lon_max = 113.7, 114.5\n",
    "    lat_min, lat_max = 22.4, 22.8\n",
    "\n",
    "    output_string = \"\"\n",
    "\n",
    "    # 过滤出深圳范围内的行\n",
    "    filtered_rows = []\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            lon, lat = map(float, row['coordinate'].split(','))\n",
    "            if lon_min <= lon <= lon_max and lat_min <= lat <= lat_max:\n",
    "                filtered_rows.append(row)\n",
    "        except (ValueError, TypeError, AttributeError) as e:\n",
    "            print(f\"Error processing row with index {index}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "    # 限制最大行数\n",
    "    filtered_rows = filtered_rows[:25]\n",
    "    \n",
    "    # 构造输出字符串\n",
    "    for row in filtered_rows:\n",
    "        content = row['content']\n",
    "        if len(content) > 50:\n",
    "            content = content[:50]\n",
    "            \n",
    "        lon, lat = map(float, row['coordinate'].split(','))\n",
    "        formatted_coordinate = f\"{lon:.3f},{lat:.3f}\"\n",
    "        \n",
    "        output_string += f\"{row['publish_time']}, {row['location']} ({formatted_coordinate}), {content}\\n\"\n",
    "    \n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13227bd-c95e-4c72-9d99-87128030ddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = prompt + '\\n\\nContents: \\n' + process_dataframe(dfs[24])\n",
    "print(generate_prompt(new_text))\n",
    "\n",
    "input_prompt = generate_prompt(new_text)\n",
    "#input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "#inputs = tokenizer(input_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "input_tokens = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "with torch.cuda.amp.autocast():\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_tokens,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.1,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa67995d-6562-4665-a3c5-866795b5609d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "output_dir = \"output\"  # 指定输出目录\n",
    "\n",
    "data = []\n",
    "save_interval = 200\n",
    "\n",
    "start_time = time.time()  # 记录开始时间\n",
    "\n",
    "for i in range(24,len(dfs)):\n",
    "    if i % 1 == 0:\n",
    "        new_text = prompt + '\\n\\nContents: \\n' + process_dataframe(dfs[i])\n",
    "        input_prompt = generate_prompt(new_text)\n",
    "        #input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "        #inputs = tokenizer(input_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "        input_tokens = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        with torch.cuda.amp.autocast():\n",
    "            generation_output = model.generate(\n",
    "                input_ids=input_tokens,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=128,\n",
    "                do_sample=False,\n",
    "                repetition_penalty=1.1,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "        print(i)\n",
    "\n",
    "        output_filename = os.path.join(output_dir, f\"output_{i}.txt\")\n",
    "        # 创建空文件（如果文件不存在）\n",
    "        with open(output_filename, \"a+\", encoding=\"utf-8\") as f:\n",
    "            pass  # 不需要写入任何内容，只是创建文件\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f226a5-7549-4e4a-bfd8-974b8f1b4521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ff56c88-cb81-42db-971c-cc2351c27997",
   "metadata": {},
   "outputs": [],
   "source": [
    "del input_tokens\n",
    "del attention_mask\n",
    "del inputs\n",
    "del generation_output\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9052002-38a7-43f4-98c2-630675c8212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "326bb8db-090b-44c6-8e5e-48069bcfa854",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dfs:\n",
    "  dfs[key] = dfs[key][dfs[key]['coordinate'] != '0,0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "445f5017-5ae7-4d76-9a12-d2c2d8da62d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dfs:\n",
    "    dfs[i] = dfs[i][~dfs[i]['content'].str.contains('转发')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dfb462c7-63ad-4a6c-99f7-830cd90c2feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prompt.txt', 'r') as file:\n",
    "    prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e85c9ad-93f2-4f8e-a0c1-dc36acb74395",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prompt_poi.txt', 'r') as file:\n",
    "    prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a8af692f-9ed8-465c-affe-103f15c19d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prompt_img.txt', 'r') as file:\n",
    "    prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59e5bc27-ee28-468e-98fb-710e2fe08cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      " You are a helpful AI assistant, answer the question as short as possible.\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      " The following is a record of posts made by users on social media, including the date, time, geographic location, and latitude and longitude coordinates of each post.\n",
      "Your task is to find the user's home, workplace or school, and work duration.\n",
      "You should judge the type of the user's current location based on time and the content of the post, and determine the user's usual work duration based on the time of location switching. If you don't have enough information, don't jump to conclusions.\n",
      "Your answer should include detailed location and precise coordinates and time, for example:\n",
      "\n",
      "The user likely lives in 位置, (longitude1, latitude1).\n",
      "The user likely works/studies at 位置, (longitude2, latitude2).\n",
      "The user's work/study duration is from -- am to -- pm. \n",
      "\n",
      "Tip: The location that often appears at midnight is likely to be home. Some users are freelancers and their home is their workplace. Number of workplace should not be more than 2.\n",
      "Do not answer anything else.\n",
      "\n",
      "Contents: \n",
      "2024-12-23 09:25 , 无 (0,0), 我在阳台上抽烟，我抽了一半，风抽了一半，我没有和风计较，可能风也有烦恼吧。【评】唯美。  \n",
      "2024-12-20 14:55, 惠州·惠州市公安局经济犯罪侦查大队 (114.44492,23.08878), 职务侵占控告。惠州市经侦支队。 惠州·惠州市公安局经济犯罪侦查大队  显示地图 [组图共2张] 原图 \n",
      "2024-12-18 18:03, 无 (0,0), 跑到龙岗来聚个餐，也是我真心的了还好，农庄还不错不过，我闻过，腊肉不香  [组图共3张] 原图 \n",
      "2024-12-18 17:38, 无 (0,0), 只要有机会，就喜欢游历校园。不管是大学，还是小学。昨晚与妻散步，经过深圳市第二实验学校。早闻其名，原来就在家旁  [组图共2张] 原图 \n",
      "2024-12-17 20:04, 无 (0,0), 鄙人从事刑事辩护入门导师杨照东大律师团队为主、众多律所合作杰作，居然有人“剽窃”，就真的太LOW了。辩护业绩要靠真本事，忽悠长不了啊网页链接  \n",
      "2024-12-15 12:51, 无 (0,0), 你的葬礼，在大多数人的眼里，只是一场聚会。     ——郭德纲【评】多么深刻的领悟！又是多么残酷的领悟！事实又何尝不是如此呢？说白了就是，不管你官大或富有，其实在别人眼里，你并没那么重要。  原图 \n",
      "2024-12-15 11:40, 无 (0,0), 204:85，弹劾通过！政治，也是有规则的，不是随意揉捏的面团。  原图 \n",
      "2024-12-12 14:25, 无 (0,0), 整周年。有风有雨，有欢笑有泪水，更多是收获与成长。心灵安宁，日渐依赖，这就是信任、理解、包容与爱。互相成就，相互成长，与年龄无关、与职业无关，这就是彼此欣赏、赞美和爱。人生不易，道阻且长，只要同心同向、相亲相爱，终将遇无不克。成长无止境，爱无限，爱无所不能………  [组图共3张] 原图 \n",
      "2024-12-11 09:03, 无 (0,0), 海关规章层级的旧《计核办法》在走私普通货物罪案件辩护中，广受质疑和诟病。降级为海关规范性文件的新《计核办法》，在科学性、合理性上有所进步，值得肯定。但，一直不理解的是降格效力层级、删除计核程序，到底为哪般？团队原创||新旧《偷逃税款计核办法》对比与解读  \n",
      "2024-12-10 10:33, 无 (0,0), 真是奇闻，闻所未闻！[破涕为笑][破涕为笑][破涕为笑]这个“海关全瞎符”符咒还花了不少心思呢[破涕为笑][破涕为笑][破涕为笑]网页链接  \n",
      "2024-12-08 20:28, 无 (0,0), 杀不死你的，终将使你更强大。  \n",
      "2024-12-07 20:06, 无 (0,0), 最美官方答复！“我们无法拒绝他进来读书，但你有权利选择离开。”[玫瑰][玫瑰][玫瑰]评：读书，不同于上学，跟学历没有半毛钱的关系关系。有学历的人，并不一定读书，而是为了学历的生吞活剥。  原图 \n",
      "2024-12-06 17:21, 无 (0,0), 养儿才知父母恩，知恩已是父母去。重蹈养儿不知恩，知恩已是自己去。【感悟】天下最难的事情是教育子女，无他。  原图 \n",
      "2024-12-06 14:48, 深圳·黄贝派出所 (114.13513148982,22.550553), 面对人生磨难，到底是越磨越难，还是难后新生？人生这场旅行不易，再难也得过。我们代理案件的当事人，哪个人容易？哪个家庭容易？ 深圳·黄贝派出所  显示地图 原图 \n",
      "2024-12-06 09:18, 无 (0,0), 一群“不干”、“不能干”、“不想干”、“干不成”的人在“研究”啥叫“值得研究”………  [组图共4张] 原图 \n",
      "2024-12-06 00:04, 无 (0,0), 今天接到检察院电话，我们辩护的某走私电子产品案当事人终于通过羁押必要性审查明天予以取保候审释放。侦查阶段取保失败、逮捕审查阶段申请不予逮捕失败、逮捕及延长侦查期间两次提请羁押必要性审查失败……没办法，行使了自做律师以来第一次投诉检察官的权利。本来要投诉缉私部门办案警察的，因家属劝阻而作罢。移送起诉后，检察院更换了承办检察官。据说，承办检察官是业务型干部，为人正直、业务能力强。几次电话联系，态度很好，也较有耐心，且一再承诺等她研究完卷宗和我们的法律意见书，就给我回电话。今天，终于接到电话了，松了口气啊。当事人家属非常好，非常信任我们。这样，反而我们的压力更大。当事人弟弟电话中就一句话：“王律师，谢谢您，其他的话我都不说了，我们周六喝酒。”每一个案件都有压力。家属格外信任的案件格外有压力只要结果好，一切都值得！         [组图共3张]  原图                         \n",
      "2024-12-04 22:00, 无 (0,0), 琼瑶阿姨“翩然”而去，那么轻盈、淡定、自然……🙏🙏🙏琼瑶遗书及《当雪花飘落》  \n",
      "2024-12-01 18:57, 株洲·株洲西站 (113.067936,27.792406), 呼啸而至。再见，株洲！ 株洲·株洲西站  显示地图 [组图共4张] 原图 \n",
      "2024-11-30 16:26, 常德·汉寿 (111.923913,28.888547), 吃着土菜、呼吸农村的新鲜空气，真的挺好！ 常德·汉寿  显示地图 [组图共2张] 原图 \n",
      "2024-11-28 19:16, 深圳·深圳北站 (114.030455,22.609009), 大姐家公去世，与妻赴湘吊唁。因堵车而改签。这个商务座还是第一次坐。感觉隐私性挺好的 深圳·深圳北站  显示地图 [组图共3张] 原图 \n",
      "2024-11-28 10:43, 无 (0,0), 《暂行办法》终于“转正”。正式办法相比之前暂行办法，合法性、合理性大大提高。[呲牙]网页链接  \n",
      "2024-11-25 23:58, 无 (0,0), 刚才接一视频。是原北京海关兄弟打过来的，旁边还坐着一深圳海关兄弟。尽管深圳这么小，自从我出来，就没跟这兄弟再见过面。视频中，深圳兄弟喝得有些嗨了，对着电话就是一顿输出，说我什么牛逼了、现在做大了等等，没有我插话的份我说，不是我牛逼，是我不想给兄弟们添麻烦。这是我的心里话，也是实话。都做律师了，而且又只是做走私犯罪案件辩护，何必让人觉得瓜田李下呢？我都做律师了，跟什么人交往有什么好顾忌的呢？但，不能不顾及兄弟们。但是，就因为是兄弟，所以没再吃吃喝喝。有时一个电话，知道记得彼此，足矣。人生很短，不给别人添麻烦，也是一种修行、修养。          原图                         \n",
      "2024-11-25 19:29, 无 (0,0), 北大葛云松教授的灵魂拷问[擦汗]  原图 \n",
      "2024-11-25 13:05, 无 (0,0), 2500万案值，“红通”追逃，罕见[捂脸][捂脸][捂脸]看来，深圳缉私的追逃力度还是蛮大的[呲牙]网页链接  \n",
      "2024-11-24 17:14, 无 (0,0), 政治性、不合理的国际贸易税收、管制，不可能轻易阻挡国际产业链的流动。本人预测：如果特朗普兑现对中国输美产品征收60%关税承诺，也就是大量中国产品走私入美的开始。[呲牙][呲牙][呲牙]走私10万颗芯片到中国，4人被捕！  \n",
      "2024-11-23 14:39, 香港·海港城港威大廈 (114.16996705,22.30114829), 每次看到“中国海关”，都还是满满的亲切感。十多年的职业生涯，情依旧。 香港·海港城港威大廈  显示地图 [组图共2张] 原图 \n",
      "2024-11-23 12:41, 深圳·福田站 (114.057068,22.538079), 第一次坐高铁从福田站往香港。体验一把，其乐也融 深圳·福田站  显示地图 [组图共2张] 原图 \n",
      "2024-11-20 13:32, 无 (0,0), 今天是2024年11月20日。2006年的今天，彭宇案发生了。审理此案的法官说出了一句令众人震惊的话：“不是你撞的，干嘛去扶？”这一事件立刻引起了社会广泛的关注和愤怒，被视为社会道德水平的倒退。  [组图共2张] 原图 \n",
      "2024-11-19 18:15, 无 (0,0), 今天对某特大走私电子产品案进行阅卷（275本卷）。综合所有涉案人员的笔录发现笔录中存在“无中生有”、“掐头去尾”、“概括总结”、“自问自答”、“指供诱供”、“先教再供”等等违法情形。这是侦查讯问的基本套路，必须提高警惕。  原图 \n",
      "2024-11-18 14:43, 无 (0,0), 欠钱不还的人，可以好好看看。魔高一尺，道高一丈。是该整治整治了。“两高”发布《关于办理拒不执行判决、裁定刑事案件适用法律若干问题的解释》  \n",
      " \n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_string = \"\"\n",
    "\n",
    "for index, row in dfs[10].iloc[0:30].iterrows():\n",
    "    output_string += f\"{row['publish_time']}, {row['location']} ({row['coordinate']}), {row['content']}\\n\"\n",
    "    img_url = row['img_urls']\n",
    "    #output_str = f\"{row['publish_time']}, {row['location']} ({row['coordinate']}), {row['content']}\"\n",
    "    #print(output_str)\n",
    "#print(output_string)\n",
    "\n",
    "#download_image(img_url, save_path)\n",
    "new_text = prompt + '\\n\\nContents: \\n' + output_string\n",
    "#print(new_text)\n",
    "print(generate_prompt(new_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13bffa53-b85e-4f34-8d89-ca35e270eaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      " You are a helpful AI assistant, answer the question as short as possible.\n",
      "user\n",
      " The following is a record of posts made by users on social media, including the date, time, geographic location, and latitude and longitude coordinates of each post.\n",
      "Your task is to find the user's home, workplace or school, and work duration.\n",
      "You should judge the type of the user's current location based on time and the content of the post, and determine the user's usual work duration based on the time of location switching. If you don't have enough information, don't jump to conclusions.\n",
      "Your answer should include detailed location and precise coordinates and time, for example:\n",
      "\n",
      "The user likely lives in 位置, (longitude1, latitude1).\n",
      "The user likely works/studies at 位置, (longitude2, latitude2).\n",
      "The user's work/study duration is from -- am to -- pm. \n",
      "\n",
      "Tip: The location that often appears at midnight is likely to be home. Some users are freelancers and their home is their workplace. Number of workplace should not be more than 2.\n",
      "Do not answer anything else.\n",
      "\n",
      "Contents: \n",
      "2024-12-06 14:48, 深圳·黄贝派出所 (114.13513148982,22.550553), 面对人生磨难，到底是越磨越难，还是难后新生？人生这场旅行不易，再难也得过。我们代理案件的当事人，哪个人容易？哪个家庭容易？ 深圳·黄贝派出所  显示地图 原图 \n",
      "2024-11-28 19:16, 深圳·深圳北站 (114.030455,22.609009), 大姐家公去世，与妻赴湘吊唁。因堵车而改签。这个商务座还是第一次坐。感觉隐私性挺好的 深圳·深圳北站  显示地图 [组图共3张] 原图 \n",
      "2024-11-23 12:41, 深圳·福田站 (114.057068,22.538079), 第一次坐高铁从福田站往香港。体验一把，其乐也融 深圳·福田站  显示地图 [组图共2张] 原图 \n",
      "2024-11-08 12:59, 深圳·深圳宝安国际机场卫星厅 (113.8048,22.640521), 第一次坐海航大飞机。老婆见我坐的时间太长，给我订的商务座。确实非常宽敞、舒适。但是，上次坐深航的体验感非常差，仍然记忆犹新。如果订票时知道是深航，我可能就拒绝了。由此观之，看人、看事，也不能一成不变。 深圳·深圳宝安国际机场卫星厅  显示地图 [组图共2张] 原图 \n",
      "2024-11-08 12:26, 深圳·深圳宝安国际机场卫星厅 (113.8048,22.640521), 出发北京。今天打的过机场，的士司机找了一条新路，居然从办公室到机场不到半个小时。酸爽 深圳·深圳宝安国际机场卫星厅  显示地图 [组图共2张] 原图 \n",
      "2024-11-01 20:17, 深圳·坪地街道第二办公楼 (114.31902,22.77799), 原来一直以为深圳一个街道办应该很小，想不到居然还有“二办”讨论问题还是很轻松的。专业人，谈起事来就是通透、到位现在政府工作人员的素质还是很高的。 深圳·坪地街道第二办公楼  显示地图 原图 \n",
      "2024-10-31 00:26, 深圳·弘源寺 (113.901324,22.701693), 晚上满桌素食，确实难为我了如果让我连续一个星期素食，估计我是坚持不了的。做什么事，如果长期坚持都不是件容易事啊 深圳·弘源寺  显示地图 原图 \n",
      "2024-10-29 22:35, 深圳·怡景花园 (114.13672,22.55507), 与妻散步。路过两小伙鲜榨现卖油柑橄榄汁。年轻人创业不易，友情购买两瓶。三秒微涩，五秒回甘。其广告语印象深刻：我命油我不油天 深圳·怡景花园  显示地图 [组图共2张] 原图 \n",
      "2024-10-22 17:08, 深圳·坪地街道办事处 (114.309333,22.774708), 很有趣的案例。如果没被判死，却被吓死了，如何处理？ 深圳·坪地街道办事处  显示地图 原图 \n",
      "2024-10-21 23:01, 深圳·深圳鹏城技师学院 (114.069501,22.525982), 夜游。来到深圳鹏程技师学院。记得二十多年前这里是北大青鸟，不知何时改名换姓了。深圳就是这样：你昨天看到的，也许今天就不是昨天看到的了。 深圳·深圳鹏城技师学院  显示地图 [组图共2张] 原图 \n",
      "2024-10-20 18:07, 深圳·金中环国际商务大厦 (114.06256,22.534322), 再现周末连续两天加班。接待客人五拨。真还有点累啊 深圳·金中环国际商务大厦  显示地图 原图 \n",
      "2024-10-01 23:16, 深圳·深圳北高铁站 (114.0295,22.609875), 回湘。 深圳·深圳北高铁站  显示地图 原图 \n",
      "2024-09-27 17:22, 深圳·金中环国际商务大厦 (114.062644,22.534374), 国庆前一天开庭。第二次庭审辅导。这是我们团队庭前常规动作，一般进行三次，过关为止。 深圳·金中环国际商务大厦  显示地图 原图 \n",
      "2024-09-21 18:57, 深圳·野湖南(侨城坊店) (113.980221,22.552229), 感谢老婆、兄弟姐妹们。又老了一岁开心、快乐、幸福😁就好！ 深圳·野湖南(侨城坊店)  显示地图 [组图共2张] 原图 \n",
      "2024-09-14 19:12, 深圳·金中环国际商务大厦 (114.062629,22.534332), 中秋放假前夜，还有三拨接待。刑事案件，时不我待啊。真正是：案件于我，只是职业；案件于当事人及其家属，是天大的事。 深圳·金中环国际商务大厦  显示地图 原图 \n",
      "2024-09-08 00:16, 深圳·九色国际影城(水围店) (114.064623,22.521595), 周六，白天加班。晚上，放松，电影一场：《异形：夺命舰》。 深圳·九色国际影城(水围店)  显示地图 原图 \n",
      "2024-09-05 21:06, 深圳·黄贝岭公园 (114.137984,22.549589), TMD国足，0：7输给日本，还有脸回来吗？到底是出了什么问题呢？ 深圳·黄贝岭公园  显示地图 \n",
      "2024-08-25 19:41, 深圳·岁集家具(华侨城店) (113.995828,22.539035), 高中同学、校友相聚。话母校、忆青春，不胜唏嘘。一晃，今年三十年了。 深圳·岁集家具(华侨城店)  显示地图 [组图共2张] 原图 \n",
      "2024-08-25 00:07, 深圳·赵记传承(罗湖黄贝店) (114.134638,22.548395), 饭后。一场电影，一盒虾球，一杯姜撞奶，完美周六。 深圳·赵记传承(罗湖黄贝店)  显示地图 [组图共3张] 原图 \n",
      "2024-08-24 18:25, 深圳·黄贝岭公园 (114.137984,22.549589), 周末。与妻对坐窗台。读读书、抽抽烟、聊聊天、看看窗外，一眼望见了深圳的山、香港的山，还有深港界河——深圳河。好不惬意。近半年来少有的惬意周末。 深圳·黄贝岭公园  显示地图 [组图共3张] 原图 \n",
      "2024-08-20 08:40, 深圳·金中环国际商务大厦 (114.062478,22.534209), “专业、专注、尽职、尽责”，是私职刑辩团队的专业本职追求，始终如一。量力而行，力所能及地做些社会公益，也是私职刑辩团队在走的路。网页链接 深圳·金中环国际商务大厦  显示地图 \n",
      "2024-08-16 18:21, 深圳·金中环国际商务大厦 (114.062478,22.534209), 梅伊诺：“公正的定罪和公正的宣告无罪共同构成了判决的公正”。 深圳·金中环国际商务大厦  显示地图 原图 \n",
      "2024-08-14 12:02, 全文 (114.062517,22.534345), 好兄弟蒋海松教授百家讲坛开讲！祝贺！今天周三8月14日中午12点cctv10、央视科教频道, 每集40分钟 。8月14日到8月23日，第一季10期十天连播，每天中午12点整，晚上重播。 第二季10集后再续。     这是《百家讲坛》首档法律文化类节目,讲述神的故事，探索法的智慧，反思人的精神。借助《西游记》分析明代社会思潮“以情叛理\"的变迁，评点古代律法治理的得失成败 ，追寻自由，向往正义，呼唤人性! 深圳·金中环国际商务大厦         [组图共6张]  原图                         \n",
      "2024-08-13 08:37, 深圳·黄贝岭公园 (114.137984,22.549589), 今天预约四拨接待，中途还要去龙岗某街道办应约开会。最后一拨接待预约晚至20：00。几天旅游欠下的“债”得一笔一笔“偿还”。 深圳·黄贝岭公园  显示地图 原图 \n",
      "2024-07-26 16:33, 深圳·深圳北高铁站 (114.0295,22.609875), 出发厦门。今天晚上，应邀参加鹭岛刑辩夜话之刑事辩护如何精细化，并做与谈发言。明天一整天，应邀第九届南方（鹭岛）刑事法论坛之海关监管立法调整与走私犯罪辩护应对，并主持专题二讨论。希望学有所获。 深圳·深圳北高铁站  显示地图 [组图共3张] 原图 \n",
      "2024-07-25 21:56, 深圳·金中环国际商务大厦 (114.062478,22.534209), 一个下午，连续六拨接待，都是案件要处理。创下做律师以来案件接待纪录。明天去厦门开会，然后陪儿子、外甥女赴内蒙古、北京旅游。陪孩子，还是必要的。毕竟，一眨眼，孩子就不再需要陪伴了。 深圳·金中环国际商务大厦  显示地图 [组图共3张] 原图 \n",
      "2024-07-22 10:08, 香港·莲塘口岸 (114.159083,22.550863), 拜登终于退选。第一次走莲塘口岸，体验感非常差。深圳最新口岸，观感不佳。 香港·莲塘口岸  显示地图 [组图共3张] 原图 \n",
      "2024-07-21 23:23, 深圳·金中环国际商务大厦 (114.062478,22.534209), 老婆组织老王家十多人深港澳旅游。八十多岁的老爷子视察我的办公室。仰望星空下的平安大厦，会想些啥呢？ 深圳·金中环国际商务大厦  显示地图 [组图共3张] 原图 \n",
      "2024-07-20 01:23, 深圳·深圳音乐厅演奏大厅 (114.057152,22.546993), 周末，参加深圳交响乐团2023—2024音乐季闭幕音乐会。听听还蛮享受的，虽然听得不是太懂 深圳·深圳音乐厅演奏大厅  显示地图 [组图共3张] 原图 \n",
      "2024-07-16 17:28, 全文 (114.062636,22.534275), 【当事人反过来安慰律师不要焦虑】某走私案货主，因通关组织（也是我们团队辩护的）被查而牵出，涉税才200多万且已全部补缴。我们呈请侦查机关申请取保候审失败，呈请检察院不予批准逮捕失败，然后又向检察院提起羁押必要性审查申请，同样失败。到看守所会见当事人，其对我们的工作满意，还特别跟我们说你们律师团队不要因为我的取保候审而焦虑，我知道我的案件涉及到巨大团伙案件，确实很难，你们已经尽力了。听到这些，我们整个团队既惊讶，又欣慰。惊讶的是，第一次有当事人安慰律师不要因为他的取保候审而焦虑。这是一种什么样的理解和宽容？我们将继续努力，一定要将其尽快取保候审。这是信任，更是责任！ 深圳·金中环国际商务大厦         [组图共3张]  原图                         \n",
      " \n",
      "assistant\n",
      "The user likely lives in 位置: 深圳·黄贝岭公园 (114.137984, 22.549589).\n",
      "The user likely works/studies at 位置: 深圳·金中环国际商务大厦 (114.062478, 22.534209).\n",
      "The user's work/study duration is from 8:37 am to 20:00 pm.\n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt(new_text)\n",
    "#input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "#inputs = tokenizer(input_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "input_tokens = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "with torch.cuda.amp.autocast():\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_tokens,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "        repetition_penalty=1.1,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dc9b03-0659-47f2-b850-0c9b99566e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "output_dir = \"output\"  # 指定输出目录\n",
    "\n",
    "data = []\n",
    "save_interval = 200\n",
    "\n",
    "start_time = time.time()  # 记录开始时间\n",
    "\n",
    "for i in range(1,len(dfs)):\n",
    "    if i % 1 == 0:\n",
    "        output_string = \"\"\n",
    "\n",
    "        for index, row in dfs[i].iloc[0:30].iterrows():\n",
    "            output_string += f\"{row['publish_time']}, {row['location']} ({row['coordinate']}), {row['content']}\\n\"\n",
    "            img_url = row['img_urls']\n",
    "            #output_str = f\"{row['publish_time']}, {row['location']} ({row['coordinate']}), {row['content']}\"\n",
    "            #print(output_str)\n",
    "        #print(output_string)\n",
    "        \n",
    "        #download_image(img_url, save_path)\n",
    "        new_text = prompt + '\\n\\nContents: \\n' + output_string\n",
    "        #print(new_text)\n",
    "        #print(generate_prompt(new_text))\n",
    "        \n",
    "        input_prompt = generate_prompt(new_text)\n",
    "        #input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "        #inputs = tokenizer(input_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "        input_tokens = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        with torch.cuda.amp.autocast():\n",
    "            generation_output = model.generate(\n",
    "                input_ids=input_tokens,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=128,\n",
    "                do_sample=False,\n",
    "                repetition_penalty=1.1,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "        print(op)\n",
    "\n",
    "        output_filename = os.path.join(output_dir, f\"output_{i}.txt\")\n",
    "        # 创建空文件（如果文件不存在）\n",
    "        with open(output_filename, \"a+\", encoding=\"utf-8\") as f:\n",
    "            pass  # 不需要写入任何内容，只是创建文件\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(op)\n",
    "        '''\n",
    "        inst_index = op.find('assistant\\n')\n",
    "        \n",
    "        if inst_index != -1:\n",
    "            #print(text)\n",
    "            #print(op[inst_index + len('assistant\\n'):])\n",
    "            data.append({\"number\": i, \"text\": text, \"sentiment\": op[inst_index + len('assistant\\n'):]})\n",
    "        else:\n",
    "            #print(\"未找到'assistant\\n'标记\")\n",
    "            data.append({\"number\": i, \"text\": text, \"sentiment\":\"\"})\n",
    "\n",
    "        # 每 save_interval 个迭代保存一次\n",
    "        if i % save_interval == 0:\n",
    "            df = pd.DataFrame(data)\n",
    "            df.to_csv(\"output_ev.csv\", encoding = 'utf-8-sig', index=False, mode='a', header=False)  # 追加模式\n",
    "            data = []\n",
    "            end_time = time.time()  # 记录结束时间\n",
    "            elapsed_time = end_time - start_time  # 计算用时\n",
    "            print(f\"已保存到 output_ev.csv，当前迭代次数：{i}，用时：{elapsed_time:.2f} 秒\")\n",
    "    \n",
    "            start_time = end_time  # 更新开始时间，用于计算下一个周期的用时\n",
    "        '''\n",
    "# 最后一次保存\n",
    "#df = pd.DataFrame(data)\n",
    "#df.to_csv(\"output_ev.csv\", encoding = 'utf-8-sig', index=False, mode='a', header=False)\n",
    "#print(\"已保存到 output_ev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39006d02-bff3-495e-bb0b-35135eae36aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fa8182-058e-447f-ab8a-3024eb2e4dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "169a146c-7395-4ded-814b-8274c3981b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"output\"  # 指定输出目录\n",
    "\n",
    "output_filename = os.path.join(output_dir, f\"output_{i}.txt\")\n",
    "# 创建空文件（如果文件不存在）\n",
    "with open(output_filename, \"a+\", encoding=\"utf-8\") as f:\n",
    "    pass  # 不需要写入任何内容，只是创建文件\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484dfd45-1e65-49dd-8e02-a46d06226ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68406361-2e71-4ba5-925c-2b42844410e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 29 Oct 2024\n",
      "\n",
      "Answer question as short as possible.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "The following is a record of posts published by users on social media, including the date, time, geographic location, and latitude and longitude coordinates of each post. Your task is to find the user's points of interest, including but not limited to the location of the user's home, workplace, school, places they often go to, and the coordinate types of these places. The location should be as detailed as possible, the coordinates must be given, and the province and city should be included as much as possible. You should combine the content of the post to determine the type of the user's current location. If you do not have enough information, do not easily draw conclusions.\n",
      "\n",
      "Contents: \n",
      "2015-03-27 13:51:00, 无 (0,0), 落日余晖，美好整个城市。  我正在使用\"城市余晖\"封面图，好漂亮，你们都快来试试！网页链接  原图 \n",
      "2015-04-03 19:29:00, 深圳·龙岗区老干中心 (114.23048652799,22.717405002209), 吃自助如果你喝可乐你就输了，因为还没有认真奋斗就饱了[撇嘴][撇嘴] 深圳·龙岗区老干中心  显示地图 原图 \n",
      "2015-03-06 01:38:00, 深圳·龙岗区老干中心 (114.23048652799,22.717405002209), 배고파 죽겠어요! 深圳·龙岗区老干中心  显示地图 \n",
      "2015-03-05 16:26:00, 深圳·龙岗区老干中心 (114.23048652799,22.717405002209), 每天都是饿到胃疼，才记得起来！！！！ 深圳·龙岗区老干中心  显示地图 \n",
      "2015-03-03 16:47:00, 深圳·龙岗区老干中心 (114.23048652799,22.717405002209), 一觉睡到现在，要不是肚子一直叫，都忘了醒来 深圳·龙岗区老干中心  显示地图 \n",
      "2015-03-03 06:41:00, 深圳·龙岗区老干中心 (114.23048652799,22.717405002209), 终于可以睡觉了！！！！ 深圳·龙岗区老干中心  显示地图 \n",
      "2015-03-02 13:43:00, 深圳·龙岗区老干中心 (114.23048652799,22.717405002209), 上个夜班，累到全身发痛！命苦啊！ 深圳·龙岗区老干中心  显示地图 \n",
      "2015-02-28 23:14:00, 深圳·龙岗区老干中心 (114.23048652799,22.717405002209), 没有肉的生活！ 深圳·龙岗区老干中心  显示地图 [组图共2张] 原图 \n",
      "2015-03-17 07:33:00, 无 (0,0), 我分享了@女生就要任性 的文章 六大食物治疗女性月经失调  \n",
      "2015-03-14 18:05:00, 无 (0,0), 月赠400分钟话费，国内电话免费呼。   http://t.cn/Rwenn7V  原图 \n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "**用户的兴趣点**\n",
      "\n",
      "1. **家**\n",
      " * 地点: 深圳市龙岗区老干中心\n",
      " * 坐标: (114.23048652799, 22.717405002209)\n",
      "2. **工作**\n",
      " * 地点: 深圳市龙岗区老干中心\n",
      " * 坐标: (114.23048652799, 22.717405002209)\n",
      "3. **学校**\n",
      " * 无信息\n",
      "4. **常去的地方**\n",
      " * 无信息\n",
      "5. **其他**\n",
      " * 深圳市龙岗区老干中心似乎是用户的常去的地方，可能是工作或生活的地点。<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": 'Answer question as short as possible.'},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        #{\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": new_text}\n",
    "    ]}\n",
    "]\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(\n",
    "    #image,\n",
    "    text = input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=256)\n",
    "print(processor.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f01f55-1885-4ed2-92b1-f50e5e40e355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2eb3df-5bba-4331-9431-65b242db8152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "343964af-348e-4e99-9f9e-cc1f8c4105d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved to downloaded_image.jpg\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def download_image(image_url, save_path):\n",
    "    # 发送HTTP GET请求获取图片内容\n",
    "    response = requests.get(image_url)\n",
    "\n",
    "    # 检查请求是否成功\n",
    "    if response.status_code == 200:\n",
    "        # 以二进制写入方式打开本地文件\n",
    "        with open(save_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Image saved to {save_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download image. Status code: {response.status_code}\")\n",
    "\n",
    "# 示例用法\n",
    "#image_url = 'https://ww1.sinaimg.cn/large/486c37b1gw6dbeejrajl1j.jpg'\n",
    "save_path = 'downloaded_image.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "56fb56ac-1063-4755-b373-0ca5f2f2a32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "<|image|>The following is a record of a user posting on social media, which contains coordinate information, but the coordinates are likely to be wrong. Analyze the user's real location and what he was doing at the time based on the text content and pictures. The location should be as detailed as possible, and the coordinates must be given, including the province and city as much as possible.\n",
      "\n",
      "Contents: \n",
      "2010-11-15 10:22:00, 无 (0,0), 刚从西藏拍的，发一张试试。  原图 \n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The user is likely in the city of Lhasa, Tibet, China. The image shows a monk standing on a bridge, with a large number of buildings in the background, which is consistent with the architecture of Lhasa. The coordinates provided, (0,0), are likely incorrect and should be ignored.\n",
      "\n",
      "The user was likely taking a photo of the monk and the surrounding area, possibly as part of a personal project or to share with others on social media. The time stamp suggests that the photo was taken in the morning, around 10:22 am, which is consistent with the monk's attire and the clear lighting in the image.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(save_path)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": new_text}\n",
    "    ]}\n",
    "]\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(\n",
    "    image,\n",
    "    text = input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=256)\n",
    "print(processor.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adbb892-9c33-455e-a41c-92a3a8485e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fd0bd3-a6b7-4b92-b8ed-93d4ccc0349f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4a2dc3-fd7f-4395-a240-2b172ffaaa58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "61b1a11f-8f04-48b2-96c0-183bd5407d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a record of a user's social media posts, including the date, time and geolocation of each post. Your task is to reconstruct the user's detailed trajectory throughout the specified period, inferring their likely activities and destinations beyond just the locations where they posted.\n",
      "\n",
      "Each line of your output should represent a distinct point of interest visited by the user, formatted like this:\n",
      "\n",
      "YYYY-MM-DD - AM/PM - Point of Interest (Latitude, Longitude), Type (if discernable)\n",
      "\n",
      "Example Output:\n",
      "\n",
      "2013-10-29 - AM - 武汉·武汉科技大学 (114.26727, 30.44067), Type: School\n",
      "2013-11-01 - PM - 民族大道 (114.397436, 30.477216), Type: Trip\n",
      "2013-11-01 - PM - 武汉 (114.397436, 30.477216) Approximately, Type: Home\n",
      "\n",
      "Important: Do not simply list the locations of the posts. Utilize the content of the posts, time stamps, and geographical proximity to infer the user's full route and potential stops even if they didn't explicitly post from those locations.\n",
      "\n",
      "User Profile: \n",
      "1. 居住地：\n",
      "根据用户的帖子内容和标签，我推测用户可能居住在深圳市。他们在帖子中提到“在深圳，想学摄影的找我吧”，并且在标签中包含“深圳”的信息。\n",
      "坐标：23.10809, 113.26467（广州，但也可能是深圳）\n",
      "2. 工作地点：\n",
      "用户在帖子中提到“教大家学摄影”，这表明他们可能是摄影教练或摄影师。虽然没有明确的工作地点，但根据他们的标签和帖子内容，我推测他们可能工作在深圳或附近地区。\n",
      "坐标：23.10809, 113.26467（广州，但也可能是深圳）\n",
      "3. 学校：\n",
      "用户在帖子中提到“最近把课件修改了一下，做成了幻灯”，这表明他们可能是摄影教练或摄影师，并且可能在学校或培训机构工作。虽然没有明确的学校信息，但根据他们的标签和帖子内容，我推测他们可能与深圳或附近地区的学校有关。\n",
      "坐标：23.10809, 113.26467（广州，但也可能是深圳）\n",
      "4. 旅游目的地：\n",
      "用户在帖子中提到“元旦去九寨沟看雪吧”，“晋陕大峡谷”，“西藏”，这表明他们喜欢旅游和摄影。根据他们的帖子内容，我推测他们可能曾经去过以下地点：\n",
      "* 九寨沟：33.33333, 103.83333\n",
      "* 晋陕大峡谷：37.91667, 109.16667\n",
      "* 西藏：29.64722, 91.11667\n",
      "\n",
      "Contents: \n",
      "2011-01-06 14:47:00, 无 (0,0), 九寨回来了。运气依然谈很好。想雪就下了点雪，想晴，就出来了太阳。呵呵，希望今年一切如愿。  \n",
      "2010-12-15 00:03:00, 无 (0,0), 转发理由:你觉得？  \n",
      "原始用户: 時光記憶SZ\n",
      "转发内容: 刚从西藏拍的，发一张试试。  原图 \n",
      "2010-11-15 10:22:00, 无 (0,0), 刚从西藏拍的，发一张试试。  原图 \n",
      "2010-11-15 10:19:00, 无 (0,0), 最近把课件修改了一下，做成了幻灯。教大家学摄影，更方便了。  \n",
      "2010-11-15 10:18:00, 无 (0,0), 转发理由:在深圳，想学摄影的找我吧。  \n",
      "原始用户: 時光記憶SZ\n",
      "转发内容: 我给自己贴了 旅游、摄影、爱美女、运动、电影、教摄影 标签。  \n",
      "2010-11-15 10:17:00, 无 (0,0), 元旦去九寨沟看雪吧。  \n",
      "2010-11-15 10:19:00, 无 (0,0), 最近把课件修改了一下，做成了幻灯。教大家学摄影，更方便了。  \n",
      "2010-11-15 10:18:00, 无 (0,0), 转发理由:在深圳，想学摄影的找我吧。  \n",
      "原始用户: 時光記憶SZ\n",
      "转发内容: 我给自己贴了 旅游、摄影、爱美女、运动、电影、教摄影 标签。  \n",
      "2010-11-15 10:17:00, 无 (0,0), 元旦去九寨沟看雪吧。  \n",
      "2011-02-16 19:26:00, 无 (0,0), 2010年，我的中国万里行记（影像游记） http://sinaurl.cn/h56pX5  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_string = \"\"\n",
    "\n",
    "for index, row in dfs[4].iloc[0:10].iterrows():\n",
    "    output_string += f\"{row['publish_time']}, {row['location']} ({row['coordinate']}), {row['content']}\\n\"\n",
    "    #output_str = f\"{row['publish_time']}, {row['location']} ({row['coordinate']}), {row['content']}\"\n",
    "    #print(output_str)\n",
    "#print(output_string)\n",
    "\n",
    "new_text = prompt + '\\n\\nContents: \\n' + output_string\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "988b3f8e-0b50-4bc4-a80a-b41a27403d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 16 Oct 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Below is a record of a user's social media posts, including the date, time and geolocation of each post. Your task is to reconstruct the user's detailed trajectory throughout the specified period, inferring their likely activities and destinations beyond just the locations where they posted.\n",
      "\n",
      "Each line of your output should represent a distinct point of interest visited by the user, formatted like this:\n",
      "\n",
      "YYYY-MM-DD - AM/PM - Point of Interest (Latitude, Longitude), Type (if discernable)\n",
      "\n",
      "Example Output:\n",
      "\n",
      "2013-10-29 - AM - 武汉·武汉科技大学 (114.26727, 30.44067), Type: School\n",
      "2013-11-01 - PM - 民族大道 (114.397436, 30.477216), Type: Trip\n",
      "2013-11-01 - PM - 武汉 (114.397436, 30.477216) Approximately, Type: Home\n",
      "\n",
      "Important: Do not simply list the locations of the posts. Utilize the content of the posts, time stamps, and geographical proximity to infer the user's full route and potential stops even if they didn't explicitly post from those locations.\n",
      "\n",
      "User Profile: \n",
      "1. 居住地：\n",
      "根据用户的帖子内容和标签，我推测用户可能居住在深圳市。他们在帖子中提到“在深圳，想学摄影的找我吧”，并且在标签中包含“深圳”的信息。\n",
      "坐标：23.10809, 113.26467（广州，但也可能是深圳）\n",
      "2. 工作地点：\n",
      "用户在帖子中提到“教大家学摄影”，这表明他们可能是摄影教练或摄影师。虽然没有明确的工作地点，但根据他们的标签和帖子内容，我推测他们可能工作在深圳或附近地区。\n",
      "坐标：23.10809, 113.26467（广州，但也可能是深圳）\n",
      "3. 学校：\n",
      "用户在帖子中提到“最近把课件修改了一下，做成了幻灯”，这表明他们可能是摄影教练或摄影师，并且可能在学校或培训机构工作。虽然没有明确的学校信息，但根据他们的标签和帖子内容，我推测他们可能与深圳或附近地区的学校有关。\n",
      "坐标：23.10809, 113.26467（广州，但也可能是深圳）\n",
      "4. 旅游目的地：\n",
      "用户在帖子中提到“元旦去九寨沟看雪吧”，“晋陕大峡谷”，“西藏”，这表明他们喜欢旅游和摄影。根据他们的帖子内容，我推测他们可能曾经去过以下地点：\n",
      "* 九寨沟：33.33333, 103.83333\n",
      "* 晋陕大峡谷：37.91667, 109.16667\n",
      "* 西藏：29.64722, 91.11667\n",
      "\n",
      "Contents: \n",
      "2011-01-06 14:47:00, 无 (0,0), 九寨回来了。运气依然谈很好。想雪就下了点雪，想晴，就出来了太阳。呵呵，希望今年一切如愿。  \n",
      "2010-12-15 00:03:00, 无 (0,0), 转发理由:你觉得？  \n",
      "原始用户: 時光記憶SZ\n",
      "转发内容: 刚从西藏拍的，发一张试试。  原图 \n",
      "2010-11-15 10:22:00, 无 (0,0), 刚从西藏拍的，发一张试试。  原图 \n",
      "2010-11-15 10:19:00, 无 (0,0), 最近把课件修改了一下，做成了幻灯。教大家学摄影，更方便了。  \n",
      "2010-11-15 10:18:00, 无 (0,0), 转发理由:在深圳，想学摄影的找我吧。  \n",
      "原始用户: 時光記憶SZ\n",
      "转发内容: 我给自己贴了 旅游、摄影、爱美女、运动、电影、教摄影 标签。  \n",
      "2010-11-15 10:17:00, 无 (0,0), 元旦去九寨沟看雪吧。  \n",
      "2010-11-15 10:19:00, 无 (0,0), 最近把课件修改了一下，做成了幻灯。教大家学摄影，更方便了。  \n",
      "2010-11-15 10:18:00, 无 (0,0), 转发理由:在深圳，想学摄影的找我吧。  \n",
      "原始用户: 時光記憶SZ\n",
      "转发内容: 我给自己贴了 旅游、摄影、爱美女、运动、电影、教摄影 标签。  \n",
      "2010-11-15 10:17:00, 无 (0,0), 元旦去九寨沟看雪吧。  \n",
      "2011-02-16 19:26:00, 无 (0,0), 2010年，我的中国万里行记（影像游记） http://sinaurl.cn/h56pX5  \n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "根据用户的帖子内容和标签，我推测用户可能居住在深圳市。他们在帖子中提到“在深圳，想学摄影的找我吧”，并且在标签中包含“深圳”的信息。因此，我将用户的居住地设定为深圳市。\n",
      "\n",
      "根据用户的帖子内容和标签，我推测用户可能工作在深圳市或附近地区。他们在帖子中提到“教大家学摄影”，这表明他们可能是摄影教练或摄影师。\n",
      "\n",
      "用户在帖子中提到“最近把课件修改了一下，做成了幻灯”，这表明他们可能在学校或培训机构工作。虽然没有明确的学校信息，但根据他们的标签和帖子内容，我推测他们可能与深圳市或附近地区的学校有关。\n",
      "\n",
      "用户在帖子中提到“元旦去九寨沟看雪吧”，“晋陕大峡谷”，“西藏”，这表明他们喜欢旅游和摄影。根据他们的帖子内容，我推测他们可能曾经去过以下地点：\n",
      "\n",
      "* 九寨沟：33.33333, 103.83333\n",
      "* 晋陕大峡谷：37.91667, 109.16667\n",
      "* 西藏：29.64722, 91.11667\n",
      "\n",
      "基于这些信息，我尝试重建用户的详细行程如下：\n",
      "\n",
      "2010-11-15 - AM - 深圳 (23.10809, 113.26467), Type: Home\n",
      "2010-11-15 - AM - 深圳 (23.10809, 113.26467), Type: Work\n",
      "2010-11-15 - AM - 学校 (23.10809, 113.26467), Type: School\n",
      "2010-11-15 - PM - 深圳 (23.10809, 113.26467), Type: Home\n",
      "2010-12-15 - PM - 西藏 (29.64722, 91.11667), Type: Tourism\n",
      "2010-12-15 - PM - 九寨沟 (33.33333, 103.83333), Type: Tourism\n",
      "2010-12-15 - PM - 晋陕大峡谷 (37.91667, 109.16667), Type: Tourism\n",
      "2011-01-06 - AM - 九寨沟 (33.33333, 103.83333), Type: Tourism\n",
      "2011-02-16 - PM - 深圳 (23.10809, 113.26467), Type: Home\n",
      "\n",
      "请注意，这是我的推测结果，可能存在一定的不确定性。<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        #{\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": new_text}\n",
    "    ]}\n",
    "]\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(\n",
    "    #image,\n",
    "    text = input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=1024)\n",
    "print(processor.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e134fc3-58d2-432c-9bbc-403b6cdd2c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2048699d-8ffb-4fbd-9943-b98d63e222d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 16 Oct 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Below is a record of a user's social media posts, including the date, time and geolocation of each post. Your task is to reconstruct the user's detailed trajectory throughout the specified period, inferring their likely activities and destinations beyond just the locations where they posted.\n",
      "\n",
      "Each line of your output should represent a distinct point of interest visited by the user, formatted like this:\n",
      "\n",
      "YYYY-MM-DD - AM/PM - Point of Interest (Latitude, Longitude), Type (if discernable)\n",
      "\n",
      "Example Output:\n",
      "\n",
      "2013-10-29 - AM - 武汉·武汉科技大学 (114.26727, 30.44067), Type: School\n",
      "2013-11-01 - PM - 民族大道 (114.397436, 30.477216), Type: Trip\n",
      "2013-11-01 - PM - 武汉 (114.397436, 30.477216) Approximately, Type: Home\n",
      "\n",
      "Important: Do not simply list the locations of the posts. Utilize the content of the posts, time stamps, and geographical proximity to infer the user's full route and potential stops even if they didn't explicitly post from those locations.\n",
      "\n",
      "User Profile: \n",
      "1. 用户的家乡或家庭地址：\n",
      "- 用户可能生活在武汉市，具体地址为民族大道（114.396344, 30.477156）。\n",
      "\n",
      "2. 用户的工作或学校：\n",
      "- 用户可能是武汉科技大学的学生，具体地址为武汉科技大学（114.26727, 30.44067）。\n",
      "\n",
      "3. 用户的常去地：\n",
      "- 用户可能经常去软件园中路（114.40347, 30.481615）。\n",
      "- 用户可能经常去白沙洲大道（114.259519, 30.440047）。\n",
      "- 用户可能经常去知行路（114.263999, 30.436998）。\n",
      "- 用户可能经常去民族大道（114.396422, 30.477919）。\n",
      "- 用户可能经常去软件园一路（114.400347, 30.473845）。\n",
      "\n",
      "\n",
      "Contents: \n",
      "2013-11-30 00:00:00, 民族大道 (114.396422,30.477919), 今日之突发事件，我爸坐车来武汉打工！这怎么可以！还好我行动果断，用我的霸道在付家坡把我爸拦截了然后又给送了回去，我爸已经平安到家我很高兴，我爱我的家人[呵呵]，我会用我的全部去爱他们保护他们❤！@呵呵，u my fml❤ 我在:民族大道  显示地图 \n",
      "2013-11-27 22:11:00, 无 (0,0), 使用有道云笔记，轻松同步、管理您各终端的所有笔记。三重备份，存储数据安全有保障。免费的超大存储空间，无限量增长。激活后即可免费获得2GB云存储空间。赶紧来体验吧。 网页链接   原图 \n",
      "2013-11-27 12:55:00, 无 (0,0), 初步验证，就我这乒乓球水平都能把他们轻轻松松11：2，11：3，哎，国乒啊，，，  \n",
      "2013-11-24 23:16:00, 民族大道 (114.39644,30.477713), 看《老有所依》看得好有感觉，，，明天又要上班了，啊，早睡～ 我在:民族大道  显示地图 \n",
      "2013-12-27 15:14:00, 南湖大道 (114.401842,30.480903), 要是上班没有规定时间规定质量的任务，那我就可以长期来厕所磨蹭，此曰“划水”，，， 我在:南湖大道  显示地图 \n",
      "2013-12-27 00:33:00, 软件园三路 (114.401268,30.474922), 不愿睡觉，睡觉很无聊[睡觉][睡觉][睡觉] 我在:软件园三路  显示地图 原图 \n",
      "2013-12-24 23:33:00, 无 (0,0), 转发理由:若得苍姐送的皮肤，我愿意去打职业，，，  \n",
      "原始用户: 小苍cany\n",
      "转发内容: 朋友们，大家平安夜快乐^_^，希望大家开开心心，平平安安，今天你吃苹果了吗？你们的平安夜如何度过的？……（因为元旦有旅游计划，所以我在努力的赶制视频，保证如期的更新）。好了，来点实际的小福利。转播此微博我会在明天抽取一位朋友获得小苍特别BUFF之：LOL皮肤任意选一个小苍赠送。明天公布。  原图 \n",
      "2013-12-18 23:33:00, 无 (0,0), 转发微博已被删除\n",
      "2013-12-18 14:21:00, 无 (0,0), 转发理由:有道云，不错不错、、、  \n",
      "原始用户: 有道云笔记收藏\n",
      "转发内容: 小伙伴们快快用起来！即日起至2014年1月30日，每周都会在小伙伴中产生一名幸运儿，获得神秘奖品！怎样做才能获奖呢？参加办法很简单，只需多多使用@有道云笔记收藏 就可以啦！剩下的就交给运气喽~每周都会有一名幸运儿呢！还不快快行动  原图 \n",
      "2013-12-16 21:21:00, 无 (0,0), 我的床花不花？？？囍  原图 \n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "根据用户的微博内容和地理位置信息，以下是用户的详细轨迹：\n",
      "\n",
      "2013-11-30 - PM - 民族大道 (114.396344, 30.477156), Type: Home\n",
      "2013-11-30 - AM - 民族大道 (114.396422, 30.477919), Type: Home\n",
      "2013-11-27 - PM - 武汉科技大学 (114.26727, 30.44067), Type: School\n",
      "2013-11-27 - AM - 软件园中路 (114.40347, 30.481615), Type: Work or Study\n",
      "2013-11-24 - PM - 武汉科技大学 (114.26727, 30.44067), Type: School\n",
      "2013-11-24 - AM - 民族大道 (114.39644, 30.477713), Type: Home\n",
      "2013-12-27 - PM - 软件园三路 (114.401268, 30.474922), Type: Relaxation\n",
      "2013-12-27 - AM - 南湖大道 (114.401842, 30.480903), Type: Work\n",
      "2013-12-24 - PM - 软件园三路 (114.401268, 30.474922), Type: Relaxation\n",
      "2013-12-18 - PM - 知行路 (114.263999, 30.436998), Type: Work or Study\n",
      "2013-12-18 - AM - 软件园一路 (114.400347, 30.473845), Type: Work or Study\n",
      "2013-12-16 - PM - 白沙洲大道 (114.259519, 30.440047), Type: Relaxation\n",
      "\n",
      "注意：用户的轨迹可能有所不同，因为一些位置信息可能不准确或不完整。<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        #{\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": new_text}\n",
    "    ]}\n",
    "]\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(\n",
    "    #image,\n",
    "    text = input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=1024)\n",
    "print(processor.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4608cea9-43b8-4f66-97aa-eab040c6f0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 16 Oct 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Below is a record of a user's social media posts, including the date, time and geolocation of each post. Your task is to reconstruct the user's detailed trajectory throughout the specified period, inferring their likely activities and destinations beyond just the locations where they posted.\n",
      "\n",
      "Each line of your output should represent a distinct point of interest visited by the user, formatted like this:\n",
      "\n",
      "YYYY-MM-DD - AM/PM - Point of Interest (Latitude, Longitude), Type (if discernable)\n",
      "\n",
      "Example Output:\n",
      "\n",
      "2013-10-29 - AM - 武汉·武汉科技大学 (114.26727, 30.44067), Type: School\n",
      "2013-11-01 - PM - 民族大道 (114.397436, 30.477216), Type: Trip\n",
      "2013-11-01 - PM - 武汉 (114.397436, 30.477216) Approximately, Type: Home\n",
      "\n",
      "Important: Do not simply list the locations of the posts. Utilize the content of the posts, time stamps, and geographical proximity to infer the user's full route and potential stops even if they didn't explicitly post from those locations.\n",
      "\n",
      "Contents: \n",
      "2013-07-27 21:47:00, 无 (0,0), 突然发现，亲情，友情，爱情，在金钱面前变的不堪一击。  \n",
      "2013-07-23 18:58:00, 无 (0,0), 一个月快完了，还不开单，天天上通班，好累  \n",
      "2013-07-18 01:30:00, 无 (0,0), 又是很晚才休息，这几天都是这样，快受不了。  \n",
      "2013-06-18 06:45:00, 无 (0,0), 转发微博已被删除\n",
      "2013-06-18 06:37:00, 无 (0,0), 今天起得太早，好困，但是想到回家就不困了，久违的家人很快就要见到你们。很开心，爸妈女儿回来了  \n",
      "2013-10-01 23:20:00, 南新路 (113.918617,22.534954), 新的一月，新的一天，新的开始，第一天就开单赚钱，继续开单，好运连连 我在:南新路  显示地图 \n",
      "2013-10-01 09:37:00, 桃园路 (113.918319,22.532962), 大家国庆快乐 我在:桃园路  显示地图 \n",
      "2013-09-30 10:38:00, 前海路 (113.913757,22.535783), #你好明天# 我们明天的明天还是上班，唉，可怜的孩子 我在:前海路  显示地图 \n",
      "2013-09-29 12:13:00, 无 (0,0), 转发理由:说的不错  \n",
      "原始用户: 加措上师-慈爱基金\n",
      "转发内容: 坚定的信念是获取成功的动力。在困难面前，如果我们能在众人都放弃时再多坚持一秒，那么，最后的胜利一定是属于我们的。很多时候，成功都是在最后一刻才蹒跚到来。因此，做任何事情，我们都不应该半途而废，哪怕前行的道路再苦再难，也要坚持下去，这样才不会在自己的人生路上留下太多的遗憾。  原图 \n",
      "2013-09-17 14:56:00, 无 (0,0), 还好，老爸没事了，放心了。  \n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "基于提供的用户社交媒体记录，我尝试重构用户的详细轨迹。以下是我的结果：\n",
      "\n",
      "2013-07-18 - PM - 无 (0,0), Type: Rest\n",
      "2013-07-23 - PM - 无 (0,0), Type: Work\n",
      "2013-07-27 - PM - 无 (0,0), Type: Reflection\n",
      "2013-06-18 - AM - 无 (0,0), Type: Travel (回家)\n",
      "2013-06-18 - AM - 无 (0,0), Type: Home\n",
      "2013-09-17 - PM - 无 (0,0), Type: Visit Family\n",
      "2013-09-30 - AM - 前海路 (113.913757,22.535783), Type: Work\n",
      "2013-09-30 - PM - 前海路 (113.913757,22.535783), Type: Home\n",
      "2013-10-01 - AM - 桃园路 (113.918319,22.532962), Type: Work\n",
      "2013-10-01 - PM - 南新路 (113.918617,22.534954), Type: Work\n",
      "\n",
      "我根据以下信息进行推断：\n",
      "\n",
      "1. 用户于 2013 年 6 月 18 日早上从某个未知地点出发，前往家里（Type: Travel），并在当天早上到达家里（Type: Home）。\n",
      "2. 用户于 2013 年 9 月 17 日下午访问了他们的家人（Type: Visit Family）。\n",
      "3. 用户于 2013 年 9 月 30 日上午和下午在前海路工作（Type: Work），并在当天晚上返回前海路（Type: Home）。\n",
      "4. 用户于 2013 年 10 月 1 日上午和下午在桃园路和南新路工作（Type: Work）。\n",
      "\n",
      "我还根据用户的社交媒体记录推断了他们的日常活动和情绪状态：\n",
      "\n",
      "1. 用户似乎经常工作长时间，经常在晚上才休息（2013 年 7 月 18 日，2013 年 7 月 23 日）。\n",
      "2. 用户似乎很关心家人，特别是在 2013 年 9 月 17 日访问家人的时候。\n",
      "3. 用户似乎很感性，\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        #{\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": new_text}\n",
    "    ]}\n",
    "]\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(\n",
    "    #image,\n",
    "    text = input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=512)\n",
    "print(processor.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf51b13-7b8a-4aad-8df9-4fe5f9d6a203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79ad44-220d-4e9b-9a3d-684fab8f24a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fda76e-1052-4b32-8600-faf5f73ff047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e99265-a2fe-420f-8f9f-769e9e9995f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd41e26-6e6e-4555-a0f0-e6e54d94013e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a94a2a3-98cf-4e76-b5c4-583373906980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd2b24-085a-4412-a8aa-6226df70495e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ebc8b-31b9-4042-9abd-1a49246030ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13180d8-92c3-4fc3-afd9-35d8e7af8d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc96fa38-954e-44e8-8ca4-66b879c25493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 15 Oct 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Below is a record of a user's social media posts, including the date, time and geolocation of each post. Your task is to reconstruct the user's detailed trajectory throughout the specified period, inferring their likely activities and destinations beyond just the locations where they posted.\n",
      "\n",
      "Each line of your output should represent a distinct point of interest visited by the user, formatted like this:\n",
      "\n",
      "YYYY-MM-DD - AM/PM - Point of Interest (Latitude, Longitude), Type (if discernable)\n",
      "\n",
      "Example Output:\n",
      "\n",
      "2013-10-29 - AM - 武汉·武汉科技大学 (114.26727, 30.44067), Type: School\n",
      "2013-11-01 - PM - 民族大道 (114.397436, 30.477216), Type: Trip\n",
      "2013-11-01 - PM - (114.397436, 30.477216)Approximately, Type: Home\n",
      "\n",
      "Important: Do not simply list the locations of the posts. Utilize the content of the posts, time stamps, and geographical proximity to infer the user's full route and potential stops even if they didn't explicitly post from those locations.\n",
      "\n",
      "Contents: \n",
      "2011-01-06 14:47:00, 无 (0,0), 九寨回来了。运气依然谈很好。想雪就下了点雪，想晴，就出来了太阳。呵呵，希望今年一切如愿。  \n",
      "2010-12-15 00:03:00, 无 (0,0), 转发理由:你觉得？  \n",
      "原始用户: 時光記憶SZ\n",
      "转发内容: 刚从西藏拍的，发一张试试。  原图 \n",
      "2010-11-15 10:22:00, 无 (0,0), 刚从西藏拍的，发一张试试。  原图 \n",
      "2010-11-15 10:19:00, 无 (0,0), 最近把课件修改了一下，做成了幻灯。教大家学摄影，更方便了。  \n",
      "2010-11-15 10:18:00, 无 (0,0), 转发理由:在深圳，想学摄影的找我吧。  \n",
      "原始用户: 時光記憶SZ\n",
      "转发内容: 我给自己贴了 旅游、摄影、爱美女、运动、电影、教摄影 标签。  \n",
      "2010-11-15 10:17:00, 无 (0,0), 元旦去九寨沟看雪吧。  \n",
      "2010-11-15 10:19:00, 无 (0,0), 最近把课件修改了一下，做成了幻灯。教大家学摄影，更方便了。  \n",
      "2010-11-15 10:18:00, 无 (0,0), 转发理由:在深圳，想学摄影的找我吧。  \n",
      "原始用户: 時光記憶SZ\n",
      "转发内容: 我给自己贴了 旅游、摄影、爱美女、运动、电影、教摄影 标签。  \n",
      "2010-11-15 10:17:00, 无 (0,0), 元旦去九寨沟看雪吧。  \n",
      "2011-02-16 19:26:00, 无 (0,0), 2010年，我的中国万里行记（影像游记） http://sinaurl.cn/h56pX5  \n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Based on the provided social media posts, I will attempt to reconstruct the user's trajectory throughout the specified period. Here's the inferred route:\n",
      "\n",
      "2010-11-15 - AM - 深圳 (23.10809, 113.26467), Type: Home\n",
      "2010-11-15 - AM - (23.10809, 113.26467)Approximately, Type: Home\n",
      "2010-11-15 - AM - 九寨沟 (33.33333, 103.83333), Type: Trip\n",
      "2010-11-15 - AM - (33.33333, 103.83333)Approximately, Type: Trip\n",
      "2010-12-15 - AM - (0, 0)Approximately, Type: Unknown\n",
      "2011-01-06 - AM - (0, 0)Approximately, Type: Unknown\n",
      "2011-02-16 - AM - (0, 0)Approximately, Type: Unknown\n",
      "2011-02-16 - PM - (0, 0)Approximately, Type: Unknown\n",
      "\n",
      "Inferred Route:\n",
      "\n",
      "* The user likely lives in 深圳 (23.10809, 113.26467) based on the post from 2010-11-15, where they mention teaching photography in深圳.\n",
      "* The user then traveled to 九寨沟 (33.33333, 103.83333) to take photos, as mentioned in the post from 2010-11-15.\n",
      "* The posts from 2010-12-15, 2011-01-06, and 2011-02-16 seem to be from unknown locations, possibly while the user was traveling or on a trip.\n",
      "* The post from 2011-02-16 mentions the user's \"中国万里行记\" (China's Long March), which suggests that the user may have been traveling extensively throughout China.\n",
      "\n",
      "Note: The inferred route is based on the content of the posts, time stamps, and geographical proximity. However, some assumptions and inferences have been made to reconstruct the user's trajectory.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**inputs, max_new_tokens=512)\n",
    "print(processor.decode(output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
