{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a8afe9-1f35-48de-8384-6063d350cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340eccc8-60ac-4b8c-9807-dcbbc86185e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354836c268684eada9207a20cbb54495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa98de5083147c59d73b4098291a853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6849501d78f4c629a53dcdb3acf608e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6b4072c418413aae8a4552a41900a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03976e081414d0eb1f345e79a871351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e9a3f2474f4e8dad42bab3accab36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ffe815f3c14a31a112abfb2c2809e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8117ded0ab3481ca0d6b93de3e21981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65a2834e34a4b22a4faf5542987633a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ddb030607e44529aa4dff590b1cd87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84487dc327a34298a465412d3268e660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             #load_in_8bit=True,\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41de4880-8b2b-416c-8a12-7629cf4d8400",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('json', data_files='selected_paragraphs.json')\n",
    "data = data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb0910b4-5a48-4363-ac75-a2c419a5db88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3036 IEEE TRANSACTIONS ON MAGNETICS, VOL. 47, NO. 10, OCTOBER 2011\\nTime-Resolved Magnetization Dynamics and Damping Constant of\\nSputtered Co/Ni Multilayers\\nT. Kato/49, Y . Matsumoto/49, S. Okamoto/50, N. Kikuchi/50, O. Kitakami/50, N. Nishizawa/51, S. Tsunashima/52, and\\nS. Iwata/49\\nDepartment of Quantum Engineering, Nagoya University, Nagoya 464-8603, Japan\\nInstitute of Multidisciplinary Research for Advanced Materials, Tohoku University, Sendai 980-8577, Japan\\nDepartment of Electrical Engineering and Computer Science, Nagoya University, Nagoya 464-8603, Japan\\nDepartment of Research, Nagoya Industrial Science Research Institute, Nagoya 464-0819, Japan\\nCo/Ni multilayers with a stack of Ta (2 nm)/[Co /40\\n /67/111 /41/Ni /40\\n /78/105 /41]\\n/Ta (30 nm) were prepared by dc magnetron sputtering, and their\\nmagnetization dynamics were measured by time-resolved magneto-optical Kerr effect (TRMOKE). The total thickness of the multilayerand perpendicular anisotropy were varied by changing the bilayer period\\n/61\\n /67/111 /43\\n /78/105and number of repeats\\n while\\n /78/105\\n /67/111was\\nkept at a constant of 2.5. The TRMOKE measurements show clear damped oscillation of the magnetization of Co/Ni multilayers after\\nthe pump pulse illumination, and the damping constant\\n of the Co/Ni multilayers was estimated from the TRMOKE waveform. The\\nestimated\\n was found to be independent both on total thickness and anisotropy ﬁeld of the multilayer and was estimated to be\\n /48\\n /48/51/53\\nfor all the multilayers. This means that the use of Ta capping and buffer layers is effective to evaluate intrinsic damping constant of theCo/Ni multilayer, and that independent control of\\nand perpendicular anisotropy are possible for the magnetic multilayers.\\nIndex Terms— Co/Ni multilayer, Gilbert damping, magnetization dynamics, time-resolved magneto-optical Kerr effect (TRMOKE).\\nI. I NTRODUCTION\\nMAGNETIC ﬁlms exhibiting a large perpendicular mag-\\nnetic anisotropy (PMA) are quite attractive for further\\nincrease of the densities of magnetic recording media [1] and\\nmagnetic random access memories [2]. Typical materials aremagnetic multilayers such as Co/Pt [3], [4], Co/Pd [5], andCo/Ni [6], whose interface structures are known to contribute\\nto the large PMA. The magnetic properties and structures for\\nsuch multilayers have been extensively studied; however, theirmagnetization dynamics are poorly understood [7]–[9]. Anunderstanding of the magnetization dynamics is of particular\\nimportance since the dynamics determines the high-speed\\nresponse of magnetic sensors and hard disk drives. Moreover,the magnetization dynamics is a key parameter for the efﬁcientwriting on the microwave-assisted magnetic recording [10] and\\ncurrent-induced magnetization switching (CIMS) [11].\\nThe Co/Ni multilayer is reported to be effective to reduce\\nthreshold currents on CIMS [2] and current-induced domainwall motion [12]. This is probably due to a tunable PMA and\\na low Gilbert damping constant\\nin Landau–Lifshitz–Gilbert\\n(LLG) equation. However, only a few papers have been reportedon the magnetization dynamics of the Co/Ni multilayer [9], [13],\\n[14], and the relationship between its layered structure and the\\ndamping constant is still unclear. It is generally believed thatthe spin–orbit interaction, which couples the spin to the lattice,plays a dominant role in the damping mechanism [15]. In ad-\\ndition, the damping constant is enhanced by the effect of spin\\npumping [16], i.e., spin relaxation of the conduction electronsat the interface between ferromagnetic and normal metals. Thespin–orbit coupling is also believed to play an important role\\non the PMA [17], and the PMA is easily modiﬁed by varying\\nManuscript received February 21, 2011; revised April 26, 2011; accepted\\nMay 22, 2011. Date of current version September 23, 2011. Corresponding au-\\nthor: T. Kato (e-mail: takeshik@nuee.nagoya-u.ac.jp).\\nDigital Object Identiﬁer 10.1109/TMAG.2011.2158082the layered structure of Co/Ni [6]. In this report, we studied\\nmagnetization dynamics of Co/Ni multilayers with various lay-\\nered structures by time-resolved magneto-optical Kerr effect(TRMOKE), and damping constants\\nof the Co/Ni multilayers\\nwere evaluated. In order to study the intrinsic damping constant\\nof the Co/Ni multilayer, we measured TRMOKE spectra ap-\\nplying various external ﬁelds, and we used Ta buffer and cappinglayers since the Ta is considered to have little effect to enhancethe damping constant through the spin pumping effect [16], [18].\\nII. S\\nAMPLE PREPARATION AND MEASUREMENTS\\nCo/Ni multilayers with a stack of Ta (2 nm)/[Co\\n/Ni\\n ]\\n/Ta (30 nm)/thermally oxidized Si sub-\\nstrate were prepared by a dc magnetron sputtering system,where\\nwas kept constant at 2.5. The bilayer period\\nwas varied from 0.8 to 1.3 nm, and the total\\nthickness of the multilayer was also varied from 6.4 to 20nm. Before the TRMOKE measurements, SiN (140 nm) layerwas additionally deposited on the multilayers by an RF mag-\\nnetron sputtering to enhance the Kerr rotation at a wavelength\\nnm (wavelength of the ultrashort pulse ﬁber laser for\\nTRMOKE measurements). Hysteresis loops were measured byan alternating gradient ﬁeld magnetometer. The loops in the\\nperpendicular direction were also measured by using anoma-\\nlous Hall effect and polar Kerr effect. For the Hall and Kerrloops, van der Pauw method and polarized angle modulationmethod were used, respectively.\\nTRMOKE spectra were measured by pump-probe method\\nusing high-power ﬁber laser with\\nnm, a pulse width\\nof 1 ps, a maximum energy of 2\\n J/pulse, and a repetition fre-\\nquency of 200 kHz [19]. The pump and probe beams were fo-\\ncused onto Co/Ni multilayers with diameters of 60 and 15\\n m\\n, respectively. The incident probe beam was normal to the ﬁlm\\nsurface and the polarization of the reﬂected probe beam was an-\\nalyzed to monitor the perpendicular component of the magne-\\ntization\\n after the illumination of the pump beam. Typical\\n0018-9464/$26.00 © 2011 IEEEKATO et al. : TIME-RESOLVED MAGNETIZATION DYNAMICS AND DAMPING CONSTANT OF SPUTTERED CO/NI MULTILAYERS 3037\\nFig. 1. /77/72 loops of Co/Ni multilayers with different bilayer period /100: (a) 1.0\\nnm, (b) 1.1 nm, (c) 1.2 nm, and (d) 1.3 nm. The loops were measured applying amagnetic ﬁeld perpendicular (solid lines) and parallel (dashed lines) to the ﬁlm\\nplane.\\nﬂuences of the pump and probe beams were 4 and 0.3 mJ/cm\\n ,\\nrespectively. During the measurements, an external ﬁeld\\nup to 3.5 kOe was applied in the direction of\\n from the ﬁlm\\nnormal to excite coherent precession after the pump beam irra-\\ndiation. The TRMOKE spectra were analyzed by a numerical\\nsimulation [19] based on the LLG to estimate the damping con-stant\\nand anisotropy ﬁeld\\n , assuming\\n -factor equals to 2.2.\\nThe\\n of the Co/Ni multilayer was also evaluated from the\\nloop and generalized Sucksmith–Thompson method [20].\\nFor the\\n loop, the effective anisotropy\\n was estimated\\nfrom the area surrounded by perpendicular and in-plane\\nloops, and the\\n was simply calculated as\\n , where\\nis the saturation magnetization. The\\n estimated by the\\nthree different ways agreed within 20%.\\nIII. R ESULTS AND DISCUSSIONS\\nFig. 1 shows\\n loops of Co/Ni multilayers with different\\nbilayer period\\n measured applying a magnetic ﬁeld along per-\\npendicular (solid lines) and in-plane (dashed lines). For\\nnm, the easy axis of the magnetization was parallel to the ﬁlmnormal direction indicating the large perpendicular anisotropy\\nof the multilayer [see Fig. 1(a)]. The anisotropy ﬁeld\\nof this\\nmultilayer was estimated to be about 1.8 kOe. The\\n is esti-\\nmated from an effective anisotropy\\n , so that it includes both\\nperpendicular anisotropy and shape anisotropy. The\\n was\\nreduced by increasing the bilayer period, i.e., by reducing thenumber of interfaces per unit thickness, and became negative at\\nnm. The\\n of all the multilayers were almost con-\\nstant indicating a constant shape anisotropy,\\n . Thus, the\\nreduction of\\n with increasing\\n is due to the reduction of the\\nperpendicular anisotropy.\\nFig. 2 shows typical TRMOKE waveforms of the Co/Ni multi-\\nlayer with\\n nm and\\n measured at various\\n .\\nFig. 2. TRMOKE waveforms of Co/Ni multilayer with bilayer period of /100 /61\\n/48 /58 /56nm and number of repeats /78 /61/49 /48 . The signals of laser-induced demag-\\nnetization and exponential decay during the recovery of the perpendicular com-\\nponent of magnetization /77\\nin raw data were subtracted to draw the ﬁgure.\\nThe signals of laser-induced demagnetization and exponential\\ndecay during the recovery of\\n in raw data [21] were subtracted\\nto draw the ﬁgure. The decaying precessional signals triggered\\nby the pump illumination were clearly seen in Fig. 2, and the pre-\\ncessional frequency increased with increasing the applied ﬁeld.The waveforms were analyzed using LLG simulation [19], wherewe assumed macro spin precession with a\\n-factor of 2.2. The\\ndamping constant\\n and anisotropy ﬁeld\\n were varied as ﬁt-\\nting parameters to reproduce the experimental results. The sim-ulated waveforms are shown as solid lines in the ﬁgure, whichcoincided well with the experiments as shown in Fig. 2.\\nFig. 3 shows the dependence of the estimated\\non\\n .\\nfor the Co/Ni multilayers with (a)\\n and\\n , (b)\\nand\\n , and (c)\\n and\\n .F o r\\nthe thin sample [see Fig. 3(a)], the\\n was almost independent\\non\\n and is estimated to be 0.035 for this sample. However,\\nwhen the total thickness of the multilayer increased, the\\n tends\\nto depend on\\n as shown in Fig. 3(b) and (c). This is prob-\\nably due to distribution of the anisotropy ﬁeld in the multilayer\\ncoming from inhomogeneity of the structure and/or increase of\\nroughness. In order to eliminate such extrinsic contributions to\\n, we extrapolated the\\n dependence of\\n to\\n ,\\neven though slightly large error bars are estimated compared to\\nthose of thin multilayers.\\nFig. 4 shows the dependence of\\n on the total thickness of\\nthe Co/Ni multilayer. The multilayers have the bilayer period of\\n0.8 or 1.0 nm, and the total thickness was varied by changing\\nthe repetition number\\n . The '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cea5452d-4378-4431-a367-7c811cee3e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>> You are a helpful assistant, Read the following text. Does it mention the Gilbert damping constant of a certain material? If so, list the corresponding material and its Gilbert damping canstant.<</SYS>>\n",
      " 3036 IEEE TRANSACTIONS ON MAGNETICS, VOL. 47, NO. 10, OCTOBER 2011\n",
      "Time-Resolved Magnetization Dynamics and Damping Constant of\n",
      "Sputtered Co/Ni Multilayers\n",
      "T. Kato/49, Y . Matsumoto/49, S. Okamoto/50, N. Kikuchi/50, O. Kitakami/50, N. Nishizawa/51, S. Tsunashima/52, and\n",
      "S. Iwata/49\n",
      "Department of Quantum Engineering, Nagoya University, Nagoya 464-8603, Japan\n",
      "Institute of Multidisciplinary Research for Advanced Materials, Tohoku University, Sendai 980-8577, Japan\n",
      "Department of Electrical Engineering and Computer Science, Nagoya University, Nagoya 464-8603, Japan\n",
      "Department of Research, Nagoya Industrial Science Research Institute, Nagoya 464-0819, Japan\n",
      "Co/Ni multilayers with a stack of Ta (2 nm)/[Co /40\n",
      " /67/111 /41/Ni /40\n",
      " /78/105 /41]\n",
      "/Ta (30 nm) were prepared by dc magnetron sputtering, and their\n",
      "magnetization dynamics were measured by time-resolved magneto-optical Kerr effect (TRMOKE). The total thickness of the multilayerand perpendicular anisotropy were varied by changing the bilayer period\n",
      "/61\n",
      " /67/111 /43\n",
      " /78/105and number of repeats\n",
      " while\n",
      " /78/105\n",
      " /67/111was\n",
      "kept at a constant of 2.5. The TRMOKE measurements show clear damped oscillation of the magnetization of Co/Ni multilayers after\n",
      "the pump pulse illumination, and the damping constant\n",
      " of the Co/Ni multilayers was estimated from the TRMOKE waveform. The\n",
      "estimated\n",
      " was found to be independent both on total thickness and anisotropy ﬁeld of the multilayer and was estimated to be\n",
      " /48\n",
      " /48/51/53\n",
      "for all the multilayers. This means that the use of Ta capping and buffer layers is effective to evaluate intrinsic damping constant of theCo/Ni multilayer, and that independent control of\n",
      "and perpendicular anisotropy are possible for the magnetic multilayers.\n",
      "Index Terms— Co/Ni multilayer, Gilbert damping, magnetization dynamics, time-resolved magneto-optical Kerr effect (TRMOKE).\n",
      "I. I NTRODUCTION\n",
      "MAGNETIC ﬁlms exhibiting a large perpendicular mag-\n",
      "netic anisotropy (PMA) are quite attractive for further\n",
      "increase of the densities of magnetic recording media [1] and\n",
      "magnetic random access memories [2]. Typical materials aremagnetic multilayers such as Co/Pt [3], [4], Co/Pd [5], andCo/Ni [6], whose interface structures are known to contribute\n",
      "to the large PMA. The magnetic properties and structures for\n",
      "such multilayers have been extensively studied; however, theirmagnetization dynamics are poorly understood [7]–[9]. Anunderstanding of the magnetization dynamics is of particular\n",
      "importance since the dynamics determines the high-speed\n",
      "response of magnetic sensors and hard disk drives. Moreover,the magnetization dynamics is a key parameter for the efﬁcientwriting on the microwave-assisted magnetic recording [10] and\n",
      "current-induced magnetization switching (CIMS) [11].\n",
      "The Co/Ni multilayer is reported to be effective to reduce\n",
      "threshold currents on CIMS [2] and current-induced domainwall motion [12]. This is probably due to a tunable PMA and\n",
      "a low Gilbert damping constant\n",
      "in Landau–Lifshitz–Gilbert\n",
      "(LLG) equation. However, only a few papers have been reportedon the magnetization dynamics of the Co/Ni multilayer [9], [13],\n",
      "[14], and the relationship between its layered structure and the\n",
      "damping constant is still unclear. It is generally believed thatthe spin–orbit interaction, which couples the spin to the lattice,plays a dominant role in the damping mechanism [15]. In ad-\n",
      "dition, the damping constant is enhanced by the effect of spin\n",
      "pumping [16], i.e., spin relaxation of the conduction electronsat the interface between ferromagnetic and normal metals. Thespin–orbit coupling is also believed to play an important role\n",
      "on the PMA [17], and the PMA is easily modiﬁed by varying\n",
      "Manuscript received February 21, 2011; revised April 26, 2011; accepted\n",
      "May 22, 2011. Date of current version September 23, 2011. Corresponding au-\n",
      "thor: T. Kato (e-mail: takeshik@nuee.nagoya-u.ac.jp).\n",
      "Digital Object Identiﬁer 10.1109/TMAG.2011.2158082the layered structure of Co/Ni [6]. In this report, we studied\n",
      "magnetization dynamics of Co/Ni multilayers with various lay-\n",
      "ered structures by time-resolved magneto-optical Kerr effect(TRMOKE), and damping constants\n",
      "of the Co/Ni multilayers\n",
      "were evaluated. In order to study the intrinsic damping constant\n",
      "of the Co/Ni multilayer, we measured TRMOKE spectra ap-\n",
      "plying various external ﬁelds, and we used Ta buffer and cappinglayers since the Ta is considered to have little effect to enhancethe damping constant through the spin pumping effect [16], [18].\n",
      "II. S\n",
      "AMPLE PREPARATION AND MEASUREMENTS\n",
      "Co/Ni multilayers with a stack of Ta (2 nm)/[Co\n",
      "/Ni\n",
      " ]\n",
      "/Ta (30 nm)/thermally oxidized Si sub-\n",
      "strate were prepared by a dc magnetron sputtering system,where\n",
      "was kept constant at 2.5. The bilayer period\n",
      "was varied from 0.8 to 1.3 nm, and the total\n",
      "thickness of the multilayer was also varied from 6.4 to 20nm. Before the TRMOKE measurements, SiN (140 nm) layerwas additionally deposited on the multilayers by an RF mag-\n",
      "netron sputtering to enhance the Kerr rotation at a wavelength\n",
      "nm (wavelength of the ultrashort pulse ﬁber laser for\n",
      "TRMOKE measurements). Hysteresis loops were measured byan alternating gradient ﬁeld magnetometer. The loops in the\n",
      "perpendicular direction were also measured by using anoma-\n",
      "lous Hall effect and polar Kerr effect. For the Hall and Kerrloops, van der Pauw method and polarized angle modulationmethod were used, respectively.\n",
      "TRMOKE spectra were measured by pump-probe method\n",
      "using high-power ﬁber laser with\n",
      "nm, a pulse width\n",
      "of 1 ps, a maximum energy of 2\n",
      " J/pulse, and a repetition fre-\n",
      "quency of 200 kHz [19]. The pump and probe beams were fo-\n",
      "cused onto Co/Ni multilayers with diameters of 60 and 15\n",
      " m\n",
      ", respectively. The incident probe beam was normal to the ﬁlm\n",
      "surface and the polarization of the reﬂected probe beam was an-\n",
      "alyzed to monitor the perpendicular component of the magne-\n",
      "tization\n",
      " after the illumination of the pump beam. Typical\n",
      "0018-9464/$26.00 © 2011 IEEEKATO et al. : TIME-RESOLVED MAGNETIZATION DYNAMICS AND DAMPING CONSTANT OF SPUTTERED CO/NI MULTILAYERS 3037\n",
      "Fig. 1. /77/72 loops of Co/Ni multilayers with different bilayer period /100: (a) 1.0\n",
      "nm, (b) 1.1 nm, (c) 1.2 nm, and (d) 1.3 nm. The loops were measured applying amagnetic ﬁeld perpendicular (solid lines) and parallel (dashed lines) to the ﬁlm\n",
      "plane.\n",
      "ﬂuences of the pump and probe beams were 4 and 0.3 mJ/cm\n",
      " ,\n",
      "respectively. During the measurements, an external ﬁeld\n",
      "up to 3.5 kOe was applied in the direction of\n",
      " from the ﬁlm\n",
      "normal to excite coherent precession after the pump beam irra-\n",
      "diation. The TRMOKE spectra were analyzed by a numerical\n",
      "simulation [19] based on the LLG to estimate the damping con-stant\n",
      "and anisotropy ﬁeld\n",
      " , assuming\n",
      " -factor equals to 2.2.\n",
      "The\n",
      " of the Co/Ni multilayer was also evaluated from the\n",
      "loop and generalized Sucksmith–Thompson method [20].\n",
      "For the\n",
      " loop, the effective anisotropy\n",
      " was estimated\n",
      "from the area surrounded by perpendicular and in-plane\n",
      "loops, and the\n",
      " was simply calculated as\n",
      " , where\n",
      "is the saturation magnetization. The\n",
      " estimated by the\n",
      "three different ways agreed within 20%.\n",
      "III. R ESULTS AND DISCUSSIONS\n",
      "Fig. 1 shows\n",
      " loops of Co/Ni multilayers with different\n",
      "bilayer period\n",
      " measured applying a magnetic ﬁeld along per-\n",
      "pendicular (solid lines) and in-plane (dashed lines). For\n",
      "nm, the easy axis of the magnetization was parallel to the ﬁlmnormal direction indicating the large perpendicular anisotropy\n",
      "of the multilayer [see Fig. 1(a)]. The anisotropy ﬁeld\n",
      "of this\n",
      "multilayer was estimated to be about 1.8 kOe. The\n",
      " is esti-\n",
      "mated from an effective anisotropy\n",
      " , so that it includes both\n",
      "perpendicular anisotropy and shape anisotropy. The\n",
      " was\n",
      "reduced by increasing the bilayer period, i.e., by reducing thenumber of interfaces per unit thickness, and became negative at\n",
      "nm. The\n",
      " of all the multilayers were almost con-\n",
      "stant indicating a constant shape anisotropy,\n",
      " . Thus, the\n",
      "reduction of\n",
      " with increasing\n",
      " is due to the reduction of the\n",
      "perpendicular anisotropy.\n",
      "Fig. 2 shows typical TRMOKE waveforms of the Co/Ni multi-\n",
      "layer with\n",
      " nm and\n",
      " measured at various\n",
      " .\n",
      "Fig. 2. TRMOKE waveforms of Co/Ni multilayer with bilayer period of /100 /61\n",
      "/48 /58 /56nm and number of repeats /78 /61/49 /48 . The signals of laser-induced demag-\n",
      "netization and exponential decay during the recovery of the perpendicular com-\n",
      "ponent of magnetization /77\n",
      "in raw data were subtracted to draw the ﬁgure.\n",
      "The signals of laser-induced demagnetization and exponential\n",
      "decay during the recovery of\n",
      " in raw data [21] were subtracted\n",
      "to draw the ﬁgure. The decaying precessional signals triggered\n",
      "by the pump illumination were clearly seen in Fig. 2, and the pre-\n",
      "cessional frequency increased with increasing the applied ﬁeld.The waveforms were analyzed using LLG simulation [19], wherewe assumed macro spin precession with a\n",
      "-factor of 2.2. The\n",
      "damping constant\n",
      " and anisotropy ﬁeld\n",
      " were varied as ﬁt-\n",
      "ting parameters to reproduce the experimental results. The sim-ulated waveforms are shown as solid lines in the ﬁgure, whichcoincided well with the experiments as shown in Fig. 2.\n",
      "Fig. 3 shows the dependence of the estimated\n",
      "on\n",
      " .\n",
      "for the Co/Ni multilayers with (a)\n",
      " and\n",
      " , (b)\n",
      "and\n",
      " , and (c)\n",
      " and\n",
      " .F o r\n",
      "the thin sample [see Fig. 3(a)], the\n",
      " was almost independent\n",
      "on\n",
      " and is estimated to be 0.035 for this sample. However,\n",
      "when the total thickness of the multilayer increased, the\n",
      " tends\n",
      "to depend on\n",
      " as shown in Fig. 3(b) and (c). This is prob-\n",
      "ably due to distribution of the anisotropy ﬁeld in the multilayer\n",
      "coming from inhomogeneity of the structure and/or increase of\n",
      "roughness. In order to eliminate such extrinsic contributions to\n",
      ", we extrapolated the\n",
      " dependence of\n",
      " to\n",
      " ,\n",
      "even though slightly large error bars are estimated compared to\n",
      "those of thin multilayers.\n",
      "Fig. 4 shows the dependence of\n",
      " on the total thickness of\n",
      "the Co/Ni multilayer. The multilayers have the bilayer period of\n",
      "0.8 or 1.0 nm, and the total thickness was varied by changing\n",
      "the repetition number\n",
      " . The  [/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(content):\n",
    "    begin = \"<s>[INST]\"\n",
    "    #syst = \"<<SYS>> You are a helpful assistant, always answer as helpfully as possible.\\n If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.<</SYS>>\\n\"\n",
    "    #inst = \"Read the following text. Does it mention the Gilbert damping constant of a certain material? If so, list the corresponding material and its Gilbert damping canstant.\\n\" + content\n",
    "    syst = \"<<SYS>> You are a helpful assistant, Read the following text. Does it mention the Gilbert damping constant of a certain material? If so, list the corresponding material and its Gilbert damping canstant.<</SYS>>\\n\"\n",
    "    inst = content\n",
    "    end = \"[/INST]\\n\"\n",
    "    prompt = (\" \").join([begin, syst, inst, end])\n",
    "    return prompt\n",
    "\n",
    "print(generate_prompt(data[0]['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b9603c1-fd1b-47df-b6e3-33bec5234737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible.\n",
      " If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. <</SYS>>\n",
      " Summarize the material MnCoFe_2O_4 in detail based on the following paragraphs. Do not answer other materials, and do not include any begining, just start with the summarize of MnCoFe_2O_4. Your audience is not interested in the article, so don’t mention the article in your answer, but distill the content from the article.\n",
      " An application study of MnCoFe_2O_4@PEG-4000 (MCF@PEG-4000) MNPs as a catalyst in the synthesis of hexahydroquinolines and benzopyrans and as an nano-adsorbent for the removal of Cu (II) and Fe (III) ions in aqueous solutions\n",
      " The novel MnCoFe_2O_4@PEG-4000 was synthesized by a simple and clean method. These nanoparticles have an almost spherical structure with an average diameter of about 21.95 nm, a saturation magnetization (Ms) of about 26.31 emu/g, and a thermal stability of about 100 °C. The application of these nanoparticles as a reusable green catalyst in the synthesis of hexahydroquinolines and benzopyrans and also as a nano-adsorbent for the removal of copper (II) (Cu (II)) and iron (III) (Fe (III)) metal ions in aqueous solutions was investigated. The use of a relatively green solvent, short reaction times, and high yield of products under optimal conditions are some of the advantages of these synthetic procedures for the preparation of hexahydroquinolines and benzopyrans in the presence of MCF@PEG-4000 MNPs as a catalyst. The adsorptive removal progress was optimized at pH = 7, the adsorbent dosage of 10 mg, and a contact time of 30 min. For the removal of both ions, the use of natural pH (7) is an important advantage of this work. The adsorption capacity (qe) of MnCoFe_2O_4@PEG-4000 MNPs for copper (II) and iron (III) was found to be about 149.9 mg/g and 599.67 mg/g, respectively, according to the second-order pseudo-models. Also, it was found that MCF coated with PEG is a better adsorbent for the adsorptive removal of metal ions than MCF due to the cover of PEG.\n",
      " The green synthesis of environmentally friendly magnetic silver complex stabilized on MnCoFe_2O_4@sodium alginate nanoparticles (MCF@S-ALG/Ag) and evaluation of their antibacterial activity\n",
      " Magnetic nanoparticles with green cover sodium alginate and Ag, MnCoFe_2O_4@Sodium alginate/Ag (MCF@S-ALG/Ag) MNPs were prepared by a simple and clean method from Sargassum Vulgare brown algae. The structure of these nanoparticles was characterized by the Fourier transform infrared (FT-IR), X-ray powder diffraction (XRD), field emission-scanning electron microscope (FE-SEM), energy-dispersive X-ray spectroscopy (EDX), thermogravimetric analysis (TGA), and vibrating sample magnetometer (VSM). Furthermore, the antibacterial activity of MCF@S-ALG/Ag MNPs was tested for two bacteria of gram-negative ( Escherichia coli ( E. coli )) bacteria and gram-positive ( Staphylococcus aureus ( S. aureus )) bacteria. The MCF@S-ALG/Ag MNPs showed the inhibition zone 16.32 mm for S. aureus and 12.84 mm for E. coli bacteria. The minimal inhibitory concentration (MIC) and the minimal bactericidal concentration (MBC) of MCF@S-ALG/Ag MNPs for both bacteria were found 20 µg/mL and 40 µg/mL, respectively. [/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt():\n",
    "    begin = \"<s>[INST] <<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible.\\n If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. <</SYS>>\\n\"\n",
    "    instruction = \"Summarize the material MnCoFe_2O_4 in detail based on the following paragraphs. Do not answer other materials, and do not include any begining, just start with the summarize of MnCoFe_2O_4. Your audience is not interested in the article, so don’t mention the article in your answer, but distill the content from the article.\\n\"\n",
    "    a = \"An application study of MnCoFe_2O_4@PEG-4000 (MCF@PEG-4000) MNPs as a catalyst in the synthesis of hexahydroquinolines and benzopyrans and as an nano-adsorbent for the removal of Cu (II) and Fe (III) ions in aqueous solutions\\n\"\n",
    "    b = \"The novel MnCoFe_2O_4@PEG-4000 was synthesized by a simple and clean method. These nanoparticles have an almost spherical structure with an average diameter of about 21.95 nm, a saturation magnetization (Ms) of about 26.31 emu/g, and a thermal stability of about 100 °C. The application of these nanoparticles as a reusable green catalyst in the synthesis of hexahydroquinolines and benzopyrans and also as a nano-adsorbent for the removal of copper (II) (Cu (II)) and iron (III) (Fe (III)) metal ions in aqueous solutions was investigated. The use of a relatively green solvent, short reaction times, and high yield of products under optimal conditions are some of the advantages of these synthetic procedures for the preparation of hexahydroquinolines and benzopyrans in the presence of MCF@PEG-4000 MNPs as a catalyst. The adsorptive removal progress was optimized at pH = 7, the adsorbent dosage of 10 mg, and a contact time of 30 min. For the removal of both ions, the use of natural pH (7) is an important advantage of this work. The adsorption capacity (qe) of MnCoFe_2O_4@PEG-4000 MNPs for copper (II) and iron (III) was found to be about 149.9 mg/g and 599.67 mg/g, respectively, according to the second-order pseudo-models. Also, it was found that MCF coated with PEG is a better adsorbent for the adsorptive removal of metal ions than MCF due to the cover of PEG.\\n\"\n",
    "    c = \"The green synthesis of environmentally friendly magnetic silver complex stabilized on MnCoFe_2O_4@sodium alginate nanoparticles (MCF@S-ALG/Ag) and evaluation of their antibacterial activity\\n\"\n",
    "    d = \"Magnetic nanoparticles with green cover sodium alginate and Ag, MnCoFe_2O_4@Sodium alginate/Ag (MCF@S-ALG/Ag) MNPs were prepared by a simple and clean method from Sargassum Vulgare brown algae. The structure of these nanoparticles was characterized by the Fourier transform infrared (FT-IR), X-ray powder diffraction (XRD), field emission-scanning electron microscope (FE-SEM), energy-dispersive X-ray spectroscopy (EDX), thermogravimetric analysis (TGA), and vibrating sample magnetometer (VSM). Furthermore, the antibacterial activity of MCF@S-ALG/Ag MNPs was tested for two bacteria of gram-negative ( Escherichia coli ( E. coli )) bacteria and gram-positive ( Staphylococcus aureus ( S. aureus )) bacteria. The MCF@S-ALG/Ag MNPs showed the inhibition zone 16.32 mm for S. aureus and 12.84 mm for E. coli bacteria. The minimal inhibitory concentration (MIC) and the minimal bactericidal concentration (MBC) of MCF@S-ALG/Ag MNPs for both bacteria were found 20 µg/mL and 40 µg/mL, respectively.\"\n",
    "    e = \"[/INST]\\n\"\n",
    "    prompt = (\" \").join([begin, instruction, a, b, c, d, e])\n",
    "    return prompt\n",
    "\n",
    "print(generate_prompt())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c00bdbd1-cbb4-40cf-b95e-f5722de31bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>> You are a helpful assistant, always answer as helpfully as possible.\n",
      " If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.<</SYS>>\n",
      " Read the following text. Does it mention the Gilbert damping constant of a certain material? If so, list the corresponding material and its Gilbert damping canstant.\n",
      "Accepted Manuscript\n",
      "The effect of growth sequence on magnetization damping in Ta/Co FeB/MgO\n",
      "structures\n",
      "Bo Liu, Dawei Huang, Ming Gao, Hongqing Tu, Kejie Wang, Xuezhon g Ruan,\n",
      "Jun Du, Jian-Wang Cai, Liang He, Jing Wu, Xinran Wang, Yongbing  Xu\n",
      "PII: S0304-8853(17)31838-3\n",
      "DOI: http://dx.doi.org/10.1016/j.jmmm.2017.08.069\n",
      "Reference: MAGMA 63103\n",
      "To appear in: Journal of Magnetism and Magnetic Materials\n",
      "Received Date: 15 June 2017\n",
      "Revised Date: 22 August 2017\n",
      "Accepted Date: 23 August 2017\n",
      "Please cite this article as: B. Liu, D. Huang, M. Gao, H. Tu, K . Wang, X. Ruan, J. Du, J-W. Cai, L. He, J. Wu, X.\n",
      "Wang, Y. Xu, The effect of growth  sequence on magnetization dam ping in Ta/CoFeB/MgO structures, Journal of\n",
      "Magnetism and Magnetic Materials  (2017), doi: http://dx.doi.org/10.1016/j.jmmm.2017.08.069\n",
      "This is a PDF file of an unedited manuscript that has been acce pted for publication. As a service to our customers\n",
      "we are providing this early version of the manuscript. The manu script will undergo copyediting, typesetting, and\n",
      "review of the resulting proof be fore it is published in its fin al form. Please note that during the production process\n",
      "errors may be discovered which could affect the content, and al l legal disclaimers that app ly to the journal pertain.  The effect of growth sequence on magnetization damp ing in \n",
      "Ta/CoFeB/MgO structures \n",
      " \n",
      "Bo Liu,a† Dawei Huang,a† Ming Gao,a Hongqing Tu,b Kejie Wang,a Xuezhong \n",
      "Ruan,a,* Jun Du,b Jian-Wang Cai,c,* Liang He,a Jing Wu,d Xinran Wang,a and \n",
      "Yongbing Xua,d,* \n",
      " \n",
      "aJiangsu Provincial Key Laboratory of Nanotechnology , Collaborative Innovation \n",
      "Center of Advanced Microstructures, School of Elect ronic Science and Engineering, \n",
      "Nanjing University, Nanjing 210093, People’s Republ ic of China \n",
      "bDepartment of Physics, Nanjing University, Nanjing 210093, People’s Republic of \n",
      "China \n",
      "cBeijing National Laboratory for Condensed Matter Ph ysics, Institute of Physics, \n",
      "Chinese Academy of Sciences \n",
      "dYork-Nanjing International Center in Spintronics (Y NICS), Department of \n",
      "Electronics and Physics, The University of York, Yo rk YO10 5DD, UK \n",
      " \n",
      "E-mail: xzruan@nju.edu.cn, jwcai@aphy.iphy.ac.cn an d ybxu@nju.edu.cn \n",
      " \n",
      "Keywords: reversed stack structures, perpendicular magnetic a nisotropy , Gilbert \n",
      "damping constant \n",
      " \n",
      " \n",
      " \n",
      "   Abstract: \n",
      "Magnetization damping is a key parameter to control  the critical current and the \n",
      "switching speed in magnetic random access memory, a nd here we report the effect of \n",
      "the growth sequence on the magnetic dynamics proper ties of perpendicularly \n",
      "magnetized Ta/CoFeB/MgO structures. Ultrathin CoFeB  films have been grown \n",
      "between Ta and MgO but with different stack sequenc es, i.e. \n",
      "substrate/Ta/CoFeB/MgO/Ta and substrate/Ta/MgO/CoFe B/Ta. The magnetization \n",
      "dynamics induced by femtosecond laser was investiga ted by using all-optical \n",
      "pump-probe measurements. We found that the Gilbert damping constant was \n",
      "modulated by reversing stack structures, which offe rs the potential to tune the \n",
      "damping parameter by the growth sequence. The Gilbe rt damping constant was \n",
      "enhanced from 0.017 for substrate/Ta/CoFeB/MgO/Ta t o 0.027 for \n",
      "substrate/Ta/MgO/CoFeB/Ta. We believe that this enh ancement originates from the \n",
      "increase of intermixing at the CoFeB/Ta when the Ta  atom layer was grown after the \n",
      "CoFeB layer. \n",
      " \n",
      "1. Introduction \n",
      "Current-induced spin-transfer torque (STT) is very important in high density \n",
      "magnetic media and spintronics devices[1-5]. Regard ing the energy consumption, it is \n",
      "critcal to find out the method to reduce the critic al current that switches the spin \n",
      "direction in STT-MRAM application. The perpendicula r magnetic anisotropy \n",
      "materials, such as L 10-ordered Fe -Pt alloys and Co/Pd multilayers integrated into    magnetic tunnel junction (MTJ) [6,7] , enable a small critical current density for \n",
      "current -induced magnetization switching [8-11]. According to S. Mangin et al[6], the \n",
      "current density /g1836/g3004/g°868 is proportional to the Gilbert damping constant α and effective \n",
      "magnetic anisotropic energy /g1837/g3048/g303°/g3033/g3033. Therefore, either decreasing the damping values \n",
      "or magnetic anisotropy energy is required to reduce  the critical current. However, \n",
      "considering the thermal stability of the MTJ device s, we cannot decrease the \n",
      "anisotropy energy too much. Thus, it is a better ch oice to reduce the damping constant \n",
      "and at the same time, relatively high magnetic anis otropy energy is preferable. The \n",
      "high perpendicular magnetic anisotropy materials su ch as L10-FePt and Co/Pt \n",
      "multilayers[7-10] that are integrated into the magn etic tunnel junction (MTJ) [11,12] \n",
      "are reported to both have low damping constant and relative high thermal stability. \n",
      "These materials are found to be in the good balance  of thermal stability and low \n",
      "damping constant. Furthermore, with the decrease of  the size of magnetic tunnel \n",
      "junction, the low power consumption denoted by ther mal sta bility factor ∆, which is \n",
      "proportional to the /g1837/g3048/g303°/g3033/g3033, is an indispensable item. Increasing /g1837/g3048/g303°/g3033/g3033 to keep ∆ \n",
      "sufficiently large is crucial. The optimization of the Gilbert damping constant α is \n",
      "critial to achieve a higher thermal stability and a  lower threshold current density [13]. \n",
      "A major breakthrough in MRAM is the discovery of th e perpendicularly magnetized \n",
      "CoFeB films sandwiched by MgO and Ta layers, which exhibit not only \n",
      "perpendicular magnetic anisotropy, but also moderat e magnetic damping \n",
      "constant[14,15]. \n",
      "In this paper, we report the effect of the stack gr owth sequence on the Gilbert   damping constants in the Ta/CoFeB/MgO structures gr own by sputtering. A \n",
      "time-resolved optical pump-probe technique was used  to investigate the precessional \n",
      "dynamics and the magnetization damping. We have obs erved a remarkable difference \n",
      "of the damping constants between the samples grown with different stack sequences. \n",
      "The experimental results clearly demonstrate that s tack growth sequence plays a key \n",
      "role in controlling damping values. . \n",
      " \n",
      "2. Methods \n",
      "We prepared two sets of samples: substrate/Ta (5)/C oFeB (1)/MgO (3)/Ta (5) \n",
      "named as sample A and substrate/Ta (5)/MgO (3)/CoFe B (1)/Ta (5) named as sample \n",
      "B (numbers are thickness in nanometers). The cappin g layer and Co 40Fe40B20 films \n",
      "were deposited on Si (001)/SiO 2 substrates by dc sputtering, whereas the MgO layer  \n",
      "was deposited by rf sputtering. The background vacu um was about 4×10/g°87−/g°873 Pa and \n",
      "the working argon pressure was 0.5 Pa. After the th in film was deposited, a \n",
      "post-annealing process at 300 ℃ in vacuum ( 4×10/g°87−/g°873 Pa) was performed for half \n",
      "an hour on both of the samples. Static magnetic pro perties were obtained by using a \n",
      "vibrating sample magnetometer(VSM). The surface mor phology and roughness were \n",
      "analyzed by atomic force microscopy (AFM). The femt osecond pulse train generated \n",
      "by a Ti: sapphire laser with a pulse duration of 50  fs and a repetition rate of 1 kHz \n",
      "was divided into pump and probe pulse beam. The pum p pulse beam with a fluence of \n",
      "3.54 mJ /g1855/g1865/g°870/uni°044  was focused to a spot of ~600 µm in diameter on the sample to excite \n",
      "the magnetization precession, while the probe pulse  beam with a fluence of 0.06   mJ /g1855/g1865/g°870/uni°044  was focused to a spot size of ~200 µm in diameter and overlapped with the \n",
      "pump laser spot on the sample surface [16-20]. The Kerr rotation of reflected probe \n",
      "pulse beam was detected by a balanced detector. A m echanical delay line was used to \n",
      "generate time delay between pump and probe pulse be ams. The magnetization \n",
      "dynamics was studied by time-resolved magneto-optic al Kerr effect (TRMOKE) \n",
      "measurements. A variable magnetic field was applied  at an angle of 30° from the \n",
      "sample plane resulting in a cant of the magnetizati on away from the normal of the \n",
      "film. All presented results were obtained at room t emperature unless otherwise \n",
      "specified. \n",
      " \n",
      "3. Results and discussion \n",
      "The magnetic hysteresis loops of sample A and sampl e B were measured with \n",
      "the in-plane and perpendicular magnetic fields usin g VSM and displayed in Fig.  1(a) \n",
      "and (b), respectively. The results display a simila r coercivity ( /g1834/g3030) and saturation \n",
      "magnetization ( /g183−/g3046) between two samples with  /g1834/g3030=5.31  Oe and /g183−/g3046=\n",
      "890/uni000−emu /g1855/g1865/g°871/uni°044  for sample A, /g1834/g3030=5.66  Oe and /g183−/g3046=836/uni000−emu /g1855/g1865/g°871/uni°044  for sample \n",
      "B. The Hc is defined along the easy axis that is ou t-of-plane [21-23]. \n",
      "The AFM images of the sample A and sample B are sho wn in Fig. 2(a) and (b), \n",
      "respectively. The root-mean-square (RMS) of the sur face roughness is about 0.83 nm \n",
      "for sample A and about 1.11 nm for sample B, respec tively, which suggests that the \n",
      "underlayer Ta below CoFeB in sample A might lead to  a reduction of the surface \n",
      "roughness. However, we found little difference of t he /g1837/g3048/g303°/g3033/g3033 defined by H/g°−°1M/g°−°−/2   between two samples with 1.24 Merg /g1855/g1865/g°871/uni°044  for sample A and 1.25 Merg /g1855/g1865/g°871/uni°044  for \n",
      "sample B, which suggests that the surface roughness  has not changed dramatically the \n",
      "static magnetic properties, such as the saturation magnetization and the effective \n",
      "anisotropy field. More specifically, the effective anisotropy field here mainly arises \n",
      "from the interfacial perpendicular anisotropy [8]. \n",
      "In pump-probe measurements, the pump laser pulse cr eated a thermal excitation \n",
      "of the electron/spin system leading to an ultrafast  demagnetization. The magnetic \n",
      "order started to recover as electron/spin-transferr ed energy to lattice and reached a \n",
      "new electron/lattice/spin equilibrium within a few picoseconds. As the heat further \n",
      "diffused into the surrounding area and down to the substrate, the magnetization was \n",
      "partially recovered. Following the recovery of the magnetization, the magnetic \n",
      "anisotropy was also recovered, leading to a rise of  [/INST]\n",
      "The article discusses the effect of growth sequence on the Gilbert damping constant in Ta/CoFeB/MgO structures. The authors find that the Gilbert damping constant is modulated by reversing the stack sequence, with an enhancement from 0.017 to 0.027. They attribute this increase to the increase of intermixing at the CoFeB/Ta interface due to the growth of Ta after CoFeB deposition. The study demonstrates the importance of understanding the effects of growth sequence on magnetic properties in perpendicularly magnetized materials for improving their\n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt(data[4]['content'])\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "  generation_output = model.generate(\n",
    "      input_ids=input_tokens,\n",
    "      max_new_tokens=128,\n",
    "      do_sample=True,\n",
    "      top_k=10,\n",
    "      top_p=0.9,\n",
    "      temperature=0.3,\n",
    "      repetition_penalty=1.15,\n",
    "      num_return_sequences=1,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7527e3-985f-40ad-a81a-3c2e5f8b5c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The article mentions the Gilbert damping constant of the Co/Ni multilayers, which is estimated to be around 0.035.\n",
      "\n",
      "The article mentions the Gilbert damping constant of a certain material, specifically Pt, in the context of spintronic devices. The authors of the article conducted experiments to measure the Gilbert damping constant of Pt/Py/Pt trilayers using ferromagnetic resonance (FMR) measurements. They found that the Gilbert damping constant increases with decreasing Py thickness and varies among different nonmagnetic metals. Specifically, they found that the Gilbert damping constant of Pt is significantly higher than that of other nonmagnetic metals, such as Ta, Cu,\n",
      "\n",
      "The article discusses the Gilbert damping constant of layered magnetic films deposited on a substrate. The authors measure the frequency shift of the films using a ferromagnetic resonance (FMR) technique and find that it is related to the interface spin-pumping effect. They also analyze the X-ray diffraction patterns of the films and find that the crystal structure of the Py layer adjacent to the Pt layer is modified, leading to an increased Gilbert damping constant. The authors compare their results with previous reports and find that the Gilbert damping constant varies for different transition metal (NM\n",
      "\n",
      "10. Magnet. 10003. 108. Magnet . Magnet Magnet Magnet Magnet Ћ08.\n",
      "10 .\n",
      "10 Љ10008.\n",
      "10 Ъ Magnet Magnet Ћ Љ Ћ08 Ћ Љ Љ Љ Ъ Magnet Magnet Magnet Magnet Ъ00 Џ00 Ћ Љ Ћ Ћ Љ Љ Љ Ъ Љ Љ Љ Љ Љ Љ Љ Ъ Љ\n",
      "\n",
      "The article discusses the effect of growth sequence on the magnetization damping constant in Ta/CoFeB/MgO structures. The authors found that the Gilbert damping constant was modulated by reversing the stack sequence, with an enhancement from 0.017 to 0.027. They attribute this increase to the increase of intermixing at the CoFeB/Ta interface when the Ta atom layer was grown after the CoFeB layer. The study demonstrates the potential to tune the damping parameter by the growth sequence, which is crucial for reducing the\n",
      "\n",
      "\n",
      "Note: The article is about the study of the Gilbert damping constant in CoFeB/Ta/MgO thin films. The authors measure the precession of the magnetization after applying an AC magnetic field and observe the dependence of the damping constant on the magnetic field. They find that the intrinsic Gilbert damping constant is around 0.017, which is consistent with previous reports. The article also discusses the enhancement of the damping by the Ta/CoFeB/MgO interface by spin pumping.\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " 0.0.0.0. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "The article mentions the Gilbert damping constant of FePt like hard magnetic recording media, which is used in heat-assisted magnetic recording (HAMR). The Gilbert damping constant is listed in the table provided in the article, with values ranging from α0.01 to α0.5.\n",
      "\n",
      "The material is FePt, and the Gilbert damping constant is α = 0.02.\n",
      "The grain size is 5 nm and 7 nm.\n",
      "The reader resolution is 44.34 nm.\n",
      "The SNR depends on the damping constant and the bit length.\n",
      "Increasing the damping constant from α = 0.02 to α = 0.1 leads to a 2.25 dB improvement in SNR for 5 nm grains and a 0.72 dB improvement for 7 n\n",
      "\n",
      "The article discusses the Gilbert damping constant of a particular material used in heat-assisted magnetic recording (HAMR). The Gilbert damping constant is a measure of the energy dissipation rate of a magnetic material due to thermal fluctuations. The study aimed to determine the effect of the Gilbert damping constant on the signal-to-noise ratio (SNR) of HAMR.\n",
      "\n",
      "The researchers used a numerical model to simulate the HAMR process and analyzed the impact of different Gilbert damping constants on the SNR. They found that increasing the Gilbert d\n",
      "\n",
      "The text does not mention the Gilbert damping constant of any specific material. However, there are several references to studies that have investigated the effect of doping or other factors on the Gilbert damping constant in various magnetic materials:\n",
      "\n",
      "* In Ref. [26], the authors study the effect of dilute Gd doping on the Gilbert damping constant in soft magnetic Fe thin films. They find that the Gilbert damping constant decreases with increasing Gd concentration.\n",
      "* In Ref. [27], the authors investigate the effect of tunable magnetization damping in transition metal tern\n",
      "\n",
      "Yes, the text mentions the Gilbert damping constant of Co2MnSi. According to the text, the Gilbert damping constant of the material is approximately 0.0008.\n",
      "\n",
      "The article mentions the Gilbert damping constant of the material CoFeB, but does not provide a specific value for the constant. The Gilbert damping constant is a material property that describes the energy loss of a magnetic moment in a magnetic field, and it is typically expressed in units of erg/cm^3.\n",
      "\n",
      "According to the article, the Gilbert damping constant of CoFeB is lower than that of CoFeB/Ta, which suggests that CoFeB has a lower damping rate than CoFeB/Ta. However, the exact value of the Gilbert damping constant\n",
      "\n",
      "The Gilbert damping constant of the material is not mentioned directly in the text. However, the article does provide some information about the damping properties of the material.\n",
      "\n",
      "In the text, the authors discuss the effect of the damping parameter α on the switching characteristics of a hard/soft composite structure. They study the effect of α on the switching times for different exchange coupling energies (IEC) using micromagnetic simulations. The authors find that the effect of α on the switching characteristics is significant only for the strong IEC case, and they conclude that alternative hard layers and insertion elements with lower damp\n",
      "\n",
      "The article mentions the Gilbert damping constant of Nickel nanowires (NWs) multiple times throughout the text. The Gilbert damping constant, also known as α, is a parameter that describes the energy loss of a magnetization precessing around a magnetic field. It is an important parameter in understanding the dynamic behavior of ferromagnetic materials, particularly in the gigahertz range.\n",
      "According to the text, the Gilbert damping constant of Nickel NWs varies with length, with values of α decreasing from shorter lengths and then saturating above 16 μm\n",
      "\n",
      "The Gilbert damping constant of the Ni NWs is not explicitly mentioned in the text, but it can be calculated based on the experimental data provided. The Gilbert damping constant (Gamma) is related to the quality factor (Q) of the resonator as follows:\n",
      "\n",
      "Gamma = 1 / Q\n",
      "\n",
      "where Q is defined as the ratio of the resonance frequency to the bandwidth of the resonance, i.e.,\n",
      "\n",
      "Q = omega / delta omega\n",
      "\n",
      "Using the experimental data provided in the text, we can calculate the Gilbert damping constant for the Ni NWs\n",
      "\n",
      "The material being studied is Nickel (Ni).\n",
      "The Gilbert damping constant of the Ni NWs is 0.35.\n",
      "The effective field of the Ni NWs is given by Heff = 2πMS(1 - 3P), where MS is the saturation magnetization and P is the polarization of the NWs.\n",
      "The resonance frequency of the Ni NWs is given by fres = γ/2πMS, where γ is the gyromagnetic ratio and M is the magnetization of the NWs.\n",
      "The frequency line\n",
      "\n",
      "The text mentions the Gilbert damping constant of a certain material, but does not specify which material it is. However, based on the context, it is likely that the material being referred to is nickel (Ni).\n",
      "\n",
      "The text provides a detailed analysis of the frequency linewidth of Ni nanowires (NWs) using the fluctuation mode method. It is observed that the frequency linewidth increases with the length of the NWs, and the effect of dc current on the frequency linewidth is also studied. The text also discusses the temperature dependence of the magnetic moments of Ni NW\n",
      "\n",
      "The article discusses the geometric size effect on the extrinsic Gilbert damping in laterally confined magnetic structures. The study used micromagnetic simulations to investigate the spin dynamics in micron-length scale patterned thin films. The effective Gilbert damping constant was found to depend on the size and shape of the pattern, as well as the externally applied magnetic field. Additionally, the simulation revealed that extrinsic damping generated around the edge region was attributed to the dephasing effect between the fundamental spin wave and other spin wave modes. The study suggests that the effect of extrinsic damping can be eliminated\n",
      "\n",
      "The material being studied is a micrometer-scale thin film, and the Gilbert damping constant is being referred to as αeff. The text states that the αeff value is 0.008 at l=5 μm, H=1 kOe, and that this value is consistent with the intrinsic value of 0.007. The article also mentions that the inhomogeneity of spin alignment is a significant factor in determining αeff, particularly at the edge of the sample. Additionally, the text notes that the additional spin wave modes at the edge region can cause extrinsic\n",
      "\n",
      "The text does not mention the Gilbert damping constant of any specific material. However, it does provide some information about the magnetic properties of Tb/Cr/Fe trilayers.\n",
      "According to the text, the saturation magnetization of Tb/Cr/Fe trilayers increases with the thickness of the Cr layer, and the magnetic moment of the Tb layer is smaller than the bulk value of Tb at 0 K. Additionally, the text mentions that the effective magnetic moment contribution of Fe and Tb layers does not change with the thickness of the inserted Cr layer, which suggests\n",
      "\n",
      "The material mentioned in the text is Tb(4 nm)/Cr( tnm)/Fe(5 nm). The Gilbert damping constant of this material is not explicitly mentioned in the text, but it can be inferred from the discussion of the angular dependence of the resonance field. According to the text, the effective magnetization of the trilayer is lower than that of the Fe film, indicating that the Tb layer also has magnetic moments. The intermixing of Cr and Fe during sputtering can also affect the magnetic moment of the trilayers. The author plans to measure the magnetic\n"
     ]
    }
   ],
   "source": [
    "for i in data:\n",
    "    input_prompt = generate_prompt(i['content'])\n",
    "    input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "    with torch.cuda.amp.autocast():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_tokens,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=True,\n",
    "            top_k=5,\n",
    "            top_p=0.9,\n",
    "            temperature=0.2,\n",
    "            repetition_penalty=1.1,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "    #print(op)\n",
    "    \n",
    "    inst_index = op.find('[/INST]')\n",
    "    \n",
    "    if inst_index != -1:\n",
    "        print(op[inst_index + len('[/INST]'):])\n",
    "    else:\n",
    "        print(\"未找到'[/INST]'标记\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6905283-ff58-466c-b354-a7edbd2062c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_prompt = generate_prompt()\n",
    "type(input_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d73243da-ca25-4bf5-9cd2-1c7551481895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible.\n",
      " If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. <</SYS>>\n",
      " There's a llama in my garden 😱 What should I do? [/INST]  Oh, wow! A llama in your garden? That's quite an unexpected surprise! 😂 While llamas can be fascinating creatures, they may not necessarily be the best guests in your garden. Here are some steps you could consider:\n",
      "\n",
      "1. Observe from a safe distance: Before doing anything, take a moment to observe the llama from a safe distance. Check if it's causing any damage or posing a threat to yourself or others.\n",
      "2. Contact local authorities: If the llama appears lost or injured, contact your local animal control agency or a nearby wildlife sanctuary for assistance. They will have the necessary expertise and equipment to safely handle the situation.\n",
      "3. Provide food and water: If the llama seems healthy and content, but just accidentally wandered into your garden, you could try providing it with some food and water. However, please ensure that the food you offer is safe and non-toxic for the llama to consume.\n",
      "4. Keep a safe distance: Remember, llamas are large animals and can be unpredictable at times. Keep a safe distance and avoid approaching the llama too closely, especially if it feels threatened or scared.\n",
      "5. Consider calling a professional: If you're unsure how to handle the situation or if the llama doesn't seem to want to leave on its own, consider hiring a professional animal control service to safely remove the llama from your property.\n",
      "\n",
      "Remember, it's important to prioritize safety above all else when dealing with unexpected visitors like llamas in your garden!\n"
     ]
    }
   ],
   "source": [
    "input_prompt = \"<s>[INST] <<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible.\\n If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. <</SYS>>\\n There's a llama in my garden 😱 What should I do? [/INST]\"\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "  generation_output = model.generate(\n",
    "      input_ids=input_tokens,\n",
    "      max_new_tokens=2000,\n",
    "      do_sample=True,\n",
    "      top_k=10,\n",
    "      top_p=0.9,\n",
    "      temperature=0.3,\n",
    "      repetition_penalty=1.15,\n",
    "      num_return_sequences=1,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc9148cc-49d4-4e74-bc10-1a8ef0788108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6571e92097dc47218013a0b68014f335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained('results/checkpoint-19500')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.load_adapter('results/checkpoint-19500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94f68fc9-962f-4f5f-8c74-739fb4e2dc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['title', 'doi', 'abstract', 'publicationDate'],\n",
      "    num_rows: 165071\n",
      "})\n",
      "Dataset({\n",
      "    features: ['title', 'abstract', 'publicationDate'],\n",
      "    num_rows: 559\n",
      "})\n",
      "The abstract of the paper:\n",
      " Inconel 625 sustainable milling surface integrity and the dependence on alloy processing route\n",
      " Abstract: The discovery of deepwater oil and gas sources has altered the scenario of world production of oil products, attracting even more attention to nickel superalloys. However, this class of materials can be used in several applications. Furthermore, nickel superalloys are highly dependent on their processing history, and the manner in which superalloys react to machining can directly affect the finished product. This work aims to evaluate the surface integrity of two different materials after cryogenic side-milling in conditions that stimulate severe plastic deformation (SPD) and high heat generation. The results show that the material response to machining depends strongly on the pre-processing route instead of most assumptions. While cryogenic cooling led to significant sub-surface hardness and microstructural changes in wrought Inconel 625 alloy, such changes were not observed for clad Inconel 625. Therefore, in order to achieve significant surface integrity changes, process parameters need to be selected and optimized accordingly. Also, the findings indicate that some new factors established significant affect/change surface integrity: (a) SPD through a high r_β/h ratio; (b) the specific pre-processing thermomechanical history of the workpiece material; and (c) and cryogenic cooling, by changing material properties, reducing temperature and altering cutting phenomena and chip formation. </s> \n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"enyuan/Abstracts\")\n",
    "data_train = data[\"train\"]\n",
    "\n",
    "custom_data = load_dataset('json', data_files='data_eval.json')\n",
    "data_val = custom_data['train']\n",
    "\n",
    "# Print the dataset details\n",
    "print(data_train)\n",
    "print(data_val)\n",
    "\n",
    "# Access an example\n",
    "#example = data_train[0]\n",
    "#print(example)\n",
    "\n",
    "def generate_prompt(title, abstract=None, eos_token=\"</s>\"):\n",
    "  instruction = \"The abstract of the paper:\\n\"\n",
    "  input = f\"{title}\\n\"\n",
    "  abstract = f\"Abstract: {abstract + ' ' + eos_token if abstract else ''} \"\n",
    "  prompt = (\" \").join([instruction, input, abstract])\n",
    "  return prompt\n",
    "\n",
    "print(generate_prompt(data_train[0][\"title\"], data_train[0][\"abstract\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "347f25a1-b25f-40a6-9f81-4c0bdcfe8e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The abstract of the paper:\n",
      " Effect of cryogenic cooling on residual stresses and surface finish of 316L during hybrid manufacturing\n",
      " Abstract:   In this work, a novel approach for reducing the residual stress in the welded joints of stainless steel is presented. A new process called Hybrid Manufacturing (HM) was developed to reduce the residual stress in the welded joints by using two different techniques namely, laser beam welding (LBW) and cryogenic treatment (CT). The effectiveness of HM technique has been studied with respect to the reduction of residual stress and improvement in surface roughness. The results showed that the residual stress can be reduced up to 40% when compared to conventional LBW method. Moreover, the surface roughness can also be improved significantly as shown by the Ra value which decreases from 25.87µm to 19.31µm after CT.\n",
      "The full text of the article: http://www.sciencedirect.com/science/article/pii/S092583881400132X\n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt(data_train[50][\"title\"])\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "  generation_output = model.generate(\n",
    "      input_ids=input_tokens,\n",
    "      max_new_tokens=1000,\n",
    "      do_sample=True,\n",
    "      top_k=10,\n",
    "      top_p=0.9,\n",
    "      temperature=0.3,\n",
    "      repetition_penalty=1.15,\n",
    "      num_return_sequences=1,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70887e88-ddc1-44b4-848b-fd554027b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('materials.txt', 'r') as file:\n",
    "    word_list = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b263087-4b39-4559-a1ca-fc0db0117772",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {\n",
    "    'title': word_list,\n",
    "    'abstract': [s.replace('_', '') for s in word_list],\n",
    "    'doi': ['material'] * len(word_list),  # 假设新数据集中没有doi信息\n",
    "    'publicationDate': [None] * len(word_list)  # 假设新数据集中没有publicationDate信息\n",
    "}\n",
    "new_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "data_train = concatenate_datasets([data_train, new_dataset])\n",
    "\n",
    "new_data = {\n",
    "    'title': [s.replace('_', '') for s in word_list],\n",
    "    'abstract': word_list,\n",
    "    'doi': ['material'] * len(word_list),  # 假设新数据集中没有doi信息\n",
    "    'publicationDate': [None] * len(word_list)  # 假设新数据集中没有publicationDate信息\n",
    "}\n",
    "new_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "data_train = concatenate_datasets([data_train, new_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a51116-f6ef-4701-939f-d07933fb8f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = data_train.select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "644d9dfc-ad91-4d3e-bb14-2ee5600631c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The material :\n",
      " NiFeAlO4 is NiFeAlO_4 </s> \n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(type, title, abstract=None, eos_token=\"</s>\"):\n",
    "    if type == 'material':\n",
    "        instruction = \"The material :\\n\"\n",
    "        input = f\"{title} is\"\n",
    "        output = f\"{abstract + ' ' + eos_token if abstract else ''} \"\n",
    "        prompt = (\" \").join([instruction, input, output])\n",
    "    else:\n",
    "        instruction = \"The abstract of the paper:\\n\"\n",
    "        input = f\"{title}\\n\"\n",
    "        output = f\"Abstract: {abstract + ' ' + eos_token if abstract else ''} \"\n",
    "        prompt = (\" \").join([instruction, input, output])\n",
    "    return prompt\n",
    "\n",
    "print(generate_prompt(data_train[-1][\"doi\"], data_train[-1][\"title\"], data_train[-1][\"abstract\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a4ce5ac-170a-42c2-8b05-b204b6bf9f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "        r=256,\n",
    "        lora_alpha=512,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bb07ccc-4f63-44d0-8e25-16ad278ff7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 5552 tokens\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Add new tokens to the tokenizer\n",
    "num_added_toks = tokenizer.add_tokens(word_list)\n",
    "print(f\"Added {num_added_toks} tokens\")\n",
    "\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f119b5df-ac5a-46f8-be81-b12943b162f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Freeze all parameters in the model\n",
    "#for param in model.parameters():\n",
    "#    param.requires_grad = False\n",
    "\n",
    "embeddings = model.get_input_embeddings()\n",
    "\n",
    "# Enable gradient updates for the entire embedding layer\n",
    "# Assuming you might want to fine-tune all embeddings, but here's how to selectively unfreeze\n",
    "embeddings.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8483b140-a7e6-4175-8e01-6e3f13879301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='results',            # 输出目录\n",
    "    num_train_epochs=2,              # 总训练轮数\n",
    "    per_device_train_batch_size=4,   # 训练的batch size\n",
    "    per_device_eval_batch_size=4,    # 验证的batch size\n",
    "    gradient_accumulation_steps=4, \n",
    "    #gradient_checkpointing=True,\n",
    "    #optim = \"paged_adamw_32bit\",\n",
    "    optim = \"adamw_torch\",\n",
    "    bf16=True,\n",
    "    #fp16=True,\n",
    "    warmup_steps=300,                # 预热步数\n",
    "    learning_rate = 1e-4,\n",
    "    max_grad_norm = 0.2,\n",
    "    #max_steps = 50,\n",
    "    #warmup_ratio = 0.03,\n",
    "    #weight_decay=0.01,               # 权重衰减\n",
    "    save_strategy=\"steps\",           # 设置保存策略为\"steps\"\n",
    "    save_steps=300,                  # 每500步保存一次模型\n",
    "    save_total_limit=3,              # 最多保存3个检查点\n",
    "    evaluation_strategy=\"epoch\",     # 设置评估策略为\"steps\"\n",
    "    group_by_length=True,\n",
    "    #eval_steps=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8e64ad6-36bd-4b68-b995-bfef056bb7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient checkpointing enabling\n",
    "model.enable_input_require_grads()\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d49402b-1027-4e8b-a777-54b6b03f7966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='676' max='24798' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  676/24798 35:04 < 20:55:32, 0.32 it/s, Epoch 0.05/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def formatting_func(prompt):\n",
    "  output = []\n",
    "\n",
    "  for a, d, s in zip(prompt[\"doi\"], prompt[\"title\"], prompt[\"abstract\"]):\n",
    "    op = generate_prompt(a, d, s)\n",
    "    output.append(op)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_val,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# We will also pre-process the model by upcasting the layer norms in float 32 for more stable training\n",
    "#for name, module in trainer.model.named_modules():\n",
    "#    if \"norm\" in name:\n",
    "#        module = module.to(torch.float32)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(f\"{output_dir}/final\")\n",
    "\n",
    "# Step Training Loss Validation Loss\n",
    "# 10 1.848200 1.746341\n",
    "# 20 1.688300 1.696681\n",
    "# 30 1.654500 1.698127\n",
    "# 40 1.579400 1.652010\n",
    "# 50 1.492600 1.701877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e70cadb-ddc9-4677-94eb-07f25c452628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5acb4c8efe3f4f02b3efb6557cc557ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/165071 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae5b320f70f43ba9c209a143cce9b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/559 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2507' max='20634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2507/20634 2:27:55 < 17:50:28, 0.28 it/s, Epoch 0.24/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def formatting_func(prompt):\n",
    "  output = []\n",
    "\n",
    "  for d, s in zip(prompt[\"title\"], prompt[\"abstract\"]):\n",
    "    op = generate_prompt(d, s)\n",
    "    output.append(op)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_val,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# We will also pre-process the model by upcasting the layer norms in float 32 for more stable training\n",
    "#for name, module in trainer.model.named_modules():\n",
    "#    if \"norm\" in name:\n",
    "#        module = module.to(torch.float32)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(f\"{output_dir}/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b1c4f3e-d749-4235-b283-9caf5a593521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1f77e5e90e4a18b8187051ddbb94b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/1.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/enyuan/llama_2_7b_materials/commit/f3e916ad96f32cf5b0ab4fc51e5eca07fd5a38e7', commit_message='Upload LlamaForCausalLM', commit_description='', oid='f3e916ad96f32cf5b0ab4fc51e5eca07fd5a38e7', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"enyuan/llama_2_7b_materials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c45ef37-ce5e-44ec-bb37-675225009e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafb560b761145218919b9f6646c4e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/enyuan/llama_2_7b_materials/commit/cccd6362887ae7730b7a9689bf36a3408e330a34', commit_message='Upload tokenizer', commit_description='', oid='cccd6362887ae7730b7a9689bf36a3408e330a34', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"enyuan/llama_2_7b_materials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c50c387-2084-4e3b-b5d6-6ac9a6d15854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the materials:\n",
      " Magnetic materials with low magnetic damping constant.\n",
      " The material is:\n",
      "Word: BiFeO_3, Probability: 0.0095\n",
      "Word: Fe_3O_4, Probability: 0.0045\n",
      "Word: Mn_2V_3O_12, Probability: 0.0043\n",
      "Word: BaAl_xCr_yFe_11O_19, Probability: 0.0034\n",
      "Word: NiRh_2S_4, Probability: 0.0033\n",
      "Word: Ti_0.94Co_0.03La_0.03O_2, Probability: 0.0031\n",
      "Word: Gd_0.67Sr_0.33MnO_3, Probability: 0.0029\n",
      "Word: Fe_0.8Ga_0.2, Probability: 0.0025\n",
      "Word: NiFe_2O_4, Probability: 0.0025\n",
      "Word: SrSm_2Fe_2O_7, Probability: 0.0020\n",
      "Word: Bi_0.5La_0.5MnO_3, Probability: 0.0020\n",
      "Word: Cu_0.5Fe_0.5Cr_2S_4, Probability: 0.0019\n",
      "Word: Fe_xZn_2-xMo_3O_8, Probability: 0.0016\n",
      "Word: CaMn_3V_4O_12, Probability: 0.0015\n",
      "Word: CdFe_2O_4, Probability: 0.0015\n",
      "Word: Ni_3O_3, Probability: 0.0015\n",
      "Word: Ni_3Sn_2, Probability: 0.0015\n",
      "Word: Ni_50Mn_29Ga_21, Probability: 0.0014\n",
      "Word: Tl_2NaFeF_6, Probability: 0.0012\n",
      "Word: Ni_1.25-xZn_xPb_0.25Fe_1.5O_4, Probability: 0.0012\n",
      "Word: Li_3V_2, Probability: 0.0012\n",
      "Word: SrSn_0.97-xFe_xSb_0.03O_3-, Probability: 0.0012\n",
      "Word: Fe_3W_3C, Probability: 0.0012\n",
      "Word: La_2MnCoO_6, Probability: 0.0011\n",
      "Word: Fe_73.5Cu_1Nb_3Si_13.5B_9, Probability: 0.0011\n",
      "Word: Sm_2BaNiO_5, Probability: 0.0011\n",
      "Word: Mn_2VSnC_2, Probability: 0.0011\n",
      "Word: Ni_48Mn_34In_12Co_6, Probability: 0.0011\n",
      "Word: SrFe_1, Probability: 0.0010\n",
      "Word: YCa_2Hf_2Fe_3O_12, Probability: 0.0010\n",
      "Word: Ni_2.08Mn_0.96Ga_0.96, Probability: 0.0010\n",
      "Word: BaFe_12O_19, Probability: 0.0010\n",
      "Word: GdFeO_3, Probability: 0.0009\n",
      "Word: SrCr_6Fe_6O_19, Probability: 0.0009\n",
      "Word: Co_2MnSi, Probability: 0.0009\n",
      "Word: FeNb_3S_6, Probability: 0.0009\n",
      "Word: Mn_2O_3, Probability: 0.0009\n",
      "Word: LiNi_0.65-xCo_0.1Mn_0.25Cr_xO_2, Probability: 0.0009\n",
      "Word: Co_50Ni_23Ga_27Al_0, Probability: 0.0009\n",
      "Word: Fe_2Ca_3, Probability: 0.0009\n",
      "Word: Ga_1-xSn_xCMn_3, Probability: 0.0009\n",
      "Word: Ni_0.865Pd_0.135, Probability: 0.0009\n",
      "Word: Co_2Y, Probability: 0.0009\n",
      "Word: NiGa_0.25Fe_1.75-xCr_xO_4, Probability: 0.0008\n",
      "Word: MnCrAlO_4, Probability: 0.0008\n",
      "Word: LiFeO_2, Probability: 0.0008\n",
      "Word: Li_0.46Zn_0.04Fe_2.5O_4, Probability: 0.0008\n",
      "Word: YO_0.9F_0.1FeAs, Probability: 0.0008\n",
      "Word: Ni_xZn_2-xGeO_4, Probability: 0.0008\n",
      "Word: Na_2NiSi_4O_10, Probability: 0.0008\n",
      "Word: Co_21Mo_2B_6, Probability: 0.0008\n",
      "Word: BaVS_3, Probability: 0.0007\n",
      "Word: RFeO_3, Probability: 0.0007\n",
      "Word: MgMn_xCr_2-xO_4, Probability: 0.0007\n",
      "Word: ZnFe_2, Probability: 0.0007\n",
      "Word: Fe_3Ni, Probability: 0.0007\n",
      "Word: Mn_3GaC, Probability: 0.0007\n",
      "Word: Co_2TiN, Probability: 0.0007\n",
      "Word: BaFe_12O_17F_2, Probability: 0.0007\n",
      "Word: Sr_2Ni_2Fe_12O_22, Probability: 0.0007\n",
      "Word: Fe_4M_4, Probability: 0.0006\n",
      "Word: La_0.67Sr_0.16Ca_0.17MnO_3, Probability: 0.0006\n",
      "Word: La_0.3R_0.2Sr_0.5Ti_0.5Fe_0.5O_3, Probability: 0.0006\n",
      "Word: ZnCoFe_2O_4, Probability: 0.0006\n",
      "Word: YReFeAlO_6, Probability: 0.0006\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(prompt, output=None, eos_token=\"</s>\"):\n",
    "    instruction = \"Answer the materials:\\n\"\n",
    "    input = f\"Magnetic materials with {prompt}\\n\"\n",
    "    output = f\"The material is:\"\n",
    "    prompt = (\" \").join([instruction, input, output])\n",
    "    return prompt\n",
    "\n",
    "input_prompt = generate_prompt('low magnetic damping constant.')\n",
    "print(input_prompt)\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tokens).logits\n",
    "\n",
    "probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "# Get the top 10 token IDs and their probabilities\n",
    "top_k = 200\n",
    "top_probabilities, top_token_ids = torch.topk(probabilities, top_k)\n",
    "\"\"\"\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "top_probabilities = top_probabilities.squeeze().tolist()\n",
    "top_token_ids = top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(top_token_ids, top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")\n",
    "\"\"\"\n",
    "# Filter tokens with IDs less than 32000\n",
    "mask = top_token_ids >= 32000\n",
    "filtered_top_token_ids = top_token_ids[mask]\n",
    "filtered_top_probabilities = top_probabilities[mask]\n",
    "\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "filtered_top_probabilities = filtered_top_probabilities.squeeze().tolist()\n",
    "filtered_top_token_ids = filtered_top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(filtered_top_token_ids, filtered_top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a28fd5c-f37e-495a-a6d8-d1efb6246cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the materials:\n",
      " Magnetic materials with low magnetocrystalline anisotropy.\n",
      " The material is:\n",
      "Word: BiFeO_3, Probability: 0.0149\n",
      "Word: NiFe_2O_4, Probability: 0.0040\n",
      "Word: Fe_3O_4, Probability: 0.0031\n",
      "Word: Gd_0.67Sr_0.33MnO_3, Probability: 0.0031\n",
      "Word: Mn_2V_3O_12, Probability: 0.0029\n",
      "Word: Ti_0.94Co_0.03La_0.03O_2, Probability: 0.0027\n",
      "Word: CaMn_3V_4O_12, Probability: 0.0021\n",
      "Word: Fe_3Ni, Probability: 0.0018\n",
      "Word: Fe_xZn_2-xMo_3O_8, Probability: 0.0018\n",
      "Word: Fe_0.8Ga_0.2, Probability: 0.0018\n",
      "Word: Ni_3O_3, Probability: 0.0018\n",
      "Word: Cu_0.5Fe_0.5Cr_2S_4, Probability: 0.0017\n",
      "Word: ZnFe_2O_4, Probability: 0.0016\n",
      "Word: La_0.3R_0.2Sr_0.5Ti_0.5Fe_0.5O_3, Probability: 0.0016\n",
      "Word: Mn_2O_3, Probability: 0.0016\n",
      "Word: Co_2TiN, Probability: 0.0015\n",
      "Word: SrSm_2Fe_2O_7, Probability: 0.0014\n",
      "Word: BaFe_12O_19, Probability: 0.0013\n",
      "Word: BaAl_xCr_yFe_11O_19, Probability: 0.0011\n",
      "Word: SrSn_0.97-xFe_xSb_0.03O_3-, Probability: 0.0010\n",
      "Word: Bi_0.5La_0.5MnO_3, Probability: 0.0010\n",
      "Word: BiFe_1, Probability: 0.0010\n",
      "Word: Pr_0.5Sr_0.45K_0.05MnO_3, Probability: 0.0009\n",
      "Word: Fe_73.5Cu_1Nb_3Si_13.5B_9, Probability: 0.0009\n",
      "Word: YFe_1-xAl_xO_3, Probability: 0.0009\n",
      "Word: Fe_1Pt_3, Probability: 0.0009\n",
      "Word: LiNi_0.65-xCo_0.1Mn_0.25Cr_xO_2, Probability: 0.0008\n",
      "Word: Ni_0.865Pd_0.135, Probability: 0.0008\n",
      "Word: NiRh_2S_4, Probability: 0.0008\n",
      "Word: Co_0.1Cd_0.9Ga_2O_4, Probability: 0.0008\n",
      "Word: Ni_2.08Mn_0.96Ga_0.96, Probability: 0.0008\n",
      "Word: CoFe_2O_4, Probability: 0.0007\n",
      "Word: Ni_48Mn_34In_12Co_6, Probability: 0.0007\n",
      "Word: Ni_0.54Zn_0.48Fe_1.98O_4, Probability: 0.0007\n",
      "Word: Co_21Mo_2B_6, Probability: 0.0007\n",
      "Word: Ga_1-xSn_xCMn_3, Probability: 0.0007\n",
      "Word: Co_72Pt_28, Probability: 0.0007\n",
      "Word: FeCr_2O_4, Probability: 0.0007\n",
      "Word: La_2MnCoO_6, Probability: 0.0007\n",
      "Word: C_5R_5FeC_6R_6, Probability: 0.0007\n",
      "Word: Cu_0.5Co_2.5-xSn_xS_4, Probability: 0.0007\n",
      "Word: FeO_3, Probability: 0.0007\n",
      "Word: Li_1-yMnRu_1-xTi_xO_4, Probability: 0.0006\n",
      "Word: Mg_xMn_1-xFe_2O_4, Probability: 0.0006\n",
      "Word: LiMgVO_4, Probability: 0.0006\n",
      "Word: Ni_1.25-xZn_xPb_0.25Fe_1.5O_4, Probability: 0.0006\n",
      "Word: Na_2NiSi_4O_10, Probability: 0.0006\n",
      "Word: Fe_2B, Probability: 0.0006\n",
      "Word: Mn_3GaC, Probability: 0.0006\n",
      "Word: MgMn_xCr_2-xO_4, Probability: 0.0006\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(prompt, output=None, eos_token=\"</s>\"):\n",
    "    instruction = \"Answer the materials:\\n\"\n",
    "    input = f\"Magnetic materials with {prompt}\\n\"\n",
    "    output = f\"The material is:\"\n",
    "    prompt = (\" \").join([instruction, input, output])\n",
    "    return prompt\n",
    "\n",
    "input_prompt = generate_prompt('low magnetocrystalline anisotropy.')\n",
    "print(input_prompt)\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tokens).logits\n",
    "\n",
    "probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "# Get the top 10 token IDs and their probabilities\n",
    "top_k = 200\n",
    "top_probabilities, top_token_ids = torch.topk(probabilities, top_k)\n",
    "\"\"\"\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "top_probabilities = top_probabilities.squeeze().tolist()\n",
    "top_token_ids = top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(top_token_ids, top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")\n",
    "\"\"\"\n",
    "# Filter tokens with IDs less than 32000\n",
    "mask = top_token_ids >= 32000\n",
    "filtered_top_token_ids = top_token_ids[mask]\n",
    "filtered_top_probabilities = top_probabilities[mask]\n",
    "\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "filtered_top_probabilities = filtered_top_probabilities.squeeze().tolist()\n",
    "filtered_top_token_ids = filtered_top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(filtered_top_token_ids, filtered_top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc4641f4-1991-454a-b83f-af5c44bb4ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the materials:\n",
      " Magnetic materials with low density of states at the Fermi level.\n",
      " The material is:\n",
      "Word: BiFeO_3, Probability: 0.0074\n",
      "Word: MgMn_xCr_2-xO_4, Probability: 0.0062\n",
      "Word: Mn_2V_3O_12, Probability: 0.0058\n",
      "Word: Fe_3O_4, Probability: 0.0053\n",
      "Word: NiFe_2O_4, Probability: 0.0033\n",
      "Word: Fe_3Ni, Probability: 0.0030\n",
      "Word: Ni_3O_3, Probability: 0.0027\n",
      "Word: Gd_0.67Sr_0.33MnO_3, Probability: 0.0025\n",
      "Word: CaMn_3V_4O_12, Probability: 0.0022\n",
      "Word: SrSn_0.97-xFe_xSb_0.03O_3-, Probability: 0.0021\n",
      "Word: ZnFe_2O_4, Probability: 0.0019\n",
      "Word: Tl_2NaFeF_6, Probability: 0.0018\n",
      "Word: Co_2TiN, Probability: 0.0017\n",
      "Word: CuNMn_3, Probability: 0.0016\n",
      "Word: Ga_1-xSn_xCMn_3, Probability: 0.0015\n",
      "Word: NiRh_2S_4, Probability: 0.0015\n",
      "Word: La_0.3R_0.2Sr_0.5Ti_0.5Fe_0.5O_3, Probability: 0.0014\n",
      "Word: La_0.67Sr_0.16Ca_0.17MnO_3, Probability: 0.0013\n",
      "Word: BaFe_12O_19, Probability: 0.0013\n",
      "Word: Co_21Mo_2B_6, Probability: 0.0012\n",
      "Word: Mn_4N, Probability: 0.0012\n",
      "Word: Fe_0.8Ga_0.2, Probability: 0.0012\n",
      "Word: BaSc_xFe_12-xO_19, Probability: 0.0011\n",
      "Word: BaAl_xCr_yFe_11O_19, Probability: 0.0011\n",
      "Word: RFeO_3, Probability: 0.0011\n",
      "Word: Fe_2B, Probability: 0.0011\n",
      "Word: CsCoCl_3, Probability: 0.0011\n",
      "Word: CoFe_2, Probability: 0.0010\n",
      "Word: Fe_2Ca_3, Probability: 0.0010\n",
      "Word: Co_2TiO_4, Probability: 0.0010\n",
      "Word: CoFe_2O_4, Probability: 0.0010\n",
      "Word: Cu_0.5Fe_0.5Cr_2S_4, Probability: 0.0010\n",
      "Word: Mn_3GaC, Probability: 0.0010\n",
      "Word: Co_0.1Cd_0.9Ga_2O_4, Probability: 0.0010\n",
      "Word: CaSrFeNbO_6, Probability: 0.0009\n",
      "Word: YO_0.9F_0.1FeAs, Probability: 0.0009\n",
      "Word: Ba_2ZnFe_18O_30, Probability: 0.0009\n",
      "Word: Fe_3-xGa_xBO_6, Probability: 0.0009\n",
      "Word: Co_3B_2O_6, Probability: 0.0009\n",
      "Word: LiMgVO_4, Probability: 0.0009\n",
      "Word: GdFeO_3, Probability: 0.0008\n",
      "Word: Fe_1Pt_3, Probability: 0.0008\n",
      "Word: Ni_50Mn_29Ga_21, Probability: 0.0008\n",
      "Word: BaCo_1-xNi_xO_3, Probability: 0.0008\n",
      "Word: Li_0.46Zn_0.04Fe_2.5O_4, Probability: 0.0008\n",
      "Word: Co_2FeSi, Probability: 0.0008\n",
      "Word: La_0.9Ca_0.1Mn, Probability: 0.0008\n",
      "Word: Co_3D, Probability: 0.0008\n",
      "Word: MgCo_2O_4, Probability: 0.0008\n",
      "Word: La_0.7Ca_0.3MnO_3, Probability: 0.0008\n",
      "Word: Ti_0.94Co_0.03La_0.03O_2, Probability: 0.0007\n",
      "Word: Dy_3Fe_5O_12, Probability: 0.0007\n",
      "Word: Mn_15Si_26, Probability: 0.0007\n",
      "Word: Ni_2.08Mn_0.96Ga_0.96, Probability: 0.0007\n",
      "Word: LiFeO_2, Probability: 0.0007\n",
      "Word: Fe_73.5Cu_1Nb_3Si_13.5B_9, Probability: 0.0007\n",
      "Word: Co_2Y, Probability: 0.0007\n",
      "Word: FeO_3, Probability: 0.0007\n",
      "Word: CoFe_1.6Er_0.1Gd_0.2Sm_0.1O_4, Probability: 0.0007\n",
      "Word: MnCrAlO_4, Probability: 0.0007\n",
      "Word: SrSm_2Fe_2O_7, Probability: 0.0007\n",
      "Word: SrCr_6Fe_6O_19, Probability: 0.0007\n",
      "Word: Ni_0.5Zn_0.5Fe_2.0O_4.0, Probability: 0.0007\n",
      "Word: Ge_0.99Mn_0.01, Probability: 0.0007\n",
      "Word: Ru_0.5Rh_0.75Cr_1.5Co_0.25O_4, Probability: 0.0007\n",
      "Word: Mn_2VSnC_2, Probability: 0.0006\n",
      "Word: DyCr_xFe_1-xO_3, Probability: 0.0006\n",
      "Word: Bi_0.5La_0.5MnO_3, Probability: 0.0006\n",
      "Word: Mg_1.5Ni_1.5BO_5, Probability: 0.0006\n",
      "Word: La_0.7-xBi_xSr_0.3MnO_3, Probability: 0.0006\n",
      "Word: Sm_2BaNiO_5, Probability: 0.0006\n",
      "Word: Fe_4Cu_2, Probability: 0.0006\n",
      "Word: Mg_xNi_yFe_3-x-yO_4, Probability: 0.0006\n",
      "Word: Ca_xFe_3-xO_4, Probability: 0.0006\n",
      "Word: Mn_2O_3, Probability: 0.0006\n",
      "Word: Ba_6NiMn_4O_15, Probability: 0.0006\n",
      "Word: Co_82Al_14Cr_4, Probability: 0.0006\n",
      "Word: Fe_50Cr_30Al_20, Probability: 0.0006\n",
      "Word: Co_2MnSi, Probability: 0.0006\n",
      "Word: Ca_3FeTa_2O_9, Probability: 0.0006\n",
      "Word: RCo_2Ge_2, Probability: 0.0006\n",
      "Word: CeFeO_3, Probability: 0.0005\n",
      "Word: La_0.5Sm_0.5FeO_3, Probability: 0.0005\n",
      "Word: Fe_xZn_2-xMo_3O_8, Probability: 0.0005\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(prompt, output=None, eos_token=\"</s>\"):\n",
    "    instruction = \"Answer the materials:\\n\"\n",
    "    input = f\"Magnetic materials with {prompt}\\n\"\n",
    "    output = f\"The material is:\"\n",
    "    prompt = (\" \").join([instruction, input, output])\n",
    "    return prompt\n",
    "\n",
    "input_prompt = generate_prompt('low density of states at the Fermi level.')\n",
    "print(input_prompt)\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tokens).logits\n",
    "\n",
    "probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "# Get the top 10 token IDs and their probabilities\n",
    "top_k = 200\n",
    "top_probabilities, top_token_ids = torch.topk(probabilities, top_k)\n",
    "\"\"\"\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "top_probabilities = top_probabilities.squeeze().tolist()\n",
    "top_token_ids = top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(top_token_ids, top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")\n",
    "\"\"\"\n",
    "# Filter tokens with IDs less than 32000\n",
    "mask = top_token_ids >= 32000\n",
    "filtered_top_token_ids = top_token_ids[mask]\n",
    "filtered_top_probabilities = top_probabilities[mask]\n",
    "\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "filtered_top_probabilities = filtered_top_probabilities.squeeze().tolist()\n",
    "filtered_top_token_ids = filtered_top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(filtered_top_token_ids, filtered_top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff7555cd-6b66-4945-900b-e8d6d8ed5ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the materials:\n",
      " Magnetic materials with low conductivity.\n",
      " The material is:\n",
      "Word: BiFeO_3, Probability: 0.0080\n",
      "Word: Fe_3O_4, Probability: 0.0077\n",
      "Word: Ti_0.94Co_0.03La_0.03O_2, Probability: 0.0026\n",
      "Word: SrSm_2Fe_2O_7, Probability: 0.0020\n",
      "Word: Mn_2V_3O_12, Probability: 0.0020\n",
      "Word: BaAl_xCr_yFe_11O_19, Probability: 0.0018\n",
      "Word: Cu_0.5Fe_0.5Cr_2S_4, Probability: 0.0018\n",
      "Word: NiFe_2O_4, Probability: 0.0018\n",
      "Word: Bi_0.5La_0.5MnO_3, Probability: 0.0016\n",
      "Word: Fe_0.8Ga_0.2, Probability: 0.0015\n",
      "Word: Gd_0.67Sr_0.33MnO_3, Probability: 0.0015\n",
      "Word: CaMn_3V_4O_12, Probability: 0.0011\n",
      "Word: SrSn_0.97-xFe_xSb_0.03O_3-, Probability: 0.0011\n",
      "Word: Ni_3O_3, Probability: 0.0010\n",
      "Word: Na_2NiSi_4O_10, Probability: 0.0010\n",
      "Word: V_2O_5, Probability: 0.0010\n",
      "Word: Co_21Mo_2B_6, Probability: 0.0010\n",
      "Word: Li_0.46Zn_0.04Fe_2.5O_4, Probability: 0.0009\n",
      "Word: C_5R_5FeC_6R_6, Probability: 0.0009\n",
      "Word: Fe_3Ni, Probability: 0.0008\n",
      "Word: Fe_3W_3C, Probability: 0.0008\n",
      "Word: Fe_xZn_2-xMo_3O_8, Probability: 0.0008\n",
      "Word: Fe_1Pt_3, Probability: 0.0007\n",
      "Word: Co_0.1Cd_0.9Ga_2O_4, Probability: 0.0007\n",
      "Word: La_0.3R_0.2Sr_0.5Ti_0.5Fe_0.5O_3, Probability: 0.0007\n",
      "Word: Pr_0.5Sr_0.45K_0.05MnO_3, Probability: 0.0007\n",
      "Word: Fe_73.5Cu_1Nb_3Si_13.5B_9, Probability: 0.0007\n",
      "Word: LiNi_0.65-xCo_0.1Mn_0.25Cr_xO_2, Probability: 0.0007\n",
      "Word: LiFeO_2, Probability: 0.0007\n",
      "Word: Ni_3Sn_2, Probability: 0.0006\n",
      "Word: Sm_2BaNiO_5, Probability: 0.0006\n",
      "Word: La_2MnCoO_6, Probability: 0.0006\n",
      "Word: NiRh_2S_4, Probability: 0.0006\n",
      "Word: Ni_xZn_2-xGeO_4, Probability: 0.0006\n",
      "Word: LiMgVO_4, Probability: 0.0006\n",
      "Word: Na_3CoF_6, Probability: 0.0006\n",
      "Word: Ca_3FeTa_2O_9, Probability: 0.0006\n",
      "Word: SrFe_11.6Zn_0.4O_19, Probability: 0.0006\n",
      "Word: Co_2TiN, Probability: 0.0006\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(prompt, output=None, eos_token=\"</s>\"):\n",
    "    instruction = \"Answer the materials:\\n\"\n",
    "    input = f\"Magnetic materials with {prompt}\\n\"\n",
    "    output = f\"The material is:\"\n",
    "    prompt = (\" \").join([instruction, input, output])\n",
    "    return prompt\n",
    "\n",
    "input_prompt = generate_prompt('low conductivity.')\n",
    "print(input_prompt)\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tokens).logits\n",
    "\n",
    "probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "# Get the top 10 token IDs and their probabilities\n",
    "top_k = 200\n",
    "top_probabilities, top_token_ids = torch.topk(probabilities, top_k)\n",
    "\"\"\"\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "top_probabilities = top_probabilities.squeeze().tolist()\n",
    "top_token_ids = top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(top_token_ids, top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")\n",
    "\"\"\"\n",
    "# Filter tokens with IDs less than 32000\n",
    "mask = top_token_ids >= 32000\n",
    "filtered_top_token_ids = top_token_ids[mask]\n",
    "filtered_top_probabilities = top_probabilities[mask]\n",
    "\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "filtered_top_probabilities = filtered_top_probabilities.squeeze().tolist()\n",
    "filtered_top_token_ids = filtered_top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(filtered_top_token_ids, filtered_top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be8b60a1-abbc-4546-af81-4f903108bd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magnetic materials with high band gap.\n",
      " The material is:\n",
      "Word: BiFeO_3, Probability: 0.0214\n",
      "Word: NiFe_2O_4, Probability: 0.0130\n",
      "Word: Fe_3O_4, Probability: 0.0122\n",
      "Word: CaMn_3V_4O_12, Probability: 0.0095\n",
      "Word: MgMn_xCr_2-xO_4, Probability: 0.0081\n",
      "Word: CoFe_2, Probability: 0.0074\n",
      "Word: Ca_xFe_3-xO_4, Probability: 0.0058\n",
      "Word: Fe_3Ni, Probability: 0.0042\n",
      "Word: CoFe_2O_4, Probability: 0.0038\n",
      "Word: Li_0.46Zn_0.04Fe_2.5O_4, Probability: 0.0037\n",
      "Word: BaFe_12O_19, Probability: 0.0037\n",
      "Word: Co_2TiN, Probability: 0.0036\n",
      "Word: Ni_0.5Zn_0.5Fe_2O_4, Probability: 0.0036\n",
      "Word: CsCoCl_3, Probability: 0.0034\n",
      "Word: Cu_0.5Fe_0.5Cr_2S_4, Probability: 0.0032\n",
      "Word: ZnFe_2O_4, Probability: 0.0026\n",
      "Word: Fe_0.8Ga_0.2, Probability: 0.0026\n",
      "Word: Ge_0.99Mn_0.01, Probability: 0.0026\n",
      "Word: La_0.3R_0.2Sr_0.5Ti_0.5Fe_0.5O_3, Probability: 0.0026\n",
      "Word: Fe_2B, Probability: 0.0026\n",
      "Word: Ni_xZn_2-xGeO_4, Probability: 0.0026\n",
      "Word: LiNi_0.65-xCo_0.1Mn_0.25Cr_xO_2, Probability: 0.0025\n",
      "Word: Mn_3GaC, Probability: 0.0023\n",
      "Word: Fe_1Pt_3, Probability: 0.0023\n",
      "Word: Sr_2Ni_2Fe_12O_22, Probability: 0.0022\n",
      "Word: Gd_0.67Sr_0.33MnO_3, Probability: 0.0021\n",
      "Word: Co_2Y, Probability: 0.0021\n",
      "Word: YReFeAlO_6, Probability: 0.0021\n",
      "Word: SrSn_0.97-xFe_xSb_0.03O_3-, Probability: 0.0021\n",
      "Word: Ca_3FeTa_2O_9, Probability: 0.0021\n",
      "Word: La_2MnCoO_6, Probability: 0.0019\n",
      "Word: GdFeO_3, Probability: 0.0019\n",
      "Word: NiRh_2S_4, Probability: 0.0018\n",
      "Word: Ni_50Mn_29Ga_21, Probability: 0.0018\n",
      "Word: BaSc_xFe_12-xO_19, Probability: 0.0018\n",
      "Word: Co_3O_4, Probability: 0.0017\n",
      "Word: Mn_15Si_26, Probability: 0.0017\n",
      "Word: Ga_1-xSn_xCMn_3, Probability: 0.0016\n",
      "Word: RFeO_3, Probability: 0.0016\n",
      "Word: LaCaBiMn_2O_7, Probability: 0.0016\n",
      "Word: Er_12Fe_82B_6, Probability: 0.0016\n",
      "Word: Ni_3O_3, Probability: 0.0015\n",
      "Word: Co_82Al_14Cr_4, Probability: 0.0015\n",
      "Word: Fe_3-xGa_xBO_6, Probability: 0.0014\n",
      "Word: BaCo_1-xNi_xO_3, Probability: 0.0014\n",
      "Word: Co_2FeSi, Probability: 0.0014\n",
      "Word: Mn_2O_3, Probability: 0.0014\n",
      "Word: Mn_5O_8, Probability: 0.0013\n",
      "Word: ZnFe_2, Probability: 0.0013\n",
      "Word: CuCo_2S_4, Probability: 0.0012\n",
      "Word: Ru_0.5Rh_0.75Cr_1.5Co_0.25O_4, Probability: 0.0012\n",
      "Word: Dy_3Fe_5O_12, Probability: 0.0012\n",
      "Word: Ni_48Mn_34In_12Co_6, Probability: 0.0012\n",
      "Word: CuEu_0.01Fe_1.99O_4, Probability: 0.0012\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(prompt, output=None, eos_token=\"</s>\"):\n",
    "    #instruction = \"Answer the materials:\\n\"\n",
    "    input = f\"Magnetic materials with {prompt}\\n\"\n",
    "    output = f\"The material is:\"\n",
    "    prompt = (\" \").join([input, output])\n",
    "    return prompt\n",
    "\n",
    "input_prompt = generate_prompt('high band gap.')\n",
    "print(input_prompt)\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tokens).logits\n",
    "\n",
    "probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "# Get the top 10 token IDs and their probabilities\n",
    "top_k = 100\n",
    "top_probabilities, top_token_ids = torch.topk(probabilities, top_k)\n",
    "\n",
    "# Filter tokens with IDs less than 32000\n",
    "mask = top_token_ids >= 32000\n",
    "filtered_top_token_ids = top_token_ids[mask]\n",
    "filtered_top_probabilities = top_probabilities[mask]\n",
    "\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "filtered_top_probabilities = filtered_top_probabilities.squeeze().tolist()\n",
    "filtered_top_token_ids = filtered_top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(filtered_top_token_ids, filtered_top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b97d150-810d-4cf9-879e-261861c030d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0fdf16af59424cbad4688074921883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained('results/checkpoint-8400')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.load_adapter('results/checkpoint-8400')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1717ae01-2ff2-4175-9d6d-0a4fe5f6365d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d23b94f46fc4fe887ebe7deb127d58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "original_embeddings = model.get_input_embeddings().weight.detach().clone()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('results/checkpoint-8400')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.load_adapter('results/checkpoint-8400')\n",
    "\n",
    "\n",
    "embeddings = model.get_input_embeddings().weight.data\n",
    "embeddings[:len(original_tokenizer)] = original_embeddings[:len(original_tokenizer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e6c9a0-ea83-464d-b371-86fbecb8f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(prompt, output=None, eos_token=\"</s>\"):\n",
    "    instruction = \"Answer the materials:\\n\"\n",
    "    input = f\"Magnetic materials with {prompt}\\n\"\n",
    "    output = f\"The molecular formula of the material:\"\n",
    "    prompt = (\" \").join([instruction, input, output])\n",
    "    return prompt\n",
    "\n",
    "input_prompt = generate_prompt('low magnetic damping constant.')\n",
    "print(input_prompt)\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tokens).logits\n",
    "\n",
    "# 在计算softmax之前，为了数值稳定性，从logits中减去每个logit的最大值\n",
    "logits_stable = logits - torch.max(logits, dim=-1, keepdim=True).values\n",
    "\n",
    "probabilities = torch.softmax(logits_stable[:, -1, :], dim=-1)\n",
    "\n",
    "# Get the top 10 token IDs and their probabilities\n",
    "top_k = 10\n",
    "top_probabilities, top_token_ids = torch.topk(probabilities, top_k)\n",
    "\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "top_probabilities = top_probabilities.squeeze().tolist()\n",
    "top_token_ids = top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(top_token_ids, top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d878c63e-2a64-4790-a3a0-5ad01d25706e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37553"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(probabilities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6db7b0b-f14a-4c1c-8cc5-765ffeca6795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   673,   278, 17279, 29901,    13,  3561,  1212,   293, 17279,\n",
       "           411,  4482, 15611,   270,  1160,   292,  4868, 29889,    13,   450,\n",
       "         13206, 16637,  7063,   310,   278,  5518, 29901]], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ae5a43c-8612-44c3-8b8a-ddc8e679b73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   673,   278, 17279, 29901,    13,  3561,  1212,   293, 17279,\n",
       "           411,  4482, 15611,   270,  1160,   292,  4868, 29889,    13,   450,\n",
       "         13206, 16637,  7063,   310,   278,  5518, 29901, 37551]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4dda011c-9377-4412-af00-4297571242c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('result', save_embedding_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ea844d33-6ef9-4ea0-b695-1518534a1dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('result/tokenizer_config.json',\n",
       " 'result/special_tokens_map.json',\n",
       " 'result/tokenizer.model',\n",
       " 'result/added_tokens.json',\n",
       " 'result/tokenizer.json')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1620556-3e89-48ab-9ac5-3baaea7775bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results/tokenizer_config.json',\n",
       " 'results/special_tokens_map.json',\n",
       " 'results/tokenizer.model',\n",
       " 'results/added_tokens.json',\n",
       " 'results/tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601970e5-872e-4680-8244-01c41a211fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "291eb404-d313-4f1f-87c3-e489d24d60e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(37553, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=256, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=256, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=256, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=256, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=37553, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e506f375-179c-4782-a1c7-44079a380f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False base_model.model.model.embed_tokens.base_layer.weight torch.float16\n",
      "True base_model.model.model.embed_tokens.lora_embedding_A.default torch.float16\n",
      "True base_model.model.model.embed_tokens.lora_embedding_B.default torch.float16\n",
      "False base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.0.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.0.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.0.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.0.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.0.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.1.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.1.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.1.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.1.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.1.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.2.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.2.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.2.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.2.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.2.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.3.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.3.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.3.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.3.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.3.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.4.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.4.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.4.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.4.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.4.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.5.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.5.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.5.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.5.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.5.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.6.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.6.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.6.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.6.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.6.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.7.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.7.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.7.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.7.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.7.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.8.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.8.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.8.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.8.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.8.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.9.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.9.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.9.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.9.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.9.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.10.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.10.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.10.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.10.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.10.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.11.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.11.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.11.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.11.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.11.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.12.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.12.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.12.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.12.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.12.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.13.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.13.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.13.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.13.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.13.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.14.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.14.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.14.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.14.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.14.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.15.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.15.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.15.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.15.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.15.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.16.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.16.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.16.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.16.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.16.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.17.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.17.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.17.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.17.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.17.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.18.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.18.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.18.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.18.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.18.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.19.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.19.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.19.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.19.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.19.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.20.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.20.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.20.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.20.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.20.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.21.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.21.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.21.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.21.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.21.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.22.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.22.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.22.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.22.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.22.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.23.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.23.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.23.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.23.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.23.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.24.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.24.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.24.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.24.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.24.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.25.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.25.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.25.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.25.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.25.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.26.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.26.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.26.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.26.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.26.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.27.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.27.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.27.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.27.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.27.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.28.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.28.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.28.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.28.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.28.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.29.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.29.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.29.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.29.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.29.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.30.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.30.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.30.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.30.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.30.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.31.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.31.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.31.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.31.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.31.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.norm.weight torch.float16\n",
      "False base_model.model.lm_head.weight torch.float16\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(param.requires_grad, name, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81fa8868-d7b2-4b52-9c5d-8d46a3d0af2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: base_model.model.model.embed_tokens.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "# Verify which parameters are trainable\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346aa5df-43be-459d-98fc-247b0a2fda7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fa5b3e-125c-4fb1-b073-82301ba20d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53006efd-ca7f-4cf7-b226-331f6f4316b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Trainable: {name}\", param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0de8f185-07da-46c0-a3af-0f45f6b39016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7faf7c779ee0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0db0120f-b1a6-441a-8dbe-d0e59fe4e9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(38544, 4096)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a34e1d06-cd59-4b28-9915-a865bb031c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6922694656"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50d8bd59-e9dd-4a70-9e38-02944052fd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(38545, 4096)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f10f6-db0d-47ad-9c4f-6d4cc888b013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
