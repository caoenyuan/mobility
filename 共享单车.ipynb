{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac193d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import csv\n",
    "import re\n",
    "import random\n",
    "k='2014-01-01'\n",
    "j='2016-12-31'\n",
    "dataa=[]\n",
    "import datetime\n",
    "dates = []\n",
    "dt = datetime.datetime.strptime(k, \"%Y-%m-%d\")\n",
    "date = k[:]\n",
    "while date <= j:\n",
    "    dates.append(date)\n",
    "    dt = dt + datetime.timedelta(1)\n",
    "    date = dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def ua():\n",
    "    \"\"\"随机获取一个浏览器用户信息\"\"\"\n",
    "\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11',\n",
    "        'Opera/9.25 (Windows NT 5.1; U; en)',\n",
    "        'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)',\n",
    "        'Mozilla/5.0 (compatible; Konqueror/3.5; Linux) KHTML/3.5.5 (like Gecko) (Kubuntu)',\n",
    "        'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.0.12) Gecko/20070731 Ubuntu/dapper-security Firefox/1.5.0.12',\n",
    "        'Lynx/2.8.5rel.1 libwww-FM/2.14 SSL-MM/1.4.1 GNUTLS/1.2.9',\n",
    "        'Mozilla/5.0 (X11; Linux i686) AppleWebKit/535.7 (KHTML, like Gecko) Ubuntu/11.04 Chromium/16.0.912.77 Chrome/16.0.912.77 Safari/535.7',\n",
    "        'Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:10.0) Gecko/20100101 Firefox/10.0',\n",
    "\n",
    "    ]\n",
    "\n",
    "    agent = random.choice(user_agents)\n",
    "\n",
    "    return agent\n",
    "\n",
    "for wor in ['共享单车']:\n",
    "    for tii in dates:  # 循环这一年的每天\n",
    "        print(tii)\n",
    "        for i in range(1, 2):#因为微博最多只能50页\n",
    "            url = f'https://s.weibo.com/weibo?q={wor}&typeall=1&suball=1&timescope=custom:{tii}-0:{tii}-23&Refer=g&page={i}'            \n",
    "            #网站url\n",
    "            headers = {\n",
    "                'cookie':'_T_WM=79452191668; XSRF-TOKEN=25c18c; WEIBOCN_FROM=1110005030; mweibo_short_token=f102c0049e; SCF=ApWp3d4Y88M0WvYCVhkgsasd3T9bHbfPd9ad5Z1MGFxfBpShVDmO-9MlABHdR9QWTo8c5jKwWUZ0BkZsMKKSRMQ.; SUB=_2A25IFprGDeRhGeFG6FYR8S7Owz-IHXVr-CaOrDV6PUJbktANLVOikW1NeauaUIGumaJdfFTuzOR3SKpyKLn2bepw; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WFaFL9wsf1sc9812OKZcN525JpX5K-hUgL.FoMRe0B7eK5E1he2dJLoI0YLxKnL1h2L1-2LxKnLBoBLBKBLxKqLBK-LB.eLxKML1-2L1hBLxKML1heLBKMLxK.LBo.L1h-LxK.LBK-LB-Bt; SSOLoginState=1695738518; ALF=1698330518; MLOGIN=1; M_WEIBOCN_PARAMS=uicode%3D20000174',\n",
    "                'user-agent': 'Chrome/98.0.4758.102 Safari/537.36'\n",
    "                #'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36',\n",
    "            }#请求头\n",
    "            \n",
    "            \n",
    "            sleep(1)#停顿1s\n",
    "            #print('1秒后继续采集')\n",
    "            while 1:\n",
    "                try:\n",
    "                    html0 = requests.get(url=url, headers=headers).text\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "            #发起请求\n",
    "            t = BeautifulSoup(html0, 'lxml')\n",
    "            #bs4数据解析\n",
    "            card = t.find_all('div', class_=\"card-wrap\")\n",
    "            print(len(card))\n",
    "            print(card)\n",
    "            \n",
    "            #定位没个博文的位置\n",
    "            try:\n",
    "                pd1 = t.find('div', class_=\"card card-no-result s-pt20b40\")#如果找没有信息到直接跳出程序\n",
    "                if pd1!=None:\n",
    "                    continue\n",
    "                    #break\n",
    "            except:\n",
    "                pd1=''\n",
    "            for p in card:\n",
    "                dic={}\n",
    "                if p.find_all('p', class_=\"txt\")!=[]:\n",
    "                    print(p.find_all('p', class_=\"txt\"))\n",
    "                    try:\n",
    "                        dic['word']=wor\n",
    "                        yh_url='https:'+p.find('div',class_=\"avator\").a['href']\n",
    "                        dic['user_url']=yh_url\n",
    "                        bw_url='https:'+p.find('div', class_=\"from\").a['href']\n",
    "                        \n",
    "                        dic['url']=bw_url\n",
    "                        yh_id=bw_url.split('/')[3]\n",
    "                        dic['user_id']=yh_id\n",
    "                        bo_id=bw_url.split('/')[4].replace('?refer_flag=1001030103_','')\n",
    "                        dic['id']=bo_id\n",
    "                        id=yh_id+'_'+bo_id\n",
    "                        dic['_id']=id\n",
    "                        txt=p.find_all('p', class_=\"txt\")[-1].find_all('a')\n",
    "                        dic['place']='无'#新增\n",
    "                        \n",
    "                        for tx in txt:\n",
    "                            try:\n",
    "                                if tx.find('i',class_=\"wbicon\").text=='2':\n",
    "                                    dic['place']=str(tx.text).replace('2','')\n",
    "                            except:\n",
    "                                pass\n",
    "                        \n",
    "                        dic['content'] = str(p.find_all('p', class_=\"txt\")[-1].text).replace('\\n', '').replace('\\u200b','').replace('收起d','')#博文内容\n",
    "                        dic['comment']= str(p.find_all('a', class_=\"woo-box-flex woo-box-alignCenter woo-box-justifyCenter\")[-2].text).replace(' ','')\n",
    "                        #dic['comment'] = str(p.find_all(text=re.compile(\"评论 .*?\"))[0]).replace(' ','').replace('评论','')#博文评论\n",
    "                        if dic['comment']== '评论':\n",
    "                            dic['comment']= 0\n",
    "                        dic['name']= p.find('a', class_=\"name\").text\n",
    "                        time = str(p.find('div', class_=\"from\").text).replace(' ', '').replace('\\n', '').replace('\\xa0','')#发博时间\n",
    "                        time = time + '来自0'\n",
    "                        dic['pubtime']= time.split('来自')[0]\n",
    "                        dic['tools']=time.split('来自')[1]\n",
    "                        dic['like']= str(p.find_all('span', class_=\"woo-like-count\")[-1].text).replace(' ', '')                    \n",
    "                        if dic['like']== '赞':\n",
    "                            dic['like']= 0\n",
    "                        dic['transfer']= str(p.find_all('a', class_=\"woo-box-flex woo-box-alignCenter woo-box-justifyCenter\")[-3].text).replace(' ', '')\n",
    "                         \n",
    "                        if dic['transfer']== '转发':\n",
    "                            dic['transfer']= 0\n",
    "                        yhurl = f'https://weibo.com/ajax/profile/info?uid={yh_id}'\n",
    "                        while 1:\n",
    "                            try:\n",
    "                                uhht = requests.get(url=yhurl, headers=headers).json()\n",
    "                                break\n",
    "                            except:\n",
    "                                pass\n",
    "                        print(uhht['data']['user'])\n",
    "                        dic['num_follows'] = uhht['data']['user']['friends_count']\n",
    "                        dic['num_fans'] = uhht['data']['user']['followers_count']\n",
    "                        dic['num_tweets'] = uhht['data']['user']['statuses_count']\n",
    "                        xb = uhht['data']['user']['gender']\n",
    "\n",
    "                        if xb == 'f':\n",
    "                            dic['gender'] = '女'\n",
    "                        else:\n",
    "                            dic['gender'] = '男'\n",
    "                        dic['location'] = uhht['data']['user']['location']\n",
    "                        dic['verified']= uhht['data']['user']['verified']\n",
    "                        dic['verified_type']= uhht['data']['user']['verified_type']\n",
    "                        try:\n",
    "                            dic['verified_reason'] = uhht['data']['user']['verified_reason']\n",
    "                        except:\n",
    "                            dic['verified_reason'] =''\n",
    "                            pass\n",
    "                        \n",
    "                        print(f\"成功写入：{dic}\")\n",
    "                    except:\n",
    "                        print(i,\"失败\")\n",
    "                        pass\n",
    "                    \n",
    "print(\"完成\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
