{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a8afe9-1f35-48de-8384-6063d350cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340eccc8-60ac-4b8c-9807-dcbbc86185e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3d7d218bea4cebba21a0def88d5135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6128881267da40e5a2dbfd8ae5a06fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d3f62995324dadab0f3a1cb93db04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a75ca730c1a4fc8ac7fb9ad72419eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54444caf438427e88e64b289d5e84ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9c8833175c49509b180b5ccb53b30b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18fd6c7659746b0a6e35eccbe7bd553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56a2418526a44e4a36a1df0044fd6a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8cb2ed0b8746518cf9eca0c23546e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85076e98404049b1b12cfc9348b5d174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3568ba74c8fa44d5acc2d0e5a361930c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbcb540f848743839300aaf69db6ddf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             #load_in_8bit=True,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7048e1a0-a9a2-4210-b9a6-c93a778d4a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        year  month location  \\\n",
      "0       2017      1       四川   \n",
      "1       2017      1       北京   \n",
      "2       2017      1       江苏   \n",
      "3       2017      1       江苏   \n",
      "4       2017      1       山东   \n",
      "...      ...    ...      ...   \n",
      "337724  2023      9       其他   \n",
      "337725  2023      9       上海   \n",
      "337726  2023      9       江苏   \n",
      "337727  2023      9       山东   \n",
      "337728  2023      9       辽宁   \n",
      "\n",
      "                                             cleaned_text  点赞数  转发数  \n",
      "0       【2017年技术趋势前瞻：自动驾驶将逐步规模化】据国外媒体报道，在某些方面，2017年的技术...    2    4  \n",
      "1       【更先进奔驰在德获批测新自动驾驶技术】日前，奔驰在德国获得了在公共道路上测试最新自动驾驶技术...   71   13  \n",
      "2       【Tesla也要研发自动驾驶技术】近日，投资分析网站SeekingAlpha报道称，Tesl...    0    0  \n",
      "3               1.打车平台叫车被恶意扣费3.智能电表很方便也很危险4.朴槿惠否认与三星合并案有关    0    0  \n",
      "4       【海外智能汽车与自动驾驶测试场盘点】目前海外独立智能车和自动驾驶测试场大约有8家。欧洲3家，...   12    3  \n",
      "...                                                   ...  ...  ...  \n",
      "337724  今天是9月28日，是史上最长小长假的最后一个工作日，今天想分享和记录一下出乎意料的坎坷但很平...    0    0  \n",
      "337725  #新能源汽车# 【车主状告特斯拉获赔3000元】 有车主在与特斯拉的官司胜诉，车主称，中1....    0    0  \n",
      "337726  #遵从自己的内心有多自由#开往黄田坝方向的（自动驾驶）列车即将进站，先下后上，注意列车与站台...    0    0  \n",
      "337727  【#郑州男子意外碾死躺道闸杆下老太#二审开庭 ，新侦查实验中，7名志愿者4人撞上模拟物】“郑...  106    7  \n",
      "337728  《方云溪言余庭》已完结（方云溪言余庭）热门小说全目录//完整版)全文阅读笔趣阁❗书名：《方云...    0    0  \n",
      "\n",
      "[337729 rows x 6 columns]\n",
      "       year  month location  \\\n",
      "0      2011      8       天津   \n",
      "1      2011      8       北京   \n",
      "2      2011      8       浙江   \n",
      "3      2011      8       浙江   \n",
      "4      2011      8       浙江   \n",
      "...     ...    ...      ...   \n",
      "69335  2023     12       北京   \n",
      "69336  2023     12       上海   \n",
      "69337  2023     12       其他   \n",
      "69338  2023     12       上海   \n",
      "69339  2023     12       广东   \n",
      "\n",
      "                                            cleaned_text  点赞数  转发数  \n",
      "0      《发展新能源汽车农村市场优势多》日前，纯电动车在行驶过程中发生自燃的现象受到了业内外的极大关...    0  0.0  \n",
      "1      【比亚迪至今共向私人销出732辆新能源汽车】比亚迪司有关人员提供数据显示，截至6月底，公司面...    0  7.0  \n",
      "2      【比亚迪：新能源车预计3年内盈利配套设施成最大瓶颈】在掌握动力电池、驱动电机和电控系统这3项...    0  2.0  \n",
      "3      工业和信息化部、科技部、国家发展改革委，明确了新能源汽车的范围，将插电式混合动力汽车、纯电动...    0  4.0  \n",
      "4      【长安汽车领跑新能源汽车产业化】在日前举行的２０１１中国品牌汽车博览会上，新能源汽车成为焦点...    0  1.0  \n",
      "...                                                  ...  ...  ...  \n",
      "69335  ~预约检测布加迪威航，心中狂喜，没想到是一个被放废了的电动汽车！2023年的最后一天，想学着...    0  0.0  \n",
      "69336  在平望服务区，看到了蔚来的二代换电站。换电站也已经贴上了广告，而且表示「换电站 ＝ 电动车的...   11  1.0  \n",
      "69337  【在海外都是哪些人在买BYD Dolphin】国内新能源发展如火如荼，去到海外发现国外的电车...    0  0.0  \n",
      "69338  2023中国汽车业：一面烈火烹油，一面血雨腥风（下）摘自集微网（文/杜莎）持续一年的无差别价...    5  2.0  \n",
      "69339  比亚迪汽车超话我预测未来几年，随着电动车成本下降，两者在整体新能源车市场份额将同步增长。这...    0  0.0  \n",
      "\n",
      "[69340 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_train = pd.read_csv('av_cleaned.csv', encoding = 'utf-8-sig')\n",
    "print(data_train)\n",
    "\n",
    "data_train_ev = pd.read_csv('ev_cleaned.csv', encoding = 'utf-8-sig')\n",
    "print(data_train_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "709235fe-21f5-4fb7-bc65-e94497683490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful AI assistant, answer the question as short as possible.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|> What is the attitude of the following text towards electric vehicles or autonomous driving? Answer with positive, negative or neutral. Also classify it as one of the following categories:\n",
      "1: Safety\n",
      "2: Price and Cost\n",
      "3: Technology maturity and infrastructure\n",
      "4: User Experience\n",
      "5: Policy and market\n",
      "6: Ethical and social impact\n",
      "7: Other or cannot be classified\n",
      "Only answer the attitude and the category number, and do not answer anything else.\n",
      "Text: 【更先进奔驰在德获批测新自动驾驶技术】日前，奔驰在德国获得了在公共道路上测试最新自动驾驶技术的许可，据悉，奔驰新的自动驾驶测试车将会搭载名为DAVOS的自动驾驶系统奔驰在德获批测新自动驾驶技术 <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(content):\n",
    "    begin = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\"\n",
    "    #syst = \"<<SYS>> You are a helpful assistant, always answer as helpfully as possible.\\n If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.<</SYS>>\\n\"\n",
    "    #inst = \"Read the following text. Does it mention the Gilbert damping constant of a certain material? If so, list the corresponding material and its Gilbert damping canstant.\\n\" + content\n",
    "    syst = \"You are a helpful AI assistant, answer the question as short as possible.<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\"\n",
    "    inst = \"What is the attitude of the following text towards electric vehicles or autonomous driving? Answer with positive, negative or neutral. Also classify it as one of the following categories:\\n1: Safety\\n2: Price and Cost\\n3: Technology maturity and infrastructure\\n4: User Experience\\n5: Policy and market\\n6: Ethical and social impact\\n7: Other or cannot be classified\\nOnly answer the attitude and the category number, and do not answer anything else.\\nText: \"+content\n",
    "    end = \"<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    prompt = (\" \").join([begin, syst, inst, end])\n",
    "    return prompt\n",
    "\n",
    "#print(generate_prompt('How are you?'))\n",
    "print(generate_prompt(data_train['cleaned_text'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8445ac2-ba1a-4764-af7a-1115eb820060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存到 output_ev.csv，当前迭代次数：0，用时：0.29 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：200，用时：56.38 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：400，用时：56.54 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：600，用时：56.13 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：800，用时：56.29 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：1000，用时：56.38 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：1200，用时：56.63 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：1400，用时：56.86 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：1600，用时：57.48 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：1800，用时：56.21 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：2000，用时：56.38 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：2200，用时：56.32 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：2400，用时：56.49 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：2600，用时：58.38 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：2800，用时：56.88 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：3000，用时：57.00 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：3200，用时：56.95 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：3400，用时：55.90 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：3600，用时：56.51 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：3800，用时：56.53 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：4000，用时：56.61 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：4200，用时：56.93 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：4400，用时：56.22 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：4600，用时：56.95 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：4800，用时：56.21 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：5000，用时：56.48 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：5200，用时：56.62 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：5400，用时：56.19 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：5600，用时：56.37 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：5800，用时：56.50 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：6000，用时：56.71 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：6200，用时：56.64 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：6400，用时：56.67 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：6600，用时：56.73 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：6800，用时：56.38 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：7000，用时：56.00 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：7200，用时：56.99 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：7400，用时：57.30 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：7600，用时：57.03 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：7800，用时：57.10 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：8000，用时：56.44 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：8200，用时：57.16 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：8400，用时：56.67 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：8600，用时：56.95 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：8800，用时：57.30 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：9000，用时：59.16 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：9200，用时：59.35 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：9400，用时：59.48 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：9600，用时：58.66 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：9800，用时：62.36 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：10000，用时：63.21 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：10200，用时：62.55 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：10400，用时：62.34 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：10600，用时：62.13 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：10800，用时：59.24 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：11000，用时：58.20 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：11200，用时：57.70 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：11400，用时：56.81 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：11600，用时：58.87 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：11800，用时：59.34 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：12000，用时：58.20 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：12200，用时：59.26 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：12400，用时：57.64 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：12600，用时：58.70 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：12800，用时：59.97 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：13000，用时：59.37 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：13200，用时：59.00 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：13400，用时：61.89 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：13600，用时：59.02 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：13800，用时：62.22 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：14000，用时：61.41 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：14200，用时：61.18 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：14400，用时：61.33 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：14600，用时：58.40 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：14800，用时：60.49 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：15000，用时：61.08 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：15200，用时：61.53 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：15400，用时：60.24 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：15600，用时：60.65 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：15800，用时：61.07 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：16000，用时：57.44 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：16200，用时：60.52 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：16400，用时：60.96 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：16600，用时：61.42 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：16800，用时：62.31 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：17000，用时：63.17 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：17200，用时：63.33 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：17400，用时：64.09 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：17600，用时：66.55 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：17800，用时：67.33 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：18000，用时：65.27 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：18200，用时：64.76 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：18400，用时：64.19 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：18600，用时：59.93 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：18800，用时：62.85 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：19000，用时：63.07 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：19200，用时：63.69 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：19400，用时：63.43 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：19600，用时：64.48 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：19800，用时：63.38 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：20000，用时：67.53 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：20200，用时：68.09 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：20400，用时：65.62 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：20600，用时：70.11 秒\n",
      "已保存到 output_ev.csv，当前迭代次数：20800，用时：68.44 秒\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "data = []\n",
    "save_interval = 200\n",
    "\n",
    "start_time = time.time()  # 记录开始时间\n",
    "\n",
    "for i, text in enumerate(data_train_ev['cleaned_text']):\n",
    "    if i % 1 == 0:\n",
    "        input_prompt = generate_prompt(text)\n",
    "        #input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "        #inputs = tokenizer(input_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "        input_tokens = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        with torch.cuda.amp.autocast():\n",
    "            generation_output = model.generate(\n",
    "                input_ids=input_tokens,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=8,\n",
    "                do_sample=False,\n",
    "                repetition_penalty=1.1,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "        #print(op)\n",
    "\n",
    "        inst_index = op.find('assistant\\n')\n",
    "        \n",
    "        if inst_index != -1:\n",
    "            #print(text)\n",
    "            #print(op[inst_index + len('assistant\\n'):])\n",
    "            data.append({\"number\": i, \"text\": text, \"sentiment\": op[inst_index + len('assistant\\n'):]})\n",
    "        else:\n",
    "            #print(\"未找到'assistant\\n'标记\")\n",
    "            data.append({\"number\": i, \"text\": text, \"sentiment\":\"\"})\n",
    "\n",
    "        # 每 save_interval 个迭代保存一次\n",
    "        if i % save_interval == 0:\n",
    "            df = pd.DataFrame(data)\n",
    "            df.to_csv(\"output_ev.csv\", encoding = 'utf-8-sig', index=False, mode='a', header=False)  # 追加模式\n",
    "            data = []\n",
    "            end_time = time.time()  # 记录结束时间\n",
    "            elapsed_time = end_time - start_time  # 计算用时\n",
    "            print(f\"已保存到 output_ev.csv，当前迭代次数：{i}，用时：{elapsed_time:.2f} 秒\")\n",
    "    \n",
    "            start_time = end_time  # 更新开始时间，用于计算下一个周期的用时\n",
    "\n",
    "# 最后一次保存\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"output_ev.csv\", encoding = 'utf-8-sig', index=False, mode='a', header=False)\n",
    "print(\"已保存到 output_ev.csv\")\n",
    "\n",
    "###########################################################################\n",
    "\n",
    "data = []\n",
    "save_interval = 300\n",
    "\n",
    "start_time = time.time()  # 记录开始时间\n",
    "\n",
    "for i, text in enumerate(data_train['cleaned_text']):\n",
    "    if i % 3 == 0:\n",
    "        input_prompt = generate_prompt(text)\n",
    "        #input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "        #inputs = tokenizer(input_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "        input_tokens = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        with torch.cuda.amp.autocast():\n",
    "            generation_output = model.generate(\n",
    "                input_ids=input_tokens,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=8,\n",
    "                do_sample=False,\n",
    "                repetition_penalty=1.1,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "        #print(op)\n",
    "\n",
    "        inst_index = op.find('assistant\\n')\n",
    "        \n",
    "        if inst_index != -1:\n",
    "            #print(text)\n",
    "            #print(op[inst_index + len('assistant\\n'):])\n",
    "            data.append({\"number\": i, \"text\": text, \"sentiment\": op[inst_index + len('assistant\\n'):]})\n",
    "        else:\n",
    "            #print(\"未找到'assistant\\n'标记\")\n",
    "            data.append({\"number\": i, \"text\": text, \"sentiment\":\"\"})\n",
    "\n",
    "        # 每 save_interval 个迭代保存一次\n",
    "        if i % save_interval == 0:\n",
    "            df = pd.DataFrame(data)\n",
    "            df.to_csv(\"output.csv\", encoding = 'utf-8-sig', index=False, mode='a', header=False)  # 追加模式\n",
    "            data = []\n",
    "            end_time = time.time()  # 记录结束时间\n",
    "            elapsed_time = end_time - start_time  # 计算用时\n",
    "            print(f\"已保存到 output.csv，当前迭代次数：{i}，用时：{elapsed_time:.2f} 秒\")\n",
    "    \n",
    "            start_time = end_time  # 更新开始时间，用于计算下一个周期的用时\n",
    "\n",
    "# 最后一次保存\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"output.csv\", encoding = 'utf-8-sig', index=False, mode='a', header=False)\n",
    "print(\"已保存到 output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f6494c7-70b7-4622-bc95-f5bd865fe0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'year'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39039961-cd6f-4c73-8912-8e9a74c05f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05290738-ee4f-4f60-9f5b-53af84dfdbaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83379a15-57ec-4c8f-8dec-d73e860811f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unique_reasons.txt', 'r') as file:\n",
    "    reasons = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1c42c52-65e9-4174-84fe-dee9a91a71fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful AI assistant, answer the question as short as possible.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|> \"technological progress.\" is a keyword for a user's attitude towards autonomous driving. Which of the following categories does it belong to?\n",
      "Only answer the category number, and do not answer anything else.\n",
      "1: Safety, including concerns about accidents\n",
      "2: Technology maturity and reliability\n",
      "3: Practicality, convenience and cost\n",
      "4: Ethical and social impact, commercialization, industrial development\n",
      "5: Expectations for new technologies\n",
      "6: Other or cannot be classified <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(content):\n",
    "    begin = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\"\n",
    "    #syst = \"<<SYS>> You are a helpful assistant, always answer as helpfully as possible.\\n If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.<</SYS>>\\n\"\n",
    "    #inst = \"Read the following text. Does it mention the Gilbert damping constant of a certain material? If so, list the corresponding material and its Gilbert damping canstant.\\n\" + content\n",
    "    syst = \"You are a helpful AI assistant, answer the question as short as possible.<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\"\n",
    "    inst = '\\\"'+content + \"\\\" is a keyword for a user's attitude towards autonomous driving. Which of the following categories does it belong to?\\nOnly answer the category number, and do not answer anything else.\\n1: Safety, including concerns about accidents\\n2: Technology maturity and reliability\\n3: Practicality, convenience and cost\\n4: Ethical and social impact, commercialization, industrial development\\n5: Expectations for new technologies\\n6: Other or cannot be classified\"\n",
    "    end = \"<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    prompt = (\" \").join([begin, syst, inst, end])\n",
    "    return prompt\n",
    "\n",
    "#print(generate_prompt('How are you?'))\n",
    "print(generate_prompt(reasons[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e8ef7ce-c92c-488a-9c91-39fab0ea01b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1406\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful AI assistant, answer the question as short as possible.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|> \"infrastructure.\" is a keyword for a user's attitude towards electric vehicles. Which of the following categories does it belong to?\n",
      "Only answer the category number, and do not answer anything else.\n",
      "1: Price and Cost\n",
      "2: Infrastructure\n",
      "3: Battery and Range\n",
      "4: Safety and Security\n",
      "5: User Experience\n",
      "6: Policy & Market\n",
      "7: Other or cannot be classified <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('unique_reasons.txt', 'r') as file:\n",
    "    reasons = file.read().splitlines()\n",
    "\n",
    "print(len(reasons))\n",
    "\n",
    "def generate_prompt(content):\n",
    "    begin = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\"\n",
    "    #syst = \"<<SYS>> You are a helpful assistant, always answer as helpfully as possible.\\n If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.<</SYS>>\\n\"\n",
    "    #inst = \"Read the following text. Does it mention the Gilbert damping constant of a certain material? If so, list the corresponding material and its Gilbert damping canstant.\\n\" + content\n",
    "    syst = \"You are a helpful AI assistant, answer the question as short as possible.<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\"\n",
    "    inst = '\\\"'+content + \"\\\" is a keyword for a user's attitude towards electric vehicles. Which of the following categories does it belong to?\\nOnly answer the category number, and do not answer anything else.\\n1: Price and Cost\\n2: Infrastructure\\n3: Battery and Range\\n4: Safety and Security\\n5: User Experience\\n6: Policy & Market\\n7: Other or cannot be classified\"\n",
    "    end = \"<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    prompt = (\" \").join([begin, syst, inst, end])\n",
    "    return prompt\n",
    "\n",
    "#print(generate_prompt('How are you?'))\n",
    "print(generate_prompt(reasons[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b534ac-0bea-4631-9c00-c00cb4c13833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存到 output.csv，当前迭代次数：0，用时：0.12 秒\n",
      "已保存到 output.csv，当前迭代次数：300，用时：39.49 秒\n",
      "已保存到 output.csv，当前迭代次数：600，用时：36.58 秒\n",
      "已保存到 output.csv，当前迭代次数：900，用时：36.80 秒\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "data = []\n",
    "save_interval = 300\n",
    "\n",
    "start_time = time.time()  # 记录开始时间\n",
    "\n",
    "for i, text in enumerate(reasons):\n",
    "    if i % 1 == 0:\n",
    "        input_prompt = generate_prompt(text)\n",
    "        #input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "        #inputs = tokenizer(input_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "        input_tokens = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        with torch.cuda.amp.autocast():\n",
    "            generation_output = model.generate(\n",
    "                input_ids=input_tokens,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=16,\n",
    "                do_sample=False,\n",
    "                repetition_penalty=1.1,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "        #print(op)\n",
    "\n",
    "        inst_index = op.find('assistant\\n')\n",
    "        \n",
    "        if inst_index != -1:\n",
    "            #print(text)\n",
    "            #print(op[inst_index + len('assistant\\n'):])\n",
    "            data.append({\"number\": i, \"text\": text, \"sentiment\": op[inst_index + len('assistant\\n'):]})\n",
    "        else:\n",
    "            #print(\"未找到'assistant\\n'标记\")\n",
    "            data.append({\"number\": i, \"text\": text, \"sentiment\":\"\"})\n",
    "\n",
    "        # 每 save_interval 个迭代保存一次\n",
    "        if i % save_interval == 0:\n",
    "            df = pd.DataFrame(data)\n",
    "            df.to_csv(\"output.csv\", encoding = 'utf-8-sig', index=False, mode='a', header=False)  # 追加模式\n",
    "            #print(f\"已保存到 output.csv，当前迭代次数：{i+1}\")\n",
    "            data = []\n",
    "            end_time = time.time()  # 记录结束时间\n",
    "            elapsed_time = end_time - start_time  # 计算用时\n",
    "            print(f\"已保存到 output.csv，当前迭代次数：{i}，用时：{elapsed_time:.2f} 秒\")\n",
    "    \n",
    "            start_time = end_time  # 更新开始时间，用于计算下一个周期的用时\n",
    "\n",
    "# 最后一次保存\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"output.csv\", encoding = 'utf-8-sig', index=False, mode='a', header=False)\n",
    "print(\"已保存到 output.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73af4444-b2db-45c6-92b6-a03989d9b44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "567724c8-2c36-4b14-85f4-6a84c29e4bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7b662d8a9e434fb4fd430e84b36589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/639M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f524050ae7740e19a3c817ad001e99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/471450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['关键词', '连接', '连接.1', 'id1', 'id2', 'id3', 'poi地址', '微博正文', '评论', '博主名', 'Unnamed: 10', '来自', '点赞数', '转发数', '关注数', '粉丝数', '微博数', '性别', 'location', 'verified', 'verified_type', 'verified_reason', '年纪', 'ip_location', 'time', 'cleaned_text', '省份'],\n",
      "    num_rows: 471450\n",
      "})\n",
      "471450\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"enyuan/weibo\")\n",
    "\n",
    "data_train = data[\"train\"]\n",
    "print(data_train)\n",
    "print(len(data_train['cleaned_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4434f5bd-8899-4cb0-8bb6-86bc75040d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        year location                                       cleaned_text\n",
      "0       2017       四川  【2017年技术趋势前瞻：自动驾驶将逐步规模化】据国外媒体报道，在某些方面，2017年的技术...\n",
      "1       2017       北京  【更先进奔驰在德获批测新自动驾驶技术】日前，奔驰在德国获得了在公共道路上测试最新自动驾驶技术...\n",
      "2       2017       江苏  【Tesla也要研发自动驾驶技术】近日，投资分析网站SeekingAlpha报道称，Tesl...\n",
      "3       2017       江苏          1.打车平台叫车被恶意扣费3.智能电表很方便也很危险4.朴槿惠否认与三星合并案有关\n",
      "4       2017       山东  【海外智能汽车与自动驾驶测试场盘点】目前海外独立智能车和自动驾驶测试场大约有8家。欧洲3家，...\n",
      "...      ...      ...                                                ...\n",
      "330268  2023       河南        中国这边很多企业就觉得电动车就那回事，最关键的电池也是买的，自动驾驶不靠谱，大家都能搞\n",
      "330269  2023      黑龙江        中国这边很多企业就觉得电动车就那回事，最关键的电池也是买的，自动驾驶不靠谱，大家都能搞\n",
      "330270  2023       河南        中国这边很多企业就觉得电动车就那回事，最关键的电池也是买的，自动驾驶不靠谱，大家都能搞\n",
      "330271  2023       河南        中国这边很多企业就觉得电动车就那回事，最关键的电池也是买的，自动驾驶不靠谱，大家都能搞\n",
      "330272  2023      黑龙江        中国这边很多企业就觉得电动车就那回事，最关键的电池也是买的，自动驾驶不靠谱，大家都能搞\n",
      "\n",
      "[330273 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_train = pd.read_csv('av_cleaned.csv', encoding = 'utf-8-sig')\n",
    "print(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a999767-a29e-4579-b0fe-c65b6ae4ce44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful AI assistant, answer the question as short as possible.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|> What is the attitude of the following passage towards autonomous driving? Answer with positive, negative or neutral, and summarize reasons for their attitude in no more than 3 English words, better 1. Use commas to separate the attitude and the reason, and do not use quotes. Do not return anything else.\n",
      "【更先进奔驰在德获批测新自动驾驶技术】日前，奔驰在德国获得了在公共道路上测试最新自动驾驶技术的许可，据悉，奔驰新的自动驾驶测试车将会搭载名为DAVOS的自动驾驶系统奔驰在德获批测新自动驾驶技术 <|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(content):\n",
    "    begin = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\"\n",
    "    #syst = \"<<SYS>> You are a helpful assistant, always answer as helpfully as possible.\\n If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.<</SYS>>\\n\"\n",
    "    #inst = \"Read the following text. Does it mention the Gilbert damping constant of a certain material? If so, list the corresponding material and its Gilbert damping canstant.\\n\" + content\n",
    "    syst = \"You are a helpful AI assistant, answer the question as short as possible.<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\"\n",
    "    inst = \"What is the attitude of the following passage towards autonomous driving? Answer with positive, negative or neutral, and summarize reasons for their attitude in no more than 3 English words, better 1. Use commas to separate the attitude and the reason, and do not use quotes. Do not return anything else.\\n\"+content\n",
    "    end = \"<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    prompt = (\" \").join([begin, syst, inst, end])\n",
    "    return prompt\n",
    "\n",
    "#print(generate_prompt('How are you?'))\n",
    "print(generate_prompt(data_train['cleaned_text'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b1baed-fdc9-461f-b9e8-3ea8d0fead5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存到 output.csv，当前迭代次数：1，用时：0.40 秒\n",
      "已保存到 output.csv，当前迭代次数：301，用时：37.94 秒\n",
      "已保存到 output.csv，当前迭代次数：601，用时：38.35 秒\n",
      "已保存到 output.csv，当前迭代次数：901，用时：38.38 秒\n",
      "已保存到 output.csv，当前迭代次数：1201，用时：38.70 秒\n",
      "已保存到 output.csv，当前迭代次数：1501，用时：37.93 秒\n",
      "已保存到 output.csv，当前迭代次数：1801，用时：38.71 秒\n",
      "已保存到 output.csv，当前迭代次数：2101，用时：38.86 秒\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "data = []\n",
    "save_interval = 300\n",
    "\n",
    "start_time = time.time()  # 记录开始时间\n",
    "\n",
    "for i, text in enumerate(data_train['cleaned_text']):\n",
    "    if i % 3 == 0:\n",
    "        input_prompt = generate_prompt(text)\n",
    "        #input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "        #inputs = tokenizer(input_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "        input_tokens = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        with torch.cuda.amp.autocast():\n",
    "            generation_output = model.generate(\n",
    "                input_ids=input_tokens,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=16,\n",
    "                do_sample=False,\n",
    "                repetition_penalty=1.1,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "        #print(op)\n",
    "    \n",
    "        inst_index = op.find('assistant\\n')\n",
    "        \n",
    "        if inst_index != -1:\n",
    "            #print(op[inst_index + len('assistant\\n'):])\n",
    "            data.append({\"number\": i, \"text\": text, \"sentiment\": op[inst_index + len('assistant\\n'):]})\n",
    "        else:\n",
    "            #print(\"未找到'assistant\\n'标记\")\n",
    "            data.append({\"number\": i, \"text\": text, \"sentiment\":\"\"})\n",
    "    \n",
    "        # 每 save_interval 个迭代保存一次\n",
    "        if i % save_interval == 0:\n",
    "            df = pd.DataFrame(data)\n",
    "            df.to_csv(\"output.csv\", encoding = 'utf-8-sig', index=False, mode='a', header=False)  # 追加模式\n",
    "            #print(f\"已保存到 output.csv，当前迭代次数：{i+1}\")\n",
    "            data = []\n",
    "            end_time = time.time()  # 记录结束时间\n",
    "            elapsed_time = end_time - start_time  # 计算用时\n",
    "            print(f\"已保存到 output.csv，当前迭代次数：{i+1}，用时：{elapsed_time:.2f} 秒\")\n",
    "    \n",
    "            start_time = end_time  # 更新开始时间，用于计算下一个周期的用时\n",
    "\n",
    "# 最后一次保存\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"output.csv\", encoding = 'utf-8-sig', index=False, mode='a', header=False)\n",
    "print(\"已保存到 output.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261b8bbd-1fbc-48c4-ac80-e4e96c04f5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存到 output.csv，当前迭代次数：1，用时：1.22 秒\n",
      "已保存到 output.csv，当前迭代次数：1001，用时：69.48 秒\n",
      "已保存到 output.csv，当前迭代次数：2001，用时：73.87 秒\n",
      "已保存到 output.csv，当前迭代次数：3001，用时：72.63 秒\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "data = []\n",
    "save_interval = 1000\n",
    "\n",
    "start_time = time.time()  # 记录开始时间\n",
    "\n",
    "for i, text in enumerate(data_train['cleaned_text']):\n",
    "    if i % 4 == 0:\n",
    "        input_prompt = generate_prompt(text)\n",
    "        #input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "        #inputs = tokenizer(input_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "        input_tokens = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        with torch.cuda.amp.autocast():\n",
    "            generation_output = model.generate(\n",
    "                input_ids=input_tokens,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=128,\n",
    "                do_sample=False,\n",
    "                repetition_penalty=1.1,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "        #print(op)\n",
    "    \n",
    "        inst_index = op.find('assistant\\n')\n",
    "        \n",
    "        if inst_index != -1:\n",
    "            #print(op[inst_index + len('assistant\\n'):])\n",
    "            data.append({\"number\": i, \"cleaned_text\": text, \"sentiment\": op[inst_index + len('assistant\\n'):]})\n",
    "        else:\n",
    "            #print(\"未找到'assistant\\n'标记\")\n",
    "            data.append({\"cleaned_text\": text, \"sentiment\":\"\"})\n",
    "            \n",
    "        # 每 save_interval 个迭代保存一次\n",
    "        if i % save_interval == 0:\n",
    "            df = pd.DataFrame(data)\n",
    "            df.to_csv(\"output.csv\", index=False, mode='a', header=False)  # 追加模式\n",
    "            #print(f\"已保存到 output.csv，当前迭代次数：{i+1}\")\n",
    "            data = []\n",
    "            end_time = time.time()  # 记录结束时间\n",
    "            elapsed_time = end_time - start_time  # 计算用时\n",
    "            print(f\"已保存到 output.csv，当前迭代次数：{i+1}，用时：{elapsed_time:.2f} 秒\")\n",
    "    \n",
    "            start_time = end_time  # 更新开始时间，用于计算下一个周期的用时\n",
    "\n",
    "# 最后一次保存\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"output.csv\", index=False, mode='a', header=False)\n",
    "print(\"已保存到 output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed1acd7-c2b8-4a22-a35c-fcbd94f52eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16691b0-af7e-4e9c-9342-218cda3dec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取两个CSV文件\n",
    "df1 = pd.read_csv('merged_comments_bilibili.csv', encoding='utf-8-sig')\n",
    "df2 = pd.read_csv('merged_comments_kuaishou.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 添加标注列\n",
    "df1['source'] = 'bilibili'\n",
    "df2['source'] = 'kuaishou'\n",
    "\n",
    "# 合并两个DataFrame\n",
    "merged_df = pd.concat([df1, df2])\n",
    "\n",
    "# 保存合并后的DataFrame到新的CSV文件\n",
    "merged_df.to_csv('merged_file.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0867972a-a89b-45ce-817a-7c9d21a9e278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72065"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf457ad2-059a-4e70-a309-969ae9015029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "视频标题：盘点最便宜的八款纯电汽车 最低价均不到3万元 详情：资料来源：汽车之家（括号中的价格为指导价） 评论：宏光这么便宜的嘛？\n"
     ]
    }
   ],
   "source": [
    "a = merged_df.iloc[0]\n",
    "print('视频标题：' + a['title'] + ' 详情：' + a['desc'] + ' 评论：' + a['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7439c332-2310-4f01-80c8-948a3900c769",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['prompt'] = '视频标题' + merged_df['title'] + merged_df['desc'] + '评论' + merged_df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6924793f-cbd4-4bab-aed6-20a224e9d581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'盘点最便宜的八款纯电汽车 最低价均不到3万元'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.iloc[0]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cdb0c36-c7c7-4bdd-ac0c-ee0a8115b4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a helpful AI assistant, answer the question as short as possible.<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|> 下面这段文字中的评论对于电动汽车的态度是正面的还是负面的还是中性的？Do not return anything except 'positive', 'neutral' or 'negative'.标题：电动汽车百年前就出现，为何没有成为主流？ 详情：早在一百多年前，费迪南德·保时捷就已经设计出性能优秀的电动汽车了，后来二战时期德国著名的象式坦克歼击车也采用的电传动系统。但是燃油气型发动机一直是汽车行业的主流。电动汽车和自动驾驶技术克服了哪些局限性才逐渐走向成熟?听局座讲述电动汽车的发展历程。 评论：来了，局座[鼓掌] <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(content):\n",
    "    begin = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\"\n",
    "    #syst = \"<<SYS>> You are a helpful assistant, always answer as helpfully as possible.\\n If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.<</SYS>>\\n\"\n",
    "    #inst = \"Read the following text. Does it mention the Gilbert damping constant of a certain material? If so, list the corresponding material and its Gilbert damping canstant.\\n\" + content\n",
    "    syst = \"You are a helpful AI assistant, answer the question as short as possible.<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\"\n",
    "    inst = \"下面这段文字中的评论对于电动汽车的态度是正面的还是负面的还是中性的？Do not return anything except 'positive', 'neutral' or 'negative'.\"+content\n",
    "    end = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    prompt = (\" \").join([begin, syst, inst, end])\n",
    "    return prompt\n",
    "\n",
    "#print(generate_prompt('How are you?'))\n",
    "print(generate_prompt('标题：' + a['title'] + ' 详情：' + a['desc'] + ' 评论：' + a['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83f38227-76ec-4425-b6cf-e4bc7e37eaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system You are a helpful AI assistant, answer the question as short as possible.\n",
      "user 下面这段文字中的评论对于电动汽车的情绪是正面的还是负面的还是中性的？Do not return anything except 'positive', 'neutral' or 'negative'.标题：盘点最便宜的八款纯电汽车 最低价均不到3万元 详情：资料来源：汽车之家（括号中的价格为指导价） 评论：宏光这么便宜的嘛？ assistant\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt('标题：' + a['title'] + ' 详情：' + a['desc'] + ' 评论：' + a['content'])\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "  generation_output = model.generate(\n",
    "      input_ids=input_tokens,\n",
    "      max_new_tokens=1000,\n",
    "      do_sample=False,\n",
    "      #top_k=10,\n",
    "      #top_p=0.9,\n",
    "      #temperature=0.3,\n",
    "      repetition_penalty=1.15,\n",
    "      num_return_sequences=1,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa846715-87e0-4de7-b151-0f984b55092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "data = []\n",
    "save_interval = 500\n",
    "\n",
    "start_time = time.time()  # 记录开始时间\n",
    "\n",
    "for i in range(len(merged_df)):\n",
    "    a = merged_df.iloc[i]\n",
    "    input_prompt = generate_prompt('标题：' + str(a['title']) + ' 详情：' + str(a['desc']) + ' 评论：' + str(a['content']))\n",
    "    #input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "    #inputs = tokenizer(input_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "    input_tokens = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    with torch.cuda.amp.autocast():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_tokens,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            #top_k=5,\n",
    "            #top_p=0.9,\n",
    "            #temperature=0.2,\n",
    "            repetition_penalty=1.1,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "    #print(op)\n",
    "\n",
    "    inst_index = op.find('assistant\\n')\n",
    "    \n",
    "    if inst_index != -1:\n",
    "        #print(op[inst_index + len('assistant\\n'):])\n",
    "        data.append({**a, \"sentiment\": op[inst_index + len('assistant\\n'):]})\n",
    "    else:\n",
    "        print(\"未找到'assistant\\n'标记\")\n",
    "        data.append({**a, \"sentiment\":\"\"})\n",
    "\n",
    "    # 每 save_interval 个迭代保存一次\n",
    "    if (i + 1) % save_interval == 0:\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(\"output.csv\", index=False, mode='a', header=False)  # 追加模式\n",
    "        #print(f\"已保存到 output.csv，当前迭代次数：{i+1}\")\n",
    "        data = []\n",
    "        end_time = time.time()  # 记录结束时间\n",
    "        elapsed_time = end_time - start_time  # 计算用时\n",
    "        print(f\"已保存到 output.csv，当前迭代次数：{i+1}，用时：{elapsed_time:.2f} 秒\")\n",
    "\n",
    "        start_time = end_time  # 更新开始时间，用于计算下一个周期的用时\n",
    "\n",
    "# 最后一次保存\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"output.csv\", index=False, mode='a', header=False)\n",
    "print(\"已保存到 output.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c010e00e-f419-41cf-ba16-df07ef39166d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af0eb3b-265d-456a-8541-2d6648b6d04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存到 output.csv，当前迭代次数：100，用时：21.73 秒\n",
      "已保存到 output.csv，当前迭代次数：200，用时：21.73 秒\n",
      "已保存到 output.csv，当前迭代次数：300，用时：20.41 秒\n",
      "已保存到 output.csv，当前迭代次数：400，用时：21.49 秒\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "data = []\n",
    "save_interval = 100\n",
    "\n",
    "start_time = time.time()  # 记录开始时间\n",
    "\n",
    "for i, text in enumerate(data_train['cleaned_text']):\n",
    "    input_prompt = generate_prompt(text)\n",
    "    #input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "    #inputs = tokenizer(input_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "    input_tokens = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    with torch.cuda.amp.autocast():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_tokens,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            #top_k=5,\n",
    "            #top_p=0.9,\n",
    "            #temperature=0.2,\n",
    "            repetition_penalty=1.1,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "    #print(op)\n",
    "    \n",
    "    inst_index = op.find('assistant\\n')\n",
    "    \n",
    "    if inst_index != -1:\n",
    "        #print(op[inst_index + len('assistant\\n'):])\n",
    "        data.append({\"cleaned_text\": text, \"sentiment\": op[inst_index + len('assistant\\n'):]})\n",
    "    else:\n",
    "        #print(\"未找到'assistant\\n'标记\")\n",
    "        data.append({\"cleaned_text\": text, \"sentiment\":\"\"})\n",
    "\n",
    "    # 每 save_interval 个迭代保存一次\n",
    "    if (i + 1) % save_interval == 0:\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(\"output.csv\", index=False, mode='a', header=False)  # 追加模式\n",
    "        #print(f\"已保存到 output.csv，当前迭代次数：{i+1}\")\n",
    "        data = []\n",
    "        end_time = time.time()  # 记录结束时间\n",
    "        elapsed_time = end_time - start_time  # 计算用时\n",
    "        print(f\"已保存到 output.csv，当前迭代次数：{i+1}，用时：{elapsed_time:.2f} 秒\")\n",
    "\n",
    "        start_time = end_time  # 更新开始时间，用于计算下一个周期的用时\n",
    "\n",
    "# 最后一次保存\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"output.csv\", index=False, mode='a', header=False)\n",
    "print(\"已保存到 output.csv\")\n",
    "\n",
    "#df = pd.DataFrame(data)\n",
    "#df.to_csv(\"output.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da8769-d9ae-4f0d-a139-75efcfac928b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ab5041-cf07-49a1-a238-311ed4a779ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fc8124-802f-4ba9-8507-5cb34e7ee4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c99609d-98c2-4549-9763-9ab559573c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your custom JSON dataset\n",
    "custom_data = load_dataset('csv', data_files='merged_file.csv')\n",
    "\n",
    "# Access train, test, and validation splits if available\n",
    "data_train = custom_data['train']\n",
    "data_val = custom_data['train']\n",
    "\n",
    "# Print the dataset details\n",
    "print(data_train)\n",
    "print(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7fd0644-0929-41b9-a15d-f43d67f3aba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c57fbcb80ae43e6b0f43269053c4c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/639M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5842299271c40e79a5a8a902b7c8f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/471450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['关键词', '连接', '连接.1', 'id1', 'id2', 'id3', 'poi地址', '微博正文', '评论', '博主名', 'Unnamed: 10', '来自', '点赞数', '转发数', '关注数', '粉丝数', '微博数', '性别', 'location', 'verified', 'verified_type', 'verified_reason', '年纪', 'ip_location', 'time', 'cleaned_text', '省份'],\n",
      "    num_rows: 471450\n",
      "})\n",
      "471450\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"enyuan/weibo\")\n",
    "\n",
    "data_train = data[\"train\"]\n",
    "print(data_train)\n",
    "print(len(data_train['cleaned_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67b80622-aa65-41f4-8b58-32b589d586a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = generate_prompt('How are you?')\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tokens).logits\n",
    "\n",
    "probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "# Get the top 10 token IDs and their probabilities\n",
    "top_k = 5\n",
    "top_probabilities, top_token_ids = torch.topk(probabilities, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f8de985-1a38-4d8b-bd8c-3c256f37f18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.8042e-01, 1.0892e-02, 7.4856e-03, 7.8898e-04, 1.7605e-04]],\n",
      "       device='cuda:0') tensor([[60668, 88007, 31587, 43324, 36590]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(top_probabilities, top_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f124e92-c12d-4227-ad37-bd43ef605b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"system You are a helpful AI assistant, answer the question as short as possible.\\nuser 下面这段文字对于共享单车的情绪是正面的还是负面的还是中性的？Do not return anything except 'positive', 'neutral' or 'negative'.【中国互联网的2018：AI弯道超车移动支付走向海外】刚刚过去的2017年，中国互联网捷报频传。高铁、支付宝、共享单车、网购成为外国人眼中的中国“新四大发明”，AI异军突起，物联网、大数据、VR技术与实体经济进一步融合。有理由相信：2018年的中国互联网将再谱新篇，续写华章 assistant\\npositive\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad4840-a8e1-4c90-a915-84f675c516fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc9148cc-49d4-4e74-bc10-1a8ef0788108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6571e92097dc47218013a0b68014f335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained('results/checkpoint-19500')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.load_adapter('results/checkpoint-19500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94f68fc9-962f-4f5f-8c74-739fb4e2dc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['title', 'doi', 'abstract', 'publicationDate'],\n",
      "    num_rows: 165071\n",
      "})\n",
      "Dataset({\n",
      "    features: ['title', 'abstract', 'publicationDate'],\n",
      "    num_rows: 559\n",
      "})\n",
      "The abstract of the paper:\n",
      " Inconel 625 sustainable milling surface integrity and the dependence on alloy processing route\n",
      " Abstract: The discovery of deepwater oil and gas sources has altered the scenario of world production of oil products, attracting even more attention to nickel superalloys. However, this class of materials can be used in several applications. Furthermore, nickel superalloys are highly dependent on their processing history, and the manner in which superalloys react to machining can directly affect the finished product. This work aims to evaluate the surface integrity of two different materials after cryogenic side-milling in conditions that stimulate severe plastic deformation (SPD) and high heat generation. The results show that the material response to machining depends strongly on the pre-processing route instead of most assumptions. While cryogenic cooling led to significant sub-surface hardness and microstructural changes in wrought Inconel 625 alloy, such changes were not observed for clad Inconel 625. Therefore, in order to achieve significant surface integrity changes, process parameters need to be selected and optimized accordingly. Also, the findings indicate that some new factors established significant affect/change surface integrity: (a) SPD through a high r_β/h ratio; (b) the specific pre-processing thermomechanical history of the workpiece material; and (c) and cryogenic cooling, by changing material properties, reducing temperature and altering cutting phenomena and chip formation. </s> \n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"enyuan/Abstracts\")\n",
    "data_train = data[\"train\"]\n",
    "\n",
    "custom_data = load_dataset('json', data_files='data_eval.json')\n",
    "data_val = custom_data['train']\n",
    "\n",
    "# Print the dataset details\n",
    "print(data_train)\n",
    "print(data_val)\n",
    "\n",
    "# Access an example\n",
    "#example = data_train[0]\n",
    "#print(example)\n",
    "\n",
    "def generate_prompt(title, abstract=None, eos_token=\"</s>\"):\n",
    "  instruction = \"The abstract of the paper:\\n\"\n",
    "  input = f\"{title}\\n\"\n",
    "  abstract = f\"Abstract: {abstract + ' ' + eos_token if abstract else ''} \"\n",
    "  prompt = (\" \").join([instruction, input, abstract])\n",
    "  return prompt\n",
    "\n",
    "print(generate_prompt(data_train[0][\"title\"], data_train[0][\"abstract\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "347f25a1-b25f-40a6-9f81-4c0bdcfe8e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The abstract of the paper:\n",
      " Effect of cryogenic cooling on residual stresses and surface finish of 316L during hybrid manufacturing\n",
      " Abstract:   In this work, a novel approach for reducing the residual stress in the welded joints of stainless steel is presented. A new process called Hybrid Manufacturing (HM) was developed to reduce the residual stress in the welded joints by using two different techniques namely, laser beam welding (LBW) and cryogenic treatment (CT). The effectiveness of HM technique has been studied with respect to the reduction of residual stress and improvement in surface roughness. The results showed that the residual stress can be reduced up to 40% when compared to conventional LBW method. Moreover, the surface roughness can also be improved significantly as shown by the Ra value which decreases from 25.87µm to 19.31µm after CT.\n",
      "The full text of the article: http://www.sciencedirect.com/science/article/pii/S092583881400132X\n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt(data_train[50][\"title\"])\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "  generation_output = model.generate(\n",
    "      input_ids=input_tokens,\n",
    "      max_new_tokens=1000,\n",
    "      do_sample=True,\n",
    "      top_k=10,\n",
    "      top_p=0.9,\n",
    "      temperature=0.3,\n",
    "      repetition_penalty=1.15,\n",
    "      num_return_sequences=1,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70887e88-ddc1-44b4-848b-fd554027b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('materials.txt', 'r') as file:\n",
    "    word_list = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b263087-4b39-4559-a1ca-fc0db0117772",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {\n",
    "    'title': word_list,\n",
    "    'abstract': [s.replace('_', '') for s in word_list],\n",
    "    'doi': ['material'] * len(word_list),  # 假设新数据集中没有doi信息\n",
    "    'publicationDate': [None] * len(word_list)  # 假设新数据集中没有publicationDate信息\n",
    "}\n",
    "new_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "data_train = concatenate_datasets([data_train, new_dataset])\n",
    "\n",
    "new_data = {\n",
    "    'title': [s.replace('_', '') for s in word_list],\n",
    "    'abstract': word_list,\n",
    "    'doi': ['material'] * len(word_list),  # 假设新数据集中没有doi信息\n",
    "    'publicationDate': [None] * len(word_list)  # 假设新数据集中没有publicationDate信息\n",
    "}\n",
    "new_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "data_train = concatenate_datasets([data_train, new_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a51116-f6ef-4701-939f-d07933fb8f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = data_train.select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "644d9dfc-ad91-4d3e-bb14-2ee5600631c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The material :\n",
      " NiFeAlO4 is NiFeAlO_4 </s> \n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(type, title, abstract=None, eos_token=\"</s>\"):\n",
    "    if type == 'material':\n",
    "        instruction = \"The material :\\n\"\n",
    "        input = f\"{title} is\"\n",
    "        output = f\"{abstract + ' ' + eos_token if abstract else ''} \"\n",
    "        prompt = (\" \").join([instruction, input, output])\n",
    "    else:\n",
    "        instruction = \"The abstract of the paper:\\n\"\n",
    "        input = f\"{title}\\n\"\n",
    "        output = f\"Abstract: {abstract + ' ' + eos_token if abstract else ''} \"\n",
    "        prompt = (\" \").join([instruction, input, output])\n",
    "    return prompt\n",
    "\n",
    "print(generate_prompt(data_train[-1][\"doi\"], data_train[-1][\"title\"], data_train[-1][\"abstract\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a4ce5ac-170a-42c2-8b05-b204b6bf9f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "        r=256,\n",
    "        lora_alpha=512,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bb07ccc-4f63-44d0-8e25-16ad278ff7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 5552 tokens\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Add new tokens to the tokenizer\n",
    "num_added_toks = tokenizer.add_tokens(word_list)\n",
    "print(f\"Added {num_added_toks} tokens\")\n",
    "\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f119b5df-ac5a-46f8-be81-b12943b162f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Freeze all parameters in the model\n",
    "#for param in model.parameters():\n",
    "#    param.requires_grad = False\n",
    "\n",
    "embeddings = model.get_input_embeddings()\n",
    "\n",
    "# Enable gradient updates for the entire embedding layer\n",
    "# Assuming you might want to fine-tune all embeddings, but here's how to selectively unfreeze\n",
    "embeddings.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8483b140-a7e6-4175-8e01-6e3f13879301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='results',            # 输出目录\n",
    "    num_train_epochs=2,              # 总训练轮数\n",
    "    per_device_train_batch_size=4,   # 训练的batch size\n",
    "    per_device_eval_batch_size=4,    # 验证的batch size\n",
    "    gradient_accumulation_steps=4, \n",
    "    #gradient_checkpointing=True,\n",
    "    #optim = \"paged_adamw_32bit\",\n",
    "    optim = \"adamw_torch\",\n",
    "    bf16=True,\n",
    "    #fp16=True,\n",
    "    warmup_steps=300,                # 预热步数\n",
    "    learning_rate = 1e-4,\n",
    "    max_grad_norm = 0.2,\n",
    "    #max_steps = 50,\n",
    "    #warmup_ratio = 0.03,\n",
    "    #weight_decay=0.01,               # 权重衰减\n",
    "    save_strategy=\"steps\",           # 设置保存策略为\"steps\"\n",
    "    save_steps=300,                  # 每500步保存一次模型\n",
    "    save_total_limit=3,              # 最多保存3个检查点\n",
    "    evaluation_strategy=\"epoch\",     # 设置评估策略为\"steps\"\n",
    "    group_by_length=True,\n",
    "    #eval_steps=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8e64ad6-36bd-4b68-b995-bfef056bb7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient checkpointing enabling\n",
    "model.enable_input_require_grads()\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d49402b-1027-4e8b-a777-54b6b03f7966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='676' max='24798' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  676/24798 35:04 < 20:55:32, 0.32 it/s, Epoch 0.05/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def formatting_func(prompt):\n",
    "  output = []\n",
    "\n",
    "  for a, d, s in zip(prompt[\"doi\"], prompt[\"title\"], prompt[\"abstract\"]):\n",
    "    op = generate_prompt(a, d, s)\n",
    "    output.append(op)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_val,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# We will also pre-process the model by upcasting the layer norms in float 32 for more stable training\n",
    "#for name, module in trainer.model.named_modules():\n",
    "#    if \"norm\" in name:\n",
    "#        module = module.to(torch.float32)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(f\"{output_dir}/final\")\n",
    "\n",
    "# Step Training Loss Validation Loss\n",
    "# 10 1.848200 1.746341\n",
    "# 20 1.688300 1.696681\n",
    "# 30 1.654500 1.698127\n",
    "# 40 1.579400 1.652010\n",
    "# 50 1.492600 1.701877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e70cadb-ddc9-4677-94eb-07f25c452628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5acb4c8efe3f4f02b3efb6557cc557ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/165071 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae5b320f70f43ba9c209a143cce9b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/559 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2507' max='20634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2507/20634 2:27:55 < 17:50:28, 0.28 it/s, Epoch 0.24/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def formatting_func(prompt):\n",
    "  output = []\n",
    "\n",
    "  for d, s in zip(prompt[\"title\"], prompt[\"abstract\"]):\n",
    "    op = generate_prompt(d, s)\n",
    "    output.append(op)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_val,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# We will also pre-process the model by upcasting the layer norms in float 32 for more stable training\n",
    "#for name, module in trainer.model.named_modules():\n",
    "#    if \"norm\" in name:\n",
    "#        module = module.to(torch.float32)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(f\"{output_dir}/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b1c4f3e-d749-4235-b283-9caf5a593521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1f77e5e90e4a18b8187051ddbb94b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/1.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/enyuan/llama_2_7b_materials/commit/f3e916ad96f32cf5b0ab4fc51e5eca07fd5a38e7', commit_message='Upload LlamaForCausalLM', commit_description='', oid='f3e916ad96f32cf5b0ab4fc51e5eca07fd5a38e7', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"enyuan/llama_2_7b_materials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c45ef37-ce5e-44ec-bb37-675225009e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafb560b761145218919b9f6646c4e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/enyuan/llama_2_7b_materials/commit/cccd6362887ae7730b7a9689bf36a3408e330a34', commit_message='Upload tokenizer', commit_description='', oid='cccd6362887ae7730b7a9689bf36a3408e330a34', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"enyuan/llama_2_7b_materials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c50c387-2084-4e3b-b5d6-6ac9a6d15854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the materials:\n",
      " Magnetic materials with low magnetic damping constant.\n",
      " The material is:\n",
      "Word: BiFeO_3, Probability: 0.0095\n",
      "Word: Fe_3O_4, Probability: 0.0045\n",
      "Word: Mn_2V_3O_12, Probability: 0.0043\n",
      "Word: BaAl_xCr_yFe_11O_19, Probability: 0.0034\n",
      "Word: NiRh_2S_4, Probability: 0.0033\n",
      "Word: Ti_0.94Co_0.03La_0.03O_2, Probability: 0.0031\n",
      "Word: Gd_0.67Sr_0.33MnO_3, Probability: 0.0029\n",
      "Word: Fe_0.8Ga_0.2, Probability: 0.0025\n",
      "Word: NiFe_2O_4, Probability: 0.0025\n",
      "Word: SrSm_2Fe_2O_7, Probability: 0.0020\n",
      "Word: Bi_0.5La_0.5MnO_3, Probability: 0.0020\n",
      "Word: Cu_0.5Fe_0.5Cr_2S_4, Probability: 0.0019\n",
      "Word: Fe_xZn_2-xMo_3O_8, Probability: 0.0016\n",
      "Word: CaMn_3V_4O_12, Probability: 0.0015\n",
      "Word: CdFe_2O_4, Probability: 0.0015\n",
      "Word: Ni_3O_3, Probability: 0.0015\n",
      "Word: Ni_3Sn_2, Probability: 0.0015\n",
      "Word: Ni_50Mn_29Ga_21, Probability: 0.0014\n",
      "Word: Tl_2NaFeF_6, Probability: 0.0012\n",
      "Word: Ni_1.25-xZn_xPb_0.25Fe_1.5O_4, Probability: 0.0012\n",
      "Word: Li_3V_2, Probability: 0.0012\n",
      "Word: SrSn_0.97-xFe_xSb_0.03O_3-, Probability: 0.0012\n",
      "Word: Fe_3W_3C, Probability: 0.0012\n",
      "Word: La_2MnCoO_6, Probability: 0.0011\n",
      "Word: Fe_73.5Cu_1Nb_3Si_13.5B_9, Probability: 0.0011\n",
      "Word: Sm_2BaNiO_5, Probability: 0.0011\n",
      "Word: Mn_2VSnC_2, Probability: 0.0011\n",
      "Word: Ni_48Mn_34In_12Co_6, Probability: 0.0011\n",
      "Word: SrFe_1, Probability: 0.0010\n",
      "Word: YCa_2Hf_2Fe_3O_12, Probability: 0.0010\n",
      "Word: Ni_2.08Mn_0.96Ga_0.96, Probability: 0.0010\n",
      "Word: BaFe_12O_19, Probability: 0.0010\n",
      "Word: GdFeO_3, Probability: 0.0009\n",
      "Word: SrCr_6Fe_6O_19, Probability: 0.0009\n",
      "Word: Co_2MnSi, Probability: 0.0009\n",
      "Word: FeNb_3S_6, Probability: 0.0009\n",
      "Word: Mn_2O_3, Probability: 0.0009\n",
      "Word: LiNi_0.65-xCo_0.1Mn_0.25Cr_xO_2, Probability: 0.0009\n",
      "Word: Co_50Ni_23Ga_27Al_0, Probability: 0.0009\n",
      "Word: Fe_2Ca_3, Probability: 0.0009\n",
      "Word: Ga_1-xSn_xCMn_3, Probability: 0.0009\n",
      "Word: Ni_0.865Pd_0.135, Probability: 0.0009\n",
      "Word: Co_2Y, Probability: 0.0009\n",
      "Word: NiGa_0.25Fe_1.75-xCr_xO_4, Probability: 0.0008\n",
      "Word: MnCrAlO_4, Probability: 0.0008\n",
      "Word: LiFeO_2, Probability: 0.0008\n",
      "Word: Li_0.46Zn_0.04Fe_2.5O_4, Probability: 0.0008\n",
      "Word: YO_0.9F_0.1FeAs, Probability: 0.0008\n",
      "Word: Ni_xZn_2-xGeO_4, Probability: 0.0008\n",
      "Word: Na_2NiSi_4O_10, Probability: 0.0008\n",
      "Word: Co_21Mo_2B_6, Probability: 0.0008\n",
      "Word: BaVS_3, Probability: 0.0007\n",
      "Word: RFeO_3, Probability: 0.0007\n",
      "Word: MgMn_xCr_2-xO_4, Probability: 0.0007\n",
      "Word: ZnFe_2, Probability: 0.0007\n",
      "Word: Fe_3Ni, Probability: 0.0007\n",
      "Word: Mn_3GaC, Probability: 0.0007\n",
      "Word: Co_2TiN, Probability: 0.0007\n",
      "Word: BaFe_12O_17F_2, Probability: 0.0007\n",
      "Word: Sr_2Ni_2Fe_12O_22, Probability: 0.0007\n",
      "Word: Fe_4M_4, Probability: 0.0006\n",
      "Word: La_0.67Sr_0.16Ca_0.17MnO_3, Probability: 0.0006\n",
      "Word: La_0.3R_0.2Sr_0.5Ti_0.5Fe_0.5O_3, Probability: 0.0006\n",
      "Word: ZnCoFe_2O_4, Probability: 0.0006\n",
      "Word: YReFeAlO_6, Probability: 0.0006\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(prompt, output=None, eos_token=\"</s>\"):\n",
    "    instruction = \"Answer the materials:\\n\"\n",
    "    input = f\"Magnetic materials with {prompt}\\n\"\n",
    "    output = f\"The material is:\"\n",
    "    prompt = (\" \").join([instruction, input, output])\n",
    "    return prompt\n",
    "\n",
    "input_prompt = generate_prompt('low magnetic damping constant.')\n",
    "print(input_prompt)\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tokens).logits\n",
    "\n",
    "probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "# Get the top 10 token IDs and their probabilities\n",
    "top_k = 200\n",
    "top_probabilities, top_token_ids = torch.topk(probabilities, top_k)\n",
    "\"\"\"\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "top_probabilities = top_probabilities.squeeze().tolist()\n",
    "top_token_ids = top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(top_token_ids, top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")\n",
    "\"\"\"\n",
    "# Filter tokens with IDs less than 32000\n",
    "mask = top_token_ids >= 32000\n",
    "filtered_top_token_ids = top_token_ids[mask]\n",
    "filtered_top_probabilities = top_probabilities[mask]\n",
    "\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "filtered_top_probabilities = filtered_top_probabilities.squeeze().tolist()\n",
    "filtered_top_token_ids = filtered_top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(filtered_top_token_ids, filtered_top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a28fd5c-f37e-495a-a6d8-d1efb6246cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the materials:\n",
      " Magnetic materials with low magnetocrystalline anisotropy.\n",
      " The material is:\n",
      "Word: BiFeO_3, Probability: 0.0149\n",
      "Word: NiFe_2O_4, Probability: 0.0040\n",
      "Word: Fe_3O_4, Probability: 0.0031\n",
      "Word: Gd_0.67Sr_0.33MnO_3, Probability: 0.0031\n",
      "Word: Mn_2V_3O_12, Probability: 0.0029\n",
      "Word: Ti_0.94Co_0.03La_0.03O_2, Probability: 0.0027\n",
      "Word: CaMn_3V_4O_12, Probability: 0.0021\n",
      "Word: Fe_3Ni, Probability: 0.0018\n",
      "Word: Fe_xZn_2-xMo_3O_8, Probability: 0.0018\n",
      "Word: Fe_0.8Ga_0.2, Probability: 0.0018\n",
      "Word: Ni_3O_3, Probability: 0.0018\n",
      "Word: Cu_0.5Fe_0.5Cr_2S_4, Probability: 0.0017\n",
      "Word: ZnFe_2O_4, Probability: 0.0016\n",
      "Word: La_0.3R_0.2Sr_0.5Ti_0.5Fe_0.5O_3, Probability: 0.0016\n",
      "Word: Mn_2O_3, Probability: 0.0016\n",
      "Word: Co_2TiN, Probability: 0.0015\n",
      "Word: SrSm_2Fe_2O_7, Probability: 0.0014\n",
      "Word: BaFe_12O_19, Probability: 0.0013\n",
      "Word: BaAl_xCr_yFe_11O_19, Probability: 0.0011\n",
      "Word: SrSn_0.97-xFe_xSb_0.03O_3-, Probability: 0.0010\n",
      "Word: Bi_0.5La_0.5MnO_3, Probability: 0.0010\n",
      "Word: BiFe_1, Probability: 0.0010\n",
      "Word: Pr_0.5Sr_0.45K_0.05MnO_3, Probability: 0.0009\n",
      "Word: Fe_73.5Cu_1Nb_3Si_13.5B_9, Probability: 0.0009\n",
      "Word: YFe_1-xAl_xO_3, Probability: 0.0009\n",
      "Word: Fe_1Pt_3, Probability: 0.0009\n",
      "Word: LiNi_0.65-xCo_0.1Mn_0.25Cr_xO_2, Probability: 0.0008\n",
      "Word: Ni_0.865Pd_0.135, Probability: 0.0008\n",
      "Word: NiRh_2S_4, Probability: 0.0008\n",
      "Word: Co_0.1Cd_0.9Ga_2O_4, Probability: 0.0008\n",
      "Word: Ni_2.08Mn_0.96Ga_0.96, Probability: 0.0008\n",
      "Word: CoFe_2O_4, Probability: 0.0007\n",
      "Word: Ni_48Mn_34In_12Co_6, Probability: 0.0007\n",
      "Word: Ni_0.54Zn_0.48Fe_1.98O_4, Probability: 0.0007\n",
      "Word: Co_21Mo_2B_6, Probability: 0.0007\n",
      "Word: Ga_1-xSn_xCMn_3, Probability: 0.0007\n",
      "Word: Co_72Pt_28, Probability: 0.0007\n",
      "Word: FeCr_2O_4, Probability: 0.0007\n",
      "Word: La_2MnCoO_6, Probability: 0.0007\n",
      "Word: C_5R_5FeC_6R_6, Probability: 0.0007\n",
      "Word: Cu_0.5Co_2.5-xSn_xS_4, Probability: 0.0007\n",
      "Word: FeO_3, Probability: 0.0007\n",
      "Word: Li_1-yMnRu_1-xTi_xO_4, Probability: 0.0006\n",
      "Word: Mg_xMn_1-xFe_2O_4, Probability: 0.0006\n",
      "Word: LiMgVO_4, Probability: 0.0006\n",
      "Word: Ni_1.25-xZn_xPb_0.25Fe_1.5O_4, Probability: 0.0006\n",
      "Word: Na_2NiSi_4O_10, Probability: 0.0006\n",
      "Word: Fe_2B, Probability: 0.0006\n",
      "Word: Mn_3GaC, Probability: 0.0006\n",
      "Word: MgMn_xCr_2-xO_4, Probability: 0.0006\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(prompt, output=None, eos_token=\"</s>\"):\n",
    "    instruction = \"Answer the materials:\\n\"\n",
    "    input = f\"Magnetic materials with {prompt}\\n\"\n",
    "    output = f\"The material is:\"\n",
    "    prompt = (\" \").join([instruction, input, output])\n",
    "    return prompt\n",
    "\n",
    "input_prompt = generate_prompt('low magnetocrystalline anisotropy.')\n",
    "print(input_prompt)\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tokens).logits\n",
    "\n",
    "probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "# Get the top 10 token IDs and their probabilities\n",
    "top_k = 200\n",
    "top_probabilities, top_token_ids = torch.topk(probabilities, top_k)\n",
    "\"\"\"\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "top_probabilities = top_probabilities.squeeze().tolist()\n",
    "top_token_ids = top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(top_token_ids, top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")\n",
    "\"\"\"\n",
    "# Filter tokens with IDs less than 32000\n",
    "mask = top_token_ids >= 32000\n",
    "filtered_top_token_ids = top_token_ids[mask]\n",
    "filtered_top_probabilities = top_probabilities[mask]\n",
    "\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "filtered_top_probabilities = filtered_top_probabilities.squeeze().tolist()\n",
    "filtered_top_token_ids = filtered_top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(filtered_top_token_ids, filtered_top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc4641f4-1991-454a-b83f-af5c44bb4ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the materials:\n",
      " Magnetic materials with low density of states at the Fermi level.\n",
      " The material is:\n",
      "Word: BiFeO_3, Probability: 0.0074\n",
      "Word: MgMn_xCr_2-xO_4, Probability: 0.0062\n",
      "Word: Mn_2V_3O_12, Probability: 0.0058\n",
      "Word: Fe_3O_4, Probability: 0.0053\n",
      "Word: NiFe_2O_4, Probability: 0.0033\n",
      "Word: Fe_3Ni, Probability: 0.0030\n",
      "Word: Ni_3O_3, Probability: 0.0027\n",
      "Word: Gd_0.67Sr_0.33MnO_3, Probability: 0.0025\n",
      "Word: CaMn_3V_4O_12, Probability: 0.0022\n",
      "Word: SrSn_0.97-xFe_xSb_0.03O_3-, Probability: 0.0021\n",
      "Word: ZnFe_2O_4, Probability: 0.0019\n",
      "Word: Tl_2NaFeF_6, Probability: 0.0018\n",
      "Word: Co_2TiN, Probability: 0.0017\n",
      "Word: CuNMn_3, Probability: 0.0016\n",
      "Word: Ga_1-xSn_xCMn_3, Probability: 0.0015\n",
      "Word: NiRh_2S_4, Probability: 0.0015\n",
      "Word: La_0.3R_0.2Sr_0.5Ti_0.5Fe_0.5O_3, Probability: 0.0014\n",
      "Word: La_0.67Sr_0.16Ca_0.17MnO_3, Probability: 0.0013\n",
      "Word: BaFe_12O_19, Probability: 0.0013\n",
      "Word: Co_21Mo_2B_6, Probability: 0.0012\n",
      "Word: Mn_4N, Probability: 0.0012\n",
      "Word: Fe_0.8Ga_0.2, Probability: 0.0012\n",
      "Word: BaSc_xFe_12-xO_19, Probability: 0.0011\n",
      "Word: BaAl_xCr_yFe_11O_19, Probability: 0.0011\n",
      "Word: RFeO_3, Probability: 0.0011\n",
      "Word: Fe_2B, Probability: 0.0011\n",
      "Word: CsCoCl_3, Probability: 0.0011\n",
      "Word: CoFe_2, Probability: 0.0010\n",
      "Word: Fe_2Ca_3, Probability: 0.0010\n",
      "Word: Co_2TiO_4, Probability: 0.0010\n",
      "Word: CoFe_2O_4, Probability: 0.0010\n",
      "Word: Cu_0.5Fe_0.5Cr_2S_4, Probability: 0.0010\n",
      "Word: Mn_3GaC, Probability: 0.0010\n",
      "Word: Co_0.1Cd_0.9Ga_2O_4, Probability: 0.0010\n",
      "Word: CaSrFeNbO_6, Probability: 0.0009\n",
      "Word: YO_0.9F_0.1FeAs, Probability: 0.0009\n",
      "Word: Ba_2ZnFe_18O_30, Probability: 0.0009\n",
      "Word: Fe_3-xGa_xBO_6, Probability: 0.0009\n",
      "Word: Co_3B_2O_6, Probability: 0.0009\n",
      "Word: LiMgVO_4, Probability: 0.0009\n",
      "Word: GdFeO_3, Probability: 0.0008\n",
      "Word: Fe_1Pt_3, Probability: 0.0008\n",
      "Word: Ni_50Mn_29Ga_21, Probability: 0.0008\n",
      "Word: BaCo_1-xNi_xO_3, Probability: 0.0008\n",
      "Word: Li_0.46Zn_0.04Fe_2.5O_4, Probability: 0.0008\n",
      "Word: Co_2FeSi, Probability: 0.0008\n",
      "Word: La_0.9Ca_0.1Mn, Probability: 0.0008\n",
      "Word: Co_3D, Probability: 0.0008\n",
      "Word: MgCo_2O_4, Probability: 0.0008\n",
      "Word: La_0.7Ca_0.3MnO_3, Probability: 0.0008\n",
      "Word: Ti_0.94Co_0.03La_0.03O_2, Probability: 0.0007\n",
      "Word: Dy_3Fe_5O_12, Probability: 0.0007\n",
      "Word: Mn_15Si_26, Probability: 0.0007\n",
      "Word: Ni_2.08Mn_0.96Ga_0.96, Probability: 0.0007\n",
      "Word: LiFeO_2, Probability: 0.0007\n",
      "Word: Fe_73.5Cu_1Nb_3Si_13.5B_9, Probability: 0.0007\n",
      "Word: Co_2Y, Probability: 0.0007\n",
      "Word: FeO_3, Probability: 0.0007\n",
      "Word: CoFe_1.6Er_0.1Gd_0.2Sm_0.1O_4, Probability: 0.0007\n",
      "Word: MnCrAlO_4, Probability: 0.0007\n",
      "Word: SrSm_2Fe_2O_7, Probability: 0.0007\n",
      "Word: SrCr_6Fe_6O_19, Probability: 0.0007\n",
      "Word: Ni_0.5Zn_0.5Fe_2.0O_4.0, Probability: 0.0007\n",
      "Word: Ge_0.99Mn_0.01, Probability: 0.0007\n",
      "Word: Ru_0.5Rh_0.75Cr_1.5Co_0.25O_4, Probability: 0.0007\n",
      "Word: Mn_2VSnC_2, Probability: 0.0006\n",
      "Word: DyCr_xFe_1-xO_3, Probability: 0.0006\n",
      "Word: Bi_0.5La_0.5MnO_3, Probability: 0.0006\n",
      "Word: Mg_1.5Ni_1.5BO_5, Probability: 0.0006\n",
      "Word: La_0.7-xBi_xSr_0.3MnO_3, Probability: 0.0006\n",
      "Word: Sm_2BaNiO_5, Probability: 0.0006\n",
      "Word: Fe_4Cu_2, Probability: 0.0006\n",
      "Word: Mg_xNi_yFe_3-x-yO_4, Probability: 0.0006\n",
      "Word: Ca_xFe_3-xO_4, Probability: 0.0006\n",
      "Word: Mn_2O_3, Probability: 0.0006\n",
      "Word: Ba_6NiMn_4O_15, Probability: 0.0006\n",
      "Word: Co_82Al_14Cr_4, Probability: 0.0006\n",
      "Word: Fe_50Cr_30Al_20, Probability: 0.0006\n",
      "Word: Co_2MnSi, Probability: 0.0006\n",
      "Word: Ca_3FeTa_2O_9, Probability: 0.0006\n",
      "Word: RCo_2Ge_2, Probability: 0.0006\n",
      "Word: CeFeO_3, Probability: 0.0005\n",
      "Word: La_0.5Sm_0.5FeO_3, Probability: 0.0005\n",
      "Word: Fe_xZn_2-xMo_3O_8, Probability: 0.0005\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(prompt, output=None, eos_token=\"</s>\"):\n",
    "    instruction = \"Answer the materials:\\n\"\n",
    "    input = f\"Magnetic materials with {prompt}\\n\"\n",
    "    output = f\"The material is:\"\n",
    "    prompt = (\" \").join([instruction, input, output])\n",
    "    return prompt\n",
    "\n",
    "input_prompt = generate_prompt('low density of states at the Fermi level.')\n",
    "print(input_prompt)\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tokens).logits\n",
    "\n",
    "probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "# Get the top 10 token IDs and their probabilities\n",
    "top_k = 200\n",
    "top_probabilities, top_token_ids = torch.topk(probabilities, top_k)\n",
    "\"\"\"\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "top_probabilities = top_probabilities.squeeze().tolist()\n",
    "top_token_ids = top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(top_token_ids, top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")\n",
    "\"\"\"\n",
    "# Filter tokens with IDs less than 32000\n",
    "mask = top_token_ids >= 32000\n",
    "filtered_top_token_ids = top_token_ids[mask]\n",
    "filtered_top_probabilities = top_probabilities[mask]\n",
    "\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "filtered_top_probabilities = filtered_top_probabilities.squeeze().tolist()\n",
    "filtered_top_token_ids = filtered_top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(filtered_top_token_ids, filtered_top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff7555cd-6b66-4945-900b-e8d6d8ed5ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the materials:\n",
      " Magnetic materials with low conductivity.\n",
      " The material is:\n",
      "Word: BiFeO_3, Probability: 0.0080\n",
      "Word: Fe_3O_4, Probability: 0.0077\n",
      "Word: Ti_0.94Co_0.03La_0.03O_2, Probability: 0.0026\n",
      "Word: SrSm_2Fe_2O_7, Probability: 0.0020\n",
      "Word: Mn_2V_3O_12, Probability: 0.0020\n",
      "Word: BaAl_xCr_yFe_11O_19, Probability: 0.0018\n",
      "Word: Cu_0.5Fe_0.5Cr_2S_4, Probability: 0.0018\n",
      "Word: NiFe_2O_4, Probability: 0.0018\n",
      "Word: Bi_0.5La_0.5MnO_3, Probability: 0.0016\n",
      "Word: Fe_0.8Ga_0.2, Probability: 0.0015\n",
      "Word: Gd_0.67Sr_0.33MnO_3, Probability: 0.0015\n",
      "Word: CaMn_3V_4O_12, Probability: 0.0011\n",
      "Word: SrSn_0.97-xFe_xSb_0.03O_3-, Probability: 0.0011\n",
      "Word: Ni_3O_3, Probability: 0.0010\n",
      "Word: Na_2NiSi_4O_10, Probability: 0.0010\n",
      "Word: V_2O_5, Probability: 0.0010\n",
      "Word: Co_21Mo_2B_6, Probability: 0.0010\n",
      "Word: Li_0.46Zn_0.04Fe_2.5O_4, Probability: 0.0009\n",
      "Word: C_5R_5FeC_6R_6, Probability: 0.0009\n",
      "Word: Fe_3Ni, Probability: 0.0008\n",
      "Word: Fe_3W_3C, Probability: 0.0008\n",
      "Word: Fe_xZn_2-xMo_3O_8, Probability: 0.0008\n",
      "Word: Fe_1Pt_3, Probability: 0.0007\n",
      "Word: Co_0.1Cd_0.9Ga_2O_4, Probability: 0.0007\n",
      "Word: La_0.3R_0.2Sr_0.5Ti_0.5Fe_0.5O_3, Probability: 0.0007\n",
      "Word: Pr_0.5Sr_0.45K_0.05MnO_3, Probability: 0.0007\n",
      "Word: Fe_73.5Cu_1Nb_3Si_13.5B_9, Probability: 0.0007\n",
      "Word: LiNi_0.65-xCo_0.1Mn_0.25Cr_xO_2, Probability: 0.0007\n",
      "Word: LiFeO_2, Probability: 0.0007\n",
      "Word: Ni_3Sn_2, Probability: 0.0006\n",
      "Word: Sm_2BaNiO_5, Probability: 0.0006\n",
      "Word: La_2MnCoO_6, Probability: 0.0006\n",
      "Word: NiRh_2S_4, Probability: 0.0006\n",
      "Word: Ni_xZn_2-xGeO_4, Probability: 0.0006\n",
      "Word: LiMgVO_4, Probability: 0.0006\n",
      "Word: Na_3CoF_6, Probability: 0.0006\n",
      "Word: Ca_3FeTa_2O_9, Probability: 0.0006\n",
      "Word: SrFe_11.6Zn_0.4O_19, Probability: 0.0006\n",
      "Word: Co_2TiN, Probability: 0.0006\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(prompt, output=None, eos_token=\"</s>\"):\n",
    "    instruction = \"Answer the materials:\\n\"\n",
    "    input = f\"Magnetic materials with {prompt}\\n\"\n",
    "    output = f\"The material is:\"\n",
    "    prompt = (\" \").join([instruction, input, output])\n",
    "    return prompt\n",
    "\n",
    "input_prompt = generate_prompt('low conductivity.')\n",
    "print(input_prompt)\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tokens).logits\n",
    "\n",
    "probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "# Get the top 10 token IDs and their probabilities\n",
    "top_k = 200\n",
    "top_probabilities, top_token_ids = torch.topk(probabilities, top_k)\n",
    "\"\"\"\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "top_probabilities = top_probabilities.squeeze().tolist()\n",
    "top_token_ids = top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(top_token_ids, top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")\n",
    "\"\"\"\n",
    "# Filter tokens with IDs less than 32000\n",
    "mask = top_token_ids >= 32000\n",
    "filtered_top_token_ids = top_token_ids[mask]\n",
    "filtered_top_probabilities = top_probabilities[mask]\n",
    "\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "filtered_top_probabilities = filtered_top_probabilities.squeeze().tolist()\n",
    "filtered_top_token_ids = filtered_top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(filtered_top_token_ids, filtered_top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be8b60a1-abbc-4546-af81-4f903108bd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magnetic materials with high band gap.\n",
      " The material is:\n",
      "Word: BiFeO_3, Probability: 0.0214\n",
      "Word: NiFe_2O_4, Probability: 0.0130\n",
      "Word: Fe_3O_4, Probability: 0.0122\n",
      "Word: CaMn_3V_4O_12, Probability: 0.0095\n",
      "Word: MgMn_xCr_2-xO_4, Probability: 0.0081\n",
      "Word: CoFe_2, Probability: 0.0074\n",
      "Word: Ca_xFe_3-xO_4, Probability: 0.0058\n",
      "Word: Fe_3Ni, Probability: 0.0042\n",
      "Word: CoFe_2O_4, Probability: 0.0038\n",
      "Word: Li_0.46Zn_0.04Fe_2.5O_4, Probability: 0.0037\n",
      "Word: BaFe_12O_19, Probability: 0.0037\n",
      "Word: Co_2TiN, Probability: 0.0036\n",
      "Word: Ni_0.5Zn_0.5Fe_2O_4, Probability: 0.0036\n",
      "Word: CsCoCl_3, Probability: 0.0034\n",
      "Word: Cu_0.5Fe_0.5Cr_2S_4, Probability: 0.0032\n",
      "Word: ZnFe_2O_4, Probability: 0.0026\n",
      "Word: Fe_0.8Ga_0.2, Probability: 0.0026\n",
      "Word: Ge_0.99Mn_0.01, Probability: 0.0026\n",
      "Word: La_0.3R_0.2Sr_0.5Ti_0.5Fe_0.5O_3, Probability: 0.0026\n",
      "Word: Fe_2B, Probability: 0.0026\n",
      "Word: Ni_xZn_2-xGeO_4, Probability: 0.0026\n",
      "Word: LiNi_0.65-xCo_0.1Mn_0.25Cr_xO_2, Probability: 0.0025\n",
      "Word: Mn_3GaC, Probability: 0.0023\n",
      "Word: Fe_1Pt_3, Probability: 0.0023\n",
      "Word: Sr_2Ni_2Fe_12O_22, Probability: 0.0022\n",
      "Word: Gd_0.67Sr_0.33MnO_3, Probability: 0.0021\n",
      "Word: Co_2Y, Probability: 0.0021\n",
      "Word: YReFeAlO_6, Probability: 0.0021\n",
      "Word: SrSn_0.97-xFe_xSb_0.03O_3-, Probability: 0.0021\n",
      "Word: Ca_3FeTa_2O_9, Probability: 0.0021\n",
      "Word: La_2MnCoO_6, Probability: 0.0019\n",
      "Word: GdFeO_3, Probability: 0.0019\n",
      "Word: NiRh_2S_4, Probability: 0.0018\n",
      "Word: Ni_50Mn_29Ga_21, Probability: 0.0018\n",
      "Word: BaSc_xFe_12-xO_19, Probability: 0.0018\n",
      "Word: Co_3O_4, Probability: 0.0017\n",
      "Word: Mn_15Si_26, Probability: 0.0017\n",
      "Word: Ga_1-xSn_xCMn_3, Probability: 0.0016\n",
      "Word: RFeO_3, Probability: 0.0016\n",
      "Word: LaCaBiMn_2O_7, Probability: 0.0016\n",
      "Word: Er_12Fe_82B_6, Probability: 0.0016\n",
      "Word: Ni_3O_3, Probability: 0.0015\n",
      "Word: Co_82Al_14Cr_4, Probability: 0.0015\n",
      "Word: Fe_3-xGa_xBO_6, Probability: 0.0014\n",
      "Word: BaCo_1-xNi_xO_3, Probability: 0.0014\n",
      "Word: Co_2FeSi, Probability: 0.0014\n",
      "Word: Mn_2O_3, Probability: 0.0014\n",
      "Word: Mn_5O_8, Probability: 0.0013\n",
      "Word: ZnFe_2, Probability: 0.0013\n",
      "Word: CuCo_2S_4, Probability: 0.0012\n",
      "Word: Ru_0.5Rh_0.75Cr_1.5Co_0.25O_4, Probability: 0.0012\n",
      "Word: Dy_3Fe_5O_12, Probability: 0.0012\n",
      "Word: Ni_48Mn_34In_12Co_6, Probability: 0.0012\n",
      "Word: CuEu_0.01Fe_1.99O_4, Probability: 0.0012\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(prompt, output=None, eos_token=\"</s>\"):\n",
    "    #instruction = \"Answer the materials:\\n\"\n",
    "    input = f\"Magnetic materials with {prompt}\\n\"\n",
    "    output = f\"The material is:\"\n",
    "    prompt = (\" \").join([input, output])\n",
    "    return prompt\n",
    "\n",
    "input_prompt = generate_prompt('high band gap.')\n",
    "print(input_prompt)\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tokens).logits\n",
    "\n",
    "probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "# Get the top 10 token IDs and their probabilities\n",
    "top_k = 100\n",
    "top_probabilities, top_token_ids = torch.topk(probabilities, top_k)\n",
    "\n",
    "# Filter tokens with IDs less than 32000\n",
    "mask = top_token_ids >= 32000\n",
    "filtered_top_token_ids = top_token_ids[mask]\n",
    "filtered_top_probabilities = top_probabilities[mask]\n",
    "\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "filtered_top_probabilities = filtered_top_probabilities.squeeze().tolist()\n",
    "filtered_top_token_ids = filtered_top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(filtered_top_token_ids, filtered_top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b97d150-810d-4cf9-879e-261861c030d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0fdf16af59424cbad4688074921883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained('results/checkpoint-8400')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.load_adapter('results/checkpoint-8400')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1717ae01-2ff2-4175-9d6d-0a4fe5f6365d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d23b94f46fc4fe887ebe7deb127d58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "original_embeddings = model.get_input_embeddings().weight.detach().clone()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('results/checkpoint-8400')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.load_adapter('results/checkpoint-8400')\n",
    "\n",
    "\n",
    "embeddings = model.get_input_embeddings().weight.data\n",
    "embeddings[:len(original_tokenizer)] = original_embeddings[:len(original_tokenizer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e6c9a0-ea83-464d-b371-86fbecb8f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(prompt, output=None, eos_token=\"</s>\"):\n",
    "    instruction = \"Answer the materials:\\n\"\n",
    "    input = f\"Magnetic materials with {prompt}\\n\"\n",
    "    output = f\"The molecular formula of the material:\"\n",
    "    prompt = (\" \").join([instruction, input, output])\n",
    "    return prompt\n",
    "\n",
    "input_prompt = generate_prompt('low magnetic damping constant.')\n",
    "print(input_prompt)\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tokens).logits\n",
    "\n",
    "# 在计算softmax之前，为了数值稳定性，从logits中减去每个logit的最大值\n",
    "logits_stable = logits - torch.max(logits, dim=-1, keepdim=True).values\n",
    "\n",
    "probabilities = torch.softmax(logits_stable[:, -1, :], dim=-1)\n",
    "\n",
    "# Get the top 10 token IDs and their probabilities\n",
    "top_k = 10\n",
    "top_probabilities, top_token_ids = torch.topk(probabilities, top_k)\n",
    "\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "top_probabilities = top_probabilities.squeeze().tolist()\n",
    "top_token_ids = top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(top_token_ids, top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d878c63e-2a64-4790-a3a0-5ad01d25706e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37553"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(probabilities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6db7b0b-f14a-4c1c-8cc5-765ffeca6795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   673,   278, 17279, 29901,    13,  3561,  1212,   293, 17279,\n",
       "           411,  4482, 15611,   270,  1160,   292,  4868, 29889,    13,   450,\n",
       "         13206, 16637,  7063,   310,   278,  5518, 29901]], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ae5a43c-8612-44c3-8b8a-ddc8e679b73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   673,   278, 17279, 29901,    13,  3561,  1212,   293, 17279,\n",
       "           411,  4482, 15611,   270,  1160,   292,  4868, 29889,    13,   450,\n",
       "         13206, 16637,  7063,   310,   278,  5518, 29901, 37551]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4dda011c-9377-4412-af00-4297571242c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('result', save_embedding_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ea844d33-6ef9-4ea0-b695-1518534a1dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('result/tokenizer_config.json',\n",
       " 'result/special_tokens_map.json',\n",
       " 'result/tokenizer.model',\n",
       " 'result/added_tokens.json',\n",
       " 'result/tokenizer.json')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1620556-3e89-48ab-9ac5-3baaea7775bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results/tokenizer_config.json',\n",
       " 'results/special_tokens_map.json',\n",
       " 'results/tokenizer.model',\n",
       " 'results/added_tokens.json',\n",
       " 'results/tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601970e5-872e-4680-8244-01c41a211fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "291eb404-d313-4f1f-87c3-e489d24d60e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(37553, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=256, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=256, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=256, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=256, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=37553, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e506f375-179c-4782-a1c7-44079a380f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False base_model.model.model.embed_tokens.base_layer.weight torch.float16\n",
      "True base_model.model.model.embed_tokens.lora_embedding_A.default torch.float16\n",
      "True base_model.model.model.embed_tokens.lora_embedding_B.default torch.float16\n",
      "False base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.0.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.0.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.0.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.0.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.0.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.1.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.1.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.1.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.1.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.1.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.2.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.2.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.2.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.2.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.2.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.3.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.3.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.3.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.3.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.3.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.4.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.4.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.4.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.4.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.4.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.5.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.5.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.5.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.5.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.5.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.6.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.6.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.6.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.6.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.6.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.7.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.7.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.7.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.7.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.7.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.8.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.8.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.8.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.8.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.8.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.9.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.9.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.9.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.9.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.9.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.10.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.10.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.10.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.10.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.10.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.11.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.11.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.11.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.11.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.11.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.12.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.12.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.12.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.12.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.12.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.13.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.13.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.13.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.13.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.13.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.14.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.14.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.14.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.14.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.14.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.15.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.15.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.15.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.15.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.15.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.16.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.16.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.16.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.16.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.16.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.17.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.17.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.17.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.17.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.17.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.18.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.18.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.18.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.18.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.18.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.19.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.19.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.19.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.19.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.19.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.20.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.20.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.20.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.20.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.20.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.21.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.21.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.21.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.21.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.21.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.22.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.22.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.22.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.22.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.22.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.23.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.23.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.23.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.23.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.23.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.24.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.24.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.24.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.24.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.24.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.25.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.25.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.25.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.25.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.25.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.26.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.26.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.26.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.26.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.26.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.27.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.27.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.27.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.27.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.27.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.28.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.28.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.28.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.28.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.28.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.29.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.29.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.29.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.29.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.29.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.30.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.30.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.30.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.30.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.30.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.31.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.31.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.31.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.31.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.31.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.norm.weight torch.float16\n",
      "False base_model.model.lm_head.weight torch.float16\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(param.requires_grad, name, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81fa8868-d7b2-4b52-9c5d-8d46a3d0af2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: base_model.model.model.embed_tokens.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "# Verify which parameters are trainable\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346aa5df-43be-459d-98fc-247b0a2fda7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fa5b3e-125c-4fb1-b073-82301ba20d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53006efd-ca7f-4cf7-b226-331f6f4316b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Trainable: {name}\", param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0de8f185-07da-46c0-a3af-0f45f6b39016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7faf7c779ee0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0db0120f-b1a6-441a-8dbe-d0e59fe4e9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(38544, 4096)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a34e1d06-cd59-4b28-9915-a865bb031c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6922694656"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50d8bd59-e9dd-4a70-9e38-02944052fd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(38545, 4096)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f10f6-db0d-47ad-9c4f-6d4cc888b013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
