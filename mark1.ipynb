{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e470cd8-840a-4127-b36d-090297fd66a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import argparse\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from peft import (\n",
    "        get_peft_model, \n",
    "        prepare_model_for_kbit_training, \n",
    "        LoraConfig\n",
    "    )\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4670ff28-27fb-473c-b53f-ec5bdbc147b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b7d75ad1b6450b97a5c68c625f317a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                            load_in_8bit=True,\n",
    "                                            device_map=\"auto\"\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58bb3669-59fc-4844-978f-d31948567a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['gilbert', 'materials'],\n",
      "    num_rows: 63\n",
      "})\n",
      "Dataset({\n",
      "    features: ['gilbert', 'materials'],\n",
      "    num_rows: 63\n",
      "})\n",
      "Answer the value:\n",
      " Gilbert damping constant of Y_3Fe_5O_12\n",
      " Value: Very low, around 0.0001 - 0.0004 </s> \n"
     ]
    }
   ],
   "source": [
    "# Load your custom JSON dataset\n",
    "custom_data = load_dataset('json', data_files='Gilbert.json')\n",
    "\n",
    "# Access train, test, and validation splits if available\n",
    "data_train = custom_data['train']\n",
    "data_val = custom_data['train']\n",
    "\n",
    "# Print the dataset details\n",
    "print(data_train)\n",
    "print(data_val)\n",
    "\n",
    "# Access an example\n",
    "#example = data_train[0]\n",
    "#print(example)\n",
    "\n",
    "def generate_prompt(materials, gilbert=None, eos_token=\"</s>\"):\n",
    "  instruction = \"Answer the value:\\n\"\n",
    "  input = f\"Gilbert damping constant of {materials}\\n\"\n",
    "  gilbert = f\"Value: {gilbert + ' ' + eos_token if gilbert else ''} \"\n",
    "  prompt = (\" \").join([instruction, input, gilbert])\n",
    "  return prompt\n",
    "\n",
    "print(generate_prompt(data_train[0][\"materials\"], data_train[0][\"gilbert\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1d55d6e-0cd8-4edd-aae3-355e449b153c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the value:\n",
      " Gilbert damping constant of Fe\n",
      " Value:  \n"
     ]
    }
   ],
   "source": [
    "print(generate_prompt('Fe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d54db31b-59b9-473f-9265-4142818dfc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "peft_model_id = \"cp/checkpoint-4200\"\n",
    "#peft_model_id = \"cp2/final\"\n",
    "peft_model = PeftModel.from_pretrained(model, peft_model_id, torch_dtype=torch.float16, offload_folder=\"lora_results/lora_7/temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0f73bb9-2a2c-4f20-a8b1-537889ea2fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the value:\n",
      " Gilbert damping constant of FeCoMn\n",
      " Value:  0.126\n"
     ]
    }
   ],
   "source": [
    "#input_prompt = generate_prompt(data_train[0][\"materials\"])\n",
    "input_prompt = generate_prompt('FeCoMn')\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "    generation_output = peft_model.generate(\n",
    "        input_ids=input_tokens,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        top_p=0.9,\n",
    "        temperature=0.3,\n",
    "        repetition_penalty=1.15,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "      )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0b4fe6e-c867-4643-8afc-b48ac446d937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model == peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8b1b0f0-b8db-4e73-bd5d-ef09466dbf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be set for finutning and batched inference\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "peft_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Loading in 8 bit ...\"\n",
    "peft_model = prepare_model_for_kbit_training(peft_model)\n",
    "#model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "37ee3021-a7c8-419c-8174-b55fc182547a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827e146dc55441448c9c026795b8ca4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/63 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.199330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.199330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.199330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dir = \"cp2\"\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "per_device_eval_batch_size = 4\n",
    "eval_accumulation_steps = 4\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_steps = 50\n",
    "logging_steps = 50\n",
    "learning_rate = 1e-4\n",
    "max_grad_norm = 0.2\n",
    "#max_steps = 50\n",
    "warmup_ratio = 0.03\n",
    "evaluation_strategy=\"epoch\"\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            optim=optim,\n",
    "            evaluation_strategy=evaluation_strategy,\n",
    "            save_steps=save_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            logging_steps=logging_steps,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            #max_steps=max_steps,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            group_by_length=True,\n",
    "            lr_scheduler_type=lr_scheduler_type,\n",
    "            ddp_find_unused_parameters=False,\n",
    "            eval_accumulation_steps=eval_accumulation_steps,\n",
    "            per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        )\n",
    "\n",
    "def formatting_func(prompt):\n",
    "  output = []\n",
    "\n",
    "  for d, s in zip(prompt[\"materials\"], prompt[\"gilbert\"]):\n",
    "    op = generate_prompt(d, s)\n",
    "    output.append(op)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_val,\n",
    "    #peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# We will also pre-process the model by upcasting the layer norms in float 32 for more stable training\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(f\"{output_dir}/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1669c18-e142-4baf-893e-5f0c9f1827d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(peft_model==model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0aa587-0d07-4903-9485-531a4eab03a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
